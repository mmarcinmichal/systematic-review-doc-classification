%\begin{landscape}
% Table generated by Excel2LaTeX from sheet 'Sheet 1'
%\begin{table}[htbp]
% \centering
 % \caption{Known and used classification method.}
    \begin{longtable}{p{.15\textwidth}p{.85\textwidth}}
    \caption{Known and used classification method.} \\
    \hline    
    \specialcell{\textbf{Aspect of work}} & \multicolumn{1}{c}{\textbf{Reference/Description}} \\
	\hline
	
	& \multicolumn{1}{c}{\textbf{~\citet{Tiwari2019}}} \\
    \specialcell{Details} &
	A novel classification method called Quantum-Inspired Binary Classifier (QIBC) resolves a binary classification problem, and it is inspired by quantum detection theory. 
    \\ 
    \specialcell{Findings} & 
	QIBC can outperform the baselines for several categories. However, some results are still unsatisfactory for some categories. 
    \\ 
    \specialcell{Challenges} & 
	An in-depth error classification analysis should be performed. Multi-label classification problems should be addressed.
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Berge2019}}} \\ 
    \specialcell{Details} &
	The Tsetlin Machine learns propositional formulae, such as IF ``rash'' AND ``reaction'' AND ``penicillin'' THEN Allergy to represent the particular facets of each category.    
    \\
    \specialcell{Findings} & 
	The proposed method captures categories using simple propositional formulae that are humans readable. The explanatory power of the Tsetlin Machine-produced clauses seems to equal that of decision trees.
    \\
    \specialcell{Challenges} & 
	A utilisation of word embeddings should be considered. The combination of different data views should be applied. Datasets with more complicated structure should be taken into account.
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Unnikrishnan2019}}} \\
    \specialcell{Details} &
	The work proposes a new approach to sparse classification and presents a comparative study of different sparse classification strategies for text classification. 
    \\
    \specialcell{Findings} & 
	The minimum reconstruction error criterion is a suitable one for the problem of text classification. The computational bottle-neck can be resolved thanks to the proposed dictionary refinement procedure.
    \\
    \specialcell{Challenges} & 
    The authors do not highlighted challenges or open problems.
	\\
		
	& \multicolumn{1}{c}{\textbf{~\citet{Pappagari2018}}} \\
    \specialcell{Details} &
	A new multi-scale Convolutional Neural Network (CNN) architecture uses raw text as input. It contains parallel convolutional layers and optimises jointly a new objective function that optimises at the same time two tasks.     
    \\ 
    \specialcell{Findings} & 
	The objective function that integrates the verification and identification tasks improves the identification task. The proposed approach does not use text pre-processing to achieve better document classification performance.
    \\ 
    \specialcell{Challenges} & 
    The sequence dynamics modelling with Long short-term memory (LSTM) should be incorporating into the proposed model.
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{AlSalemi2018}}} \\
	\specialcell{} & See Table~\ref{tab:fsm} \\
    %\specialcell{Details} &
	%See Table~\ref{tab:fsm}
    %\\ 
    %\specialcell{Findings} & 
	%See Table~\ref{tab:fsm}  
    %\\ 
    %\specialcell{Challenges} & 
	%See Table~\ref{tab:fsm}
	%\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Feng2017}}} \\
    \specialcell{Details} &
    The authors consider the overfitting problem and propose a quantitative measurement, rate of overfitting, denoted as RO. Also, they propose an algorithm called AdaBELM.
    \\ 
    \specialcell{Findings} & 
    Extreme Learning Machine (ELM) suffers from a significant overfitting problem. The proposed model AdaBELM resolve this drawback and has good generalizability, as revealed by its high performance.
    \\ 
    \specialcell{Challenges} & 
    The authors do not highlighted challenges or open problems.
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Sharma2017a}}} \\ 
    \specialcell{Details} &
    The work proposed a new hierarchical sparse based classifier and explores the idea of sparse coding for text classification and seeding the dictionary using the principal components.      
    \\
    \specialcell{Findings} & 
	The proposed hierarchical classifier works better than flat sparse based classifier. The Principal Components Analysis (PCA) may be used to create an overcomplete dictionary.
    \\ 
    \specialcell{Challenges} & 
	More research with other datasets should be conducted. The semantic information should be taken into account.
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Benites2017}}} \\
    \specialcell{Details} &
    The work explores a scalable extension (Hierarchical Adaptive Resonance Associative Map, HARAM) to the fuzzy Adaptive Resonance Associative Map (ARAM) neural network for the quick classification of high-dimensional and large data.
    \\ 
    \specialcell{Findings} & 
	Hierarchical ARAM (HARAM) is faster than ARAM. A voting classification procedure increases accuracy. The Adaptive Resonance Theory (ART) neural networks are highly parallelized.
    \\ 
    \specialcell{Challenges} & 
	The authors noted that the details of implementation can be an issue.
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Sharma2016}}} \\
    \specialcell{Details} &
    The work explores the idea of sparse coding for text classification and seeding the dictionary using the principal components. Also, the work explores using Support Vector Machines (SVMs) with frequency-based kernels.    
    \\ 
    \specialcell{Findings} & 
    The Principal Components Analysis (PCA) may be used to create an overcomplete dictionary. SVMs with Hellingerâ€™s kernel and without PCA mades the best results. A voting classification procedure increases outcomes.  
    \\ 
    \specialcell{Challenges} & 
    The semantic information should be taken into account. Better strategies for combining the classifiers need to be explored.
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Jin2016}}} \\
    \specialcell{Details} &
	The authors built a text classifier by using a Naive Bayes model with utilising a new structure called bag-of-embeddings probabilities.
    \\ 
    \specialcell{Findings} & 
	The model is conceptually simple, with only parameters being embedding vectors, trained using a variation of the skip-gram method. The proposed model outperforms state-of-the-art methods for both balanced and imbalanced data.   
    \\ 
    \specialcell{Challenges} & 
	Leveraging unlabeled data for semi-supervised learning should be considered. The other neural documents models should be exploited to achieve better accuracies.
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{AlSalemi2016}}} \\
    \specialcell{} & See Table~\ref{tab:fsm} \\
    %\specialcell{Details} &
	%See Table~\ref{tab:fsm}
    %\\ 
    %\specialcell{Findings} & 
	%See Table~\ref{tab:fsm}  
    %\\ 
    %\specialcell{Challenges} & 
	%See Table~\ref{tab:fsm}
	%\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Pang2015}}} \\ 
    \specialcell{Details} &
	A new classification method called CenKNN combines the strengths of two widely used text classification techniques, k-Nearest Neighbours (k-NN), and Centroid.
    \\ 
    \specialcell{Findings} & 
	CenKNN overcomes drawbacks of k-NN classifier. CenKNN works better than Centroid. The proposed method is appopriate for highly imbalanced corpora with a small number of classes. Support Vector Machines (SVMs) is a better choice for large balanced corpora.  
    \\ 
    \specialcell{Challenges} & 
	CenKNN should be improved to handle sub-clusters and/or a large number of classes. Multi-label classification problems should be addressed.
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Kusner2015}}} \\
    \specialcell{Details} &
    A distance function (Word Mover's Distance, WMD) measures the dissimilarity between two text documents, and it is as an instance of the Earth Mover's Distance.        
    \\ 
    \specialcell{Findings} & 
    The metric method leads to low error rates across all investigated data sets. WMD is also the slowest metric to compute (the solution to speed up the computations has shown).
    \\ 
    \specialcell{Challenges} & 
    The interpretability of the method should be explored. The document structure should be taken into account in distance function. 
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Feng2015}}} \\ 
    \specialcell{} & See Table~\ref{tab:fsm} \\
    %\specialcell{Details} &
	%See Table~\ref{tab:fsm}
    %\\ 
    %\specialcell{Findings} & 
	%See Table~\ref{tab:fsm}  
    %\\ 
    %\specialcell{Challenges} & 
	%See Table~\ref{tab:fsm}
	%\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Gomez2014}}} \\
    \specialcell{Details} &
    The classification inference is based on reconstruction errors of each classification model for each class, i.e. measuring the difference between the set of reconstructed documents and the original one. 
    \\ 
    \specialcell{Findings} & 
	The proposed method creates a model that well generalises the classification problem. The method performance depends on the number of principal components. The method generally performs better than the rest of the classifiers when a dataset has select properties.
    \\
    \specialcell{Challenges} & 
	Other text classification tasks should be explored. The output prediction of the model should be combined with other classifiers to refine the final prediction.
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Lo2012}}} \\ 
    \specialcell{Details} &
	The work explores the background net~\citep{Chen2011, Lo2011} and a set of different reasoning methods created on top of the net to resolve a document classification task.    
    \\ 
    \specialcell{Findings} & 
	The method gives a good performance without requiring a great effort in preprocessing. 
    \\
    \specialcell{Challenges} & 
    The authors shall further study how to obtain fuzzy association between terms based on granules of articles to achieve a more flexible and powerful approach.
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Sainath2010}}} \\ 
    \specialcell{Details} & 
    The work compares three different frameworks of using the sparse coding solution with different vocabulary size to generate an actual classification decision.
    \\ 
    \specialcell{Findings} & 
	All training documents not only makes the size of the dictionary much larger but also enforces a stronger need for sparseness on coefficients. Sparse coding method offers slight but promising results over the Naive Bayes classifier.  	
	\\ 
	\specialcell{Challenges} & 
	Feature selection techniques should be incorporate. Comparison with other learning methods should be performed. 
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Li2010}}} \\
    \specialcell{Details} &
    The authors improve multi-class text classification thanks to using Error-Correcting Output Coding (ECOC) with sub-class partitions.  
    \\ 
    \specialcell{Findings} & 
    In ECOC, sub-class partition information of positive and negative classes is available but ignored even though it has value for binary classification. No one algorithm can win on every dataset and situation.
    \\
    \specialcell{Challenges} & 
    More experiments on more datasets should be performed. Non-text applications should be considered. Local search algorithms should be explored to improve the proposed strategy.
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Jin2010}}} \\
    \specialcell{Details} &
	The authors create a new classification which is based on prototype learning, where training data is represented as a set of points (prototypes) in a feature space.  
    \\ 
    \specialcell{Findings} & 
	The authors observed that the proposed method produces larger average hypothesis margin than the other prototype learning algorithms.
    \\ 
    \specialcell{Challenges} & 
	Te method as a learning criterion can also be applied to other classifier structures based on gradient descent, such as neural networks and quadratic discriminant functions.
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Xia2009}}} \\ 
    \specialcell{Details} &
    The work explores the linear classification approach, i.e. during the training process, is computed the matrix of scores (the contribution table) and a document is classified into the group with the largest score combination.  
    \\ 
    \specialcell{Findings} & 
	The method has a less time complexity. The method do not need to know the semantic contribution of a term makes to a document it occurs in.
    \\ 
    \specialcell{Challenges} & 
	The different feature weights operation should be considered. 
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Larochelle2008}}} \\ 
    \specialcell{Details} &
    The authors incorporate labels into the training process of the Restricted Boltzmann Machines (RBMS) and propose two models: (1) Discriminative Restricted Boltzmann Machines (DRBMs), and (2) Hybrid Discriminative Restricted Boltzmann Machines (HDRBMs).    
    \\ 
    \specialcell{Findings} & 
    RBMs can and should be used as stand-alone non-linear classifiers. RBMs would be good at capturing the conditional statistical relationship between multiple tasks or in the components in a complex target space.
    \\ 
    \specialcell{Challenges} & 
    More challenging settings such as in multi-task or structured output problems should be considered. The mean-field approximations should be applied. For large but sparse input vectors should be introduced less computationally expensive learning.  
    \\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Genkin2007}}} \\
    \specialcell{Details} &
    A Laplace prior regularisation term is used in Bayesian logistic regression approach. Also, the optimisation method is proposed.   
    \\ 
    \specialcell{Findings} & 
	Classification results depend on the feature selection and configuration of classification method. The authors found a strong correlation between the number of positive training examples and the number of features chosen.
    \\
    \specialcell{Challenges} & 
    Other lasso based feature selection algorithm should explore. Scaling algorithms that estimate \textit{regularization paths} to huge applications remains challenging.
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Qian2007}}} \\
    \specialcell{Details} &
	The work explores the Associative Text Categorization (ATC) concept to produce semantic-aware classifier, which includes understandable rules for text categorization.  
    \\ 
    \specialcell{Findings} & 
	The work confirms an observation from earlier research~\cite{Joachims1997}. A vertical rule-pruning method can greatly help reduce the computational cost.
    \\ 
    \specialcell{Challenges} & 
	The authors do not highlighted challenges or open problems.
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Gliozzo2005}}} \\
    \specialcell{Details} &
	The proposed algorithm utilizes a generalized similarity measure based on Latent Semantic Spaces and a Gaussian Mixture algorithm as a principled method to scale similarity scores into probabilities.     
    \\
    \specialcell{Findings} & 
	We can obtain competitive performance using only the category names as initial seeds.
    \\ 
    \specialcell{Challenges} & 
	The optimal procedures for collecting seed features should be investigated. Contribution of additional seeds performance should be explored. Optimal combinations of intensional and extensional supervision should be investigated.
	\\
		
	& \multicolumn{1}{c}{\textbf{~\citet{Zhang2005}}} \\ 
    \specialcell{Details} &
	The authors studied kernel on the multinomial manifold that enables Support Vector Machines (SVMs) to effectively exploit the intrinsic geometric structure of text data.    
    \\ 
    \specialcell{Findings} & 
	The Negative Geodesic Distance (NGD) on the multinomial manifold is a conditionally positive definite (cpd) kernel, and it leads to accuracy improvements over kernels assuming Euclidean geometry. Linear kernel and TF-IDF with $l_2$ regularisation achieve the second result.
    \\ 
    \specialcell{Challenges} & 
    Negative Geodesic Distance (NGD) kernel should be extended to other manifolds (particularly for multimedia tasks). Other kernel methods should be taken into account.  
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Li2004}}} \\
    \specialcell{Details} &
    The authors propose an improved and adaptive k-NN strategy to deal with it problems.   
    \\ 
    \specialcell{Findings} & 
	The proposed methods are less sensitive to the parameter k and can adequately classify documents belonging to smaller classes with a large k. The proposed strategy is adequate for cases where estimating the parameter k via cross-validation is not possible, and the class distribution of a training set is skewed.
    \\ 
    \specialcell{Challenges} & 
    Multi-label classification problems should be addressed. The question of how to evaluate a dataset for text categorisation should be addressed. A guideline on how to build a useful training collection for text categorisation should be developed.
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Rennie2003}}} \\
    \specialcell{Details} &
	The work explores how the given validation method performs on a selection of regularisation parameter of classification method called Regularized Least Squares Classification (RLSC).      
    \\ 
    \specialcell{Findings} & 
	For RLSC the LOOCV selects consistently a regularization parameter that is too large.
    \\
    \specialcell{Challenges} & 
    Other text data sets should be considered.
	\\
	
    \hline
     \label{tab:cm}
    \end{longtable}%
  %\label{tab:addlabel}%
%\end{table}%
%\end{landscape}