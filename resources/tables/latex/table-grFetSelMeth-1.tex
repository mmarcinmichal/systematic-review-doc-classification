%\begin{landscape}
% Table generated by Excel2LaTeX from sheet 'Sheet 1'
%\begin{table}[htbp]
% \centering
 % \caption{Known and used feature selection method.}
    \begin{longtable}{p{.15\textwidth}p{.85\textwidth}}
    \caption{Known and used feature selection method.} \\
    \hline    
    \specialcell{\textbf{Aspect of work}} & \multicolumn{1}{c}{\textbf{Reference/Description}} \\
	\hline
	
	& \multicolumn{1}{c}{\textbf{~\citet{Brockmeier2018}}} \\
    \specialcell{Details} &
    The authors for the feature selection utilise method called descriptive clustering that consists of automatically organizing data instances into clusters and generating a descriptive summary for each cluster. Based on the each cluster the more discriminative features are extracted.    
    \\ 
    \specialcell{Findings} & 
    The results showed that the proposed method performs accurately and yield feature subsets that are indicative of the cluster content. 
    \\
    \specialcell{Challenges} & 
    In future work the authors want to investigate the propsoed method using more complex features including multi-word expressions, named entities, and clusters of features themselves.
    \\
	
	& \multicolumn{1}{c}{\textbf{~\citet{AlSalemi2018}}} \\
    \specialcell{Details} &
	The authors investigate seven feature ranking methods in order to improve performance of classification method called RFBoost~\cite{AlSalemi2016}. Also, the authors introduce an accelerated version of RFBoost, called RFBoost1. The proposed algorithm selects only one feature, based on its weight. Furthermore, the authors use a document representation called BoWT~\cite{AlSalemi2015}.    
    \\ 
    \specialcell{Findings} &
	RFBoost is an improved and accelerated version of AdaBoost.MH. The results show that there is no best feature ranking method in general, as the performances of the feature ranking methods depend on the nature of the datasets. The main feature of RFBoost1 is its fast performance.
    \\
    \specialcell{Challenges} & 
    Future works can consider improving RFBoost1, and the authors want to investigate the use of other existing feature selection methods for improving both RFBoost and RFBoost1.
    \\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Hassaine2017}}} \\
    \specialcell{Details} &
    The authors represent the corpus of documents by a binary relation in which the objects correspond to the documents, and the attributes correspond to words. After that, the proposed approach extracts keywords in hierarchical importance order by a hyper rectangle tree.  
    \\
    \specialcell{Findings} & 
    The hyper rectangle algorithm in itself provides discriminating features which are almost independent of the chosen weights. The logistic regression classifier outperforms random forests on all tested datasets due to better handling of a large number of features.
    \\
    \specialcell{Challenges} & 
    Future works can consider applying the proposed method for other tasks such as anomaly detection, sentiment analysis or document indexing and ranking.
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Rehman2017}}} \\
    \specialcell{Details} &
	The authors proposed method called Normalized Difference Measure (NDM) elevates the rank of a term having either the true positive rate (tpr) or false positive rate (fpr), value closer to zero, among the terms having equal |tpr - fpr| values. The proposed feature ranking metric assigns higher ranks to the terms frequent in one class and infrequent in the other class.    
    \\
    \specialcell{Findings} & 
	For text datasets, document frequency determines the degree of discrimination power of a term. A term occurring with different document frequencies in positive and negative classes is relatively more discriminative than a term having similar document frequencies in both the classes. NDM may for large datasets boost terms which are rare in both classes. 
    \\
    \specialcell{Challenges} & 
	The authors do not highlighted challenges or open problems.
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Tang2016}}} \\ 
    \specialcell{Details} &
    The authors select a specific feature subset for each category, namely class-specific features. Because such feature selection gives different feature spaces, the authors use Baggenstoss’s PDF Project Theorem (PPT) to reformulate Bayes decision rule for classification with these selected class-specific features.      
    \\ 
    \specialcell{Findings} & 
    The results show improvements of the proposed approach for a small number of features, when Information Gain, Relevance Score, and Maximum Discrimination feature selection criteria are used. Also, when more features are selected, the classification performances of all considered methods are improved, leading to a small improvement of the proposed approach.  
    \\
    \specialcell{Challenges} & 
    The authors do not highlighted challenges or open problems. 
    \\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Tang2016a}}} \\
    \specialcell{Details} &
    The authors revisited two information measures: Kullback-Leibler divergence and Jeffreys divergence for binary hypothesis testing, and analyse their asymptotic properties relating to type I and type II errors of a Bayesian classifier. Next, the authors introduced a new divergence measure, called Jeffreys-Multi-Hypothesis (JMH) divergence, to measure multi-distribution divergence for multi-class classification. Based on the JMH-divergence, they developed two efficient feature selection methods, termed maximum discrimination (MD) and $\chi^2$ methods, for text categorisation. 
    \\
    \specialcell{Findings} & 
    The authors note that almost all of the filter based feature selection approaches that utilise the information theory measures use binary variables. Also, these filter approaches rank the features by only exploring the intrinsic characteristics of data based on the feature relevancy without considering their discriminative information in classifiers.
    \\
    \specialcell{Challenges} & 
    The authors consider exploring how the feature dependence may be used to improve the proposed solution and develop feature selection algorithms by weighting each feature, aiming to maximize the discriminative capacity. The authors want to incorporate the feature selection approaches into other advanced machine learning algorithms to enhance the learning for rare categories.
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Li2016}}} \\
    \specialcell{Details} &
	The work explores how to enhance the feature selection methods such as Chi-square and Information Gain by considering the importance of a feature in a document. The authors propose the formula that change the values of the feature selection methods parameters from integers to real values.    
    \\
    \specialcell{Findings} & 
	The proposed importance weighted feature selection strategy helps the traditional Chi-square and Information Gain metrics obtain better results, especially when using fewer features on imbalanced datasets.
    \\
    \specialcell{Challenges} & 
    The work suggests more research with other datasets and apply the proposed importance weighted feature selection strategy into revising other existing feature selection metchods.
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{AlSalemi2016}}} \\
    \specialcell{Details} &
	The proposed RFBoost algorithm is based on filtering a small fixed number of ranked features in each boosting round rather than using all features, as AdaBoost.MH does. The authors proposed two methods for ranking the features: (1) One Boosting Round (OBR), and (2) Labeled Latent Dirichlet Allocation (LLDA).
    \\
    \specialcell{Findings} & 
	The results demonstrate that RFBoost with the new weighting policies and the LLDA-based feature ranking significantly outperformed all other evaluated algorithms. OBR-based feature ranking yielded the worst performance overall.
    \\
    \specialcell{Challenges} & 
    Future works can investigate the use of RFBoost to solve other multi-label classification problems. In addition, the authors want to investigate the use of other feature ranking methods, as feature ranking is the core idea for improving RFBoost’s effectiveness.
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Wang2016}}} \\
    \specialcell{Details} &
	The authors propose a new feature selection method called Categorical Document Frequency Divided by Categorical Number (CDFDC). The idea of the proposed method bases on adding information about categories including a given term/word in the original formula of Categorical Document Frequency (CDF) to increase terms' discrimination.    
    \\
    \specialcell{Findings} & 
	The authors found that CDFP VM method~\citep{Li2014a} does not perform significantly better than the other feature selection methods in English date set, especially not better than CDF. The high computation complexity may not bring high precision or recall rate, except that we need stable and predictable time efficiency.
    \\
    \specialcell{Challenges} & 
    In the future work the authors want to experiment on larger data set and find the connection between algorithm complexity and documents-time-efficiency.
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Zong2015}}} \\
    \specialcell{Details} &
	The authors create two stages feature selection method called DFS+Similarity for text categorization. First, the proposed method selects features with strong discriminative power. In the second stage the method considers the semantic similarity between features and documents based on the modified term frequency-inverse document frequency (TF-IDF) weight. Finally, on the top of the method, Support Vector Machines utilise the reweighted and pruned feature space to learn a classification model.    
    \\
    \specialcell{Findings} & 
	On the 20-Newsgroups dataset, all the five methods Discriminative features selection (DFS), DFS+Similarity, Chi Square ($\chi2$) statistic, information gain (IG) and mutual information (MI)) produce their worst results when the number of features is the lowest (1000). Amongst the five methods, MI and IG perform relatively poorer than others. Also, MI and IG are also sensitive to the number of features.
    \\
    \specialcell{Challenges} & 
    Future work can consider the characteristics of features distribution in each category of documents. The authors consider address a problem of feature selection in multi-label text categorization.
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Feng2015}}} \\
    \specialcell{Details} &
	The authors create the optimal set of features that is characterized by global and local section index for a group and single feature respectively. Under the naive Bayes assumption, the Global Selection Index is factorized into the Local Selection Indexes, which can in turn be explicitly calculated by the posterior feature selection probability in its own dimension. So, as a result, the high dimension can be reduced to the dimension of each feature itself. Finally, the $2^p$ possible feature subset selection models can be pruned by applying a cutoff for the Latent Semantic Indexes (LSIs) to balance the bias and variance, and the prediction function using Bayesian model averaging can be obtained as a product of much less than p single feature model averages. After modeling the uncertainty of feature subset selection, the author propose a latent selection augmented naive (LSAN) Bayes classifier to provide a good fit to the data.     
    \\
    \specialcell{Findings} & 
	Comparing with the original naive Bayes classifier, the LSAN classifier keeps the promptness of the computation, improves the classification accuracy distinctly, and is competitive with state-of-the-art classifiers. Feature selection and feature weighting can be combined organically in the proposed classifier. The authors conclude that, the proposed model calculates the strength of relevance to the label for each feature, and gives a probabilistic weighted classifier based on Bayesian model averaging. Furthermore, the high dimension can be reduced in this model when dealing with not only the feature selection indexes, but also the future prediction.
    \\
    \specialcell{Challenges} & 
    Future works can consider feature selection and feature weighting under the multinomial naive Bayes, multivariate Poisson model, and combined models (NB with SVM) to improve performance. Future work should be focus on to find the optimal value of the LSI truncation threshold, and discover the corresponding laws for feature selection in text classification. A possible way is to test the statistical significance of the LSAN classifier as suggested to test the significance of the naive Bayes classifier.
	\\
	
    & \multicolumn{1}{c}{\textbf{~\citet{Li2015}}} \\
    \specialcell{Details} & 
    The authors propose method called Weighted Document Frequency (WDF) to create a feature ranking. The ranking bases on the information about how the feature is essential for a document. The importance of a feature in the document is based on two weighting schema TF and TF-IDF.   
    \\
    \specialcell{Findings} & 
	The proposed method outperforms a baseline method called document frequency (DF), but there is no difference between the proposed method and Chi-Square method.  	
	\\
	\specialcell{Challenges} & 
	The work suggests more research with other datasets and applies the proposed feature selection metric into other text mining applications, e.g. text clustering, sentiment analysis. The authors plan to explore further why WDF based on TF-IDF weight does not perform better than WDF, which based on TF weight schema. 
	\\
    	
	& \multicolumn{1}{c}{\textbf{~\citet{Rehman2015}}} \\ 
    \specialcell{Details} &
    The work explores a new feature ranking metric termed as relative discrimination criterion (RDC). The proposed metric enhances ranks of the terms present in only one class or the terms for which the term counts in one class are relatively high then the other class.
    \\
    \specialcell{Findings} & 
    The proposed feature selection strategy helps to choose better features. Hovewer, the RDC measure requires somewhat more computations than the other compared feature ranking metrics. The authors highlight that these computations can be run independently of each other in a parallel way.
    \\
    \specialcell{Challenges} & 
    The work suggests more research with other datasets varying in size, class skew and sparseness. The authors consider to make the term counts more effective in contributing to determining the term rank. Also, the RDC’s performance can be enhanced by tuning different parameters.
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Yan2008}}} \\ 
    \specialcell{Details} &
    The authors integrate feature selection and feature extraction, by using the created optimisation framework. Under this framework, the authors develop a novel feature selection algorithm called Trace Oriented Feature Analysis (TOFA). The novel feature selection algorithm optimises the feature extraction objective function in the solution space of feature selection algorithms.     
    \\
    \specialcell{Findings} & 
	The authors showed that many commonly used algorithms in several previous works are special cases of the proposed unified objective function. The proposed feature selection algorithm can achieve the optimal solution according to its objective function. TOFA is suitable for large scale text data, and it can handle the unsupervised and semi-supervised cases which cannot be solved by traditional algorithms such as Information Gain (IG).
    \\
    \specialcell{Challenges} & 
    In the future, the authors want to analyze the relationship between data distribution and framework's parameter and give information about how to select the optimal setting, i.e. select an optimal objective function according to the data distribution without cross-validation.
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Tesar2006}}} \\ 
    \specialcell{Details} &
    The authors propose a suffix tree based algorythm to discover itemset where itemset contains different and valulable words. In the work, the authors focus attention on the comparison between using n-grams and using itemsets in term of classification accuracy.     
    \\
    \specialcell{Findings} & 
	The bigrams seems to be more suitable for text classification and all feature selection methods. Generally, for any data collection, a feature subset selection approach should be determined on the basis of the principle of the classifier which is used. The employed many bigrams and 2-itemsets to extend the bag-of-words (BOW) document model representation did not overcome the best results achieved by the simple BOW approach.
    \\
    \specialcell{Challenges} & 
    The authors suppose that for a more succesfull text classifier (e.g. SVM) the improvements obtained by incorporating bigrams or 2-itemsets will be generally less noticeable and not very contributing. The authors expect that due to the statistical sparseness, high level n-grams and itemsets cannot affect the text classification performance significantly.
	\\

    \hline
     \label{tab:fsm}
    \end{longtable}%
  %\label{tab:addlabel}%
%\end{table}%
%\end{landscape}