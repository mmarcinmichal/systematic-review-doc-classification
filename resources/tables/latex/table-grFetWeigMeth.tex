\begin{landscape}
% Table generated by Excel2LaTeX from sheet 'Sheet 1'
%\begin{table}[htbp]
% \centering
 % \caption{Known and used schemes of feature weighting.}
    \begin{longtable}{lp{.3\textwidth}p{.8\textwidth}}
    \caption{Known and used schemes of feature weighting.} \\
    \hline    
    Reference & \multicolumn{1}{c}{Aspect of work} & \multicolumn{1}{c}{Description} \\
	\hline
	
	\multirow{3}[0]{*}{~\citep{Tang2020}} & 
    \specialcell{Technical and algorithmic \\ aspect of the work} &
    The authors describe a new term weighting schemat called frequency-inverse exponential frequency (TF-IEF). The proposed method replaces inverse document frequency (IDF) in the frequency-inverse document frequency (TF-IDF) weighting schema with the new global weighting factor IEF to characterize the global weighting factor in the corpus. Thanks to that, we can significantly reduce the effect of feature (term) with high local weighting factor TF in term weighting. As a result, a more representative feature can be generated. 
    \\ & 
    \specialcell{Findings/recommendations \\ of the research} & 
    In work, the authors show that TF-IEF outperforms the state-of-the-art term weighting schemes, such as TF-CHI2 and TF-IG.
    \\ & 
    \specialcell{Highlighted challenges \\ or open problems} & 
    There is no highlighted challenges or open problems.
    \\
    
	\multirow{3}[0]{*}{~\citep{Chen2016}} & 
    \specialcell{Technical and algorithmic \\ aspect of the work} &
    The authors describe a new term weighting schema called Term frequency \& Inverse gravity moment (TF-IGM). The proposed method is a type of Supervised Term Weighting (STW) approach where during term weighting is incorporate information about the class, i.e. we weighting terms by exploiting the known categorical information in a training corpus. The proposed method incorporates a new statistical model to precisely measure the class distinguishing power of a term. Particularly, it makes full use of the fine-grained term distribution across different classes of text.  
    \\ & 
    \specialcell{Findings/recommendations \\ of the research} & 
    TF-IGM outperforms the famous TF-IDF and the state-of-the-art supervised term weighting schemes. Besides, some new findings different from previous studies are obtained and analyzed in-depth in the paper.
    \\ & 
    \specialcell{Highlighted challenges \\ or open problems} & 
    The authors in the future want to conduct comparative studies on the IGM model as a new measure of sample distribution non-uniformity and the traditional statistical models such as variance and entropy. Also, the authors highlight the possibility of applying the IGM model to feature dimension reduction and sentiment analysis. 
	\\
	
	\multirow{3}[10]{*}{~\citep{Luo2011}} & 
    \specialcell{Technical and algorithmic \\ aspect of the work} & 
    The authors propose a novel term weighting scheme by exploiting the semantics of categories and indexing terms. The proposed term weighting approach includes by the following steps: (1) Determining the semantics of categories based on terms appearing in category labels as well as the interpretations of these terms by WordNet, (2) For each category, estimating the semantic similarity of each term with the category, and (3) For each category, combining the semantic similarity of each term with the category and its term frequency in a document to obtain the feature vector of each document. 
    \\ & 
    \specialcell{Findings/recommendations \\ of the research} & 
    The proposed approach outperforms TF-IDF when the amount of training data is small, or the content of documents is focused on well-defined categories.    
	\\ & 
	\specialcell{Highlighted challenges \\ or open problems} & 
	The authors in the future want to employ other ontologies with wider coverage (such as Wikipedia) for expressing the senses of words and category labels. Also, they plan to explore different ways of representing the semantics of categories and other similarity measures.   
	\\
	
    \hline
     \label{tab:fw}
    \end{longtable}%
  %\label{tab:addlabel}%
%\end{table}%
\end{landscape}