%\begin{landscape}
% Table generated by Excel2LaTeX from sheet 'Sheet 1'
%\begin{table}[htbp]
% \centering
 % \caption{Known and used classification method.}
    \begin{longtable}{p{.15\textwidth}p{.85\textwidth}}
    \caption{Known and used classification method.} \\
    \hline    
    \specialcell{\textbf{Aspect of work}} & \multicolumn{1}{c}{\textbf{Reference/Description}} \\
	\hline
	
	& \multicolumn{1}{c}{\textbf{~\citet{Tiwari2019}}} \\
    \specialcell{Details} &
	The authors propose a novel classification method called Quantum-Inspired Binary Classifier (QIBC) for resolve a binary classification problem. For a given category and the set of training samples, the method uses the projector, i.e. a detection operator which must be the one whose eigenvalues are 0 and 1, for each category to identify whether the test sample is about the class or not. To determine whether the test sample is about the class, 1 is examined against a vectorial representation of the test sample.  
    \\ 
    \specialcell{Findings} & 
	QIBC can outperform the baselines for several categories. So, the quantum-based approach seems to be adopted to solve a classification problem. However, some results are still unsatisfactory for some categories. 
    \\ 
    \specialcell{Challenges} & 
	The authors want to conduct in-depth error classification analysis, i.e. they want to understand better the reason as to why the proposed model fails for some classes and performs better for other classes. Also, they want to try to implement a multi-classifier inspired by quantum detection theory for multi-class classification problems.
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Berge2019}}} \\ 
    \specialcell{Details} &
	The authors experimented with the text representation where the terms of a text are treated as propositional variables. From these variables, the authors captured categories using simple propositional formulae, such as: IF ``rash'' AND ``reaction'' AND ``penicillin'' THEN Allergy. The Tsetlin Machine learns these formulae from the labelled text, utilizing conjunctive clauses to represent the particular facets of each category. Therefore, the absence of terms (negated features) also can be used for categorization purposes.    
    \\
    \specialcell{Findings} & 
	The study shows that the proposed method captures categories using simple propositional formulae that are also easy to interpret for humans. The authors observed that the explanatory power of the Tsetlin Machine-produced clauses seems to equal that of decision trees.
    \\
    \specialcell{Challenges} & 
	The authors plan to examine how to use the Tsetlin Machine for unsupervised learning of word embeddings. The authors want to investigate how the sparse structure of documents can be taken advantage of to speed up learning. The authors plan to leverage local context windows to learn structure in paragraphs and sentences for more precise text representation. The authors are interested in how structured medical data can be combined with the medical narrative to support precision medicine. The authors plan to do experiments with the Tsetlin Machine on datasets that are unsolvable for linear algorithms.
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Unnikrishnan2019}}} \\
    \specialcell{Details} &
	The work presents a comparative study of different sparse classification strategies for text classification with an empirical demonstration of the effectiveness of this class of algorithms on text data. The work proposes a dynamic class-wise dictionary selection algorithm and a dictionary refinement algorithm which addresses the computational complexity aspect along with improving the classification performance of the best performing algorithm as observed in the comparative study.    
    \\
    \specialcell{Findings} & 
	The study shows the minimum reconstruction error criterion is the suitable one for the problem of text classification and the authors observed that when the minimum reconstruction error criterion is used with classwise dictionaries, the results are even better. The computational bottle-neck can be resolved thanks to a dictionary refinement procedure.
    \\
    \specialcell{Challenges} & 
    The authors do not highlighted challenges or open problems.
	\\
		
	& \multicolumn{1}{c}{\textbf{~\citet{Pappagari2018}}} \\
    \specialcell{Details} &
	The work investigates a new multi-scale Convolutional Neural Network (CNN) architecture for document classification using raw text as a input. The proposed architecture contains parallel convolutional layers and optimises jointly the new objectctive function. The new objective function puts into account and optimises at the same time two tasks: (1) verification task where is deciding whether two given documents are from the same class, and (2) document classification tasks.     
    \\ 
    \specialcell{Findings} & 
	The objective function that integrating the verification and identification tasks improves identification task. The proposed approach works on raw text and do not consider any kind of ad-hoc techniques such as feature weighting, or vocabulary selection for achive better document classification performance.
    \\ 
    \specialcell{Challenges} & 
    The authors want to explore incorporating sequence dynamics modeling with Long short-term memory (LSTM) into this framework.
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{AlSalemi2018}}} \\
	\specialcell{} & See Table~\ref{tab:fsm} \\
    %\specialcell{Details} &
	%See Table~\ref{tab:fsm}
    %\\ 
    %\specialcell{Findings} & 
	%See Table~\ref{tab:fsm}  
    %\\ 
    %\specialcell{Challenges} & 
	%See Table~\ref{tab:fsm}
	%\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Feng2017}}} \\
    \specialcell{Details} &
    The authors consider the overfitting problem and propose a quantitative measurement, rate of overfitting, denoted as RO. Furthermore, the authors address the problem of overfitting of the extreme learning machine (ELM). They propose an algorithm called AdaBELM, which is based on the idea of weak classifiers compilation from AdaBoost to resolve this drawback.
    \\ 
    \specialcell{Findings} & 
    The authors noted that ELM suffers from a significant overfitting problem, which is due to the output weights being computed by the Moore-Penrose generalized inverse, which is a least-squares minimization issue. The proposed model AdaBELM resolve the drawback mentioned above. The proposed model has good generalizability, as revealed by its high performance on balanced, unbalanced, and real application data.
    \\ 
    \specialcell{Challenges} & 
    The authors do not highlighted challenges or open problems.
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Sharma2017a}}} \\ 
    \specialcell{Details} &
    The work explores the idea of sparse coding for text classification and seeding the dictionary using the principal components of all the training documents. The work investigates how the proposed hierarchical classifier outperforms other solutions.      
    \\
    \specialcell{Findings} & 
	The principal components analysis may be used to create an overcomplete dictionary for text classification purpose. Hierarchical classification based on the sparse coding classifiers which the weighted decomposition principal component analysis to construct dictionary technique works better than shallow/flat classification based on the sparse coding classifier. The weighted decomposition principal component analysis works well in the field of document classification.
    \\ 
    \specialcell{Challenges} & 
	The work suggests more research with other datasets. Possibly using the semantic information may also improve the performance of text classification.
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Benites2017}}} \\
    \specialcell{Details} &
    The work explores a scalable extension to the fuzzy Adaptive Resonance Associative Map (ARAM) neural network, which was specially developed for the quick classification of high-dimensional and large data. This extension aims at increasing the classification speed by adding an extra layer for clustering learned prototypes into large clusters.
    \\ 
    \specialcell{Findings} & 
	The study shows that in comparison with ARAM, Hierarchical ARAM (HARAM) is faster. The performacnce in term of accuracy may increase when we use a voiting classification procedure. A major advantage of Adaptive Resonance Theory (ART) neural networks nowadays is that they can be highly parallelized using GPUs and multi-core CPUs.
    \\ 
    \specialcell{Challenges} & 
	The authors noted that the details of implementation can be an issue.
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Sharma2016}}} \\
    \specialcell{Details} &
    The work explores the idea of sparse coding for text classification and seeding the dictionary using the principal components of all the training documents. Also, the work explores frequency-based kernels such as HIK, Chi-kernel and Hellinger’s kernel for text classification using Support Vector Machines (SVMs).    
    \\ 
    \specialcell{Findings} & 
    The Principal Component Analysis (PCA) can be used to create the overcomplete dictionary, and based on such the dictionary we may achieve good classification results comparable to the method without PCA. SVMs with Hellinger’s kernel and without PCA transformation of data set made the best results. The study demonstrates that effectiveness in text classification may be increased by combining sparse representation classification strategies and SVMs based classifiers, i.e. voting procedure. The work observes that misclassification in 20 Newsgroup corpus can be attributed to the presence of highly confusing classes. It is also found that complicated classes are also semantically related.
    \\ 
    \specialcell{Challenges} & 
    The work suggests the classifier built using the semantic information may improve the performance of text classification.  The approach explored for combining the classifiers can be extended to any the classifiers. Better strategies for combining the classifiers need to be explored.
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Jin2016}}} \\
    \specialcell{Details} &
	The authors built a text classifier by using a Naive Bayes model with utilising a new structure called bag-of-embeddings probabilities. The text class, for a new document, is predicted by maximising the probabilities of embedding vectors of its words under the category.
    \\ 
    \specialcell{Findings} & 
	The created model is conceptually simple, with only parameters being embedding vectors, trained using a variation of the skip-gram method. Experiments on two standard datasets showed that the proposed model outperforms state-of-the-art methods for both balanced and imbalanced data.   
    \\ 
    \specialcell{Challenges} & 
	The authors want to consider leveraging unlabeled data for semi-supervised learning. The authors want to exploit neural documents models, which contains richer parameters spaces by capturing word relations, for potentially better accuracies.
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{AlSalemi2016}}} \\
    \specialcell{} & See Table~\ref{tab:fsm} \\
    %\specialcell{Details} &
	%See Table~\ref{tab:fsm}
    %\\ 
    %\specialcell{Findings} & 
	%See Table~\ref{tab:fsm}  
    %\\ 
    %\specialcell{Challenges} & 
	%See Table~\ref{tab:fsm}
	%\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Pang2015}}} \\ 
    \specialcell{Details} &
	The authors propose a new classification method called CenKNN. The approach combines the strengths of two widely used text classification techniques, k-Nearest Neighbours (k-NN) and centroid based (Centroid) classifiers. First, CenKNN projects high-dimensional (often hundreds of thousands) documents into a low-dimensional (normally a few dozen) space spanned by class centroids. Second, the method uses the k-d tree structure to find K nearest neighbours to classify a new example efficiently.
    \\ 
    \specialcell{Findings} & 
	The study shows that the CenKNN overcomes two issues related to existing k-NN text classifiers, i.e., sensitivity to imbalanced class distributions and irrelevant or noisy term features. CenKNN reduces the expensive computation time in k-NN substantially. Also, CenKNN works better than Centroid. The proposed method is also empirically preferable to another well-known classifier, Support Vector Machines (SVMs), on highly imbalanced corpora with a small number of classes. SVMs is a better choice for large balanced corpora in terms of classification.  
    \\ 
    \specialcell{Challenges} & 
	The authors want to improve CenKNN to handle sub-clusters and/or a large number of classes, e.g., through the use of hierarchical classification and class hierarchy. Also, the authors want to extend the work to multi-label text classification.
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Kusner2015}}} \\
    \specialcell{Details} &
    The authors created a distance function (Word Mover’s Distance, WMD) which measures the dissimilarity between two text documents as the minimum amount of distance that the embedded words of one document need to ``travel'' to reach the embedded words of another document. The authors showed that this distance metric could be cast as an instance of the Earth Mover’s Distance.      
    \\ 
    \specialcell{Findings} & 
    The proposed metric method leads to low error rates across all investigated data sets. The proposed WMD yields by far the most accurate classification results. The authors fair to say that it is also the slowest metric to compute. However, they also show the solution to speed up the proposed distance computations.
    \\ 
    \specialcell{Challenges} & 
    The authors want to explore the interpretability of the method. Also, they want to incorporate document structure into the distances between words by adding penalty terms if two words occur in different sections of similarly structured documents. If, for example, the WMD metric is used to measure the distance between academic papers, it might make sense to penalize word movements between the introduction and method section more than word movements from one introduction to another. 
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Feng2015}}} \\ 
    \specialcell{} & See Table~\ref{tab:fsm} \\
    %\specialcell{Details} &
	%See Table~\ref{tab:fsm}
    %\\ 
    %\specialcell{Findings} & 
	%See Table~\ref{tab:fsm}  
    %\\ 
    %\specialcell{Challenges} & 
	%See Table~\ref{tab:fsm}
	%\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Gomez2014}}} \\
    \specialcell{Details} &
    The proposed classification algorithm utilises the property of the Principal Component Analysis (PCA). During the training phase, the documents of each class are used to create a model that can transform the documents to lower space and for this purpose is used PCA. Thanks to that we receive PCA model for each class. During testing, given a new unseen document, each model projects such document to lower space and, then reconstructs the document using each one of the models again and finally computes the reconstruction errors, by measuring the Frobenius norm of the difference between the set of reconstructed documents and the original one. The model which produces the smallest error indicates the category to be assigned to the new document.  
    \\ 
    \specialcell{Findings} & 
	The authors observed that for 20 newgroup dataset the skewness is low, but the entropy is high, meaning the distribution of documents along the categories is soft, but the uncertainty of such distribution is high. The proposed method creates a model that well generalise the classification problem, i.e. the created model works well when we have a limited number of the training examples. Results have shown that the mRE classifier performs well in terms of accuracy (or micro-F1) and macro-F1. The selection of the number of principal components is an essential issue for the mRE classifier. The authors from the experiments observed that for datasets with high entropy and especially for datasets with high skewness, the mRE classifier generally performs better than the rest of the classifiers.
    \\
    \specialcell{Challenges} & 
	The authors want to apply the mRE method to other text classification tasks. In particular, they are interested in using it for hierarchical classification in large datasets. The authors suggest that the mRE could be used inside the taxonomy to model the internal categories. It would be interesting as well, to combine the output prediction of the mRE with other classifiers to refine the final prediction.
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Lo2012}}} \\ 
    \specialcell{Details} &
	The work explores the background net~\citep{Chen2011, Lo2011} and a set of different reasoning methods created on top of the net to resolve a document classification task. The main idea is to create a structure called bacground net (a graph presetns contextual association between terms) and after learning of such structure is performed probabilistic reasoning  to classify new document.    
    \\ 
    \specialcell{Findings} & 
	The proposed method gives a good performance without requiring a great effort in preprocessing. 
    \\
    \specialcell{Challenges} & 
    The authors shall further study how to obtain fuzzy association between terms based on granules of articles to achieve a more flexible and powerful approach.
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Sainath2010}}} \\ 
    \specialcell{Details} & 
    The goal of the work is to show that a proposed solution applies the idea of sparse coding for text classification without focuses on any feature selection techniques may achieve promising results. The work compares three different frameworks of using the sparse coding solution to generate an actual classification decision. Also, the work explores the sensitivity of the sparse coding method to the vocabulary size of the training documents.
    \\ 
    \specialcell{Findings} & 
	The study shows that using all training documents not only makes the size of the dictionary much larger but also enforces a stronger need for sparseness on coefficients. Also. the results show that sparse coding method offers slight but promising results over the Naive Bayes classifier.  	
	\\ 
	\specialcell{Challenges} & 
	The work proposes the exploration of feature selection techniques, and comparison the achieved results to Support Vector Machines (SVMs). 
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Li2010}}} \\
    \specialcell{Details} &
    The authors improve multiclass text classification thanks to using Error-Correcting Output Coding (ECOC) with sub-class partitions. The authors note that, when training such a binary classifier, sub-class distribution within positive and negative classes is neglected and they try to utilise this information to improve a binary classifier. To this purpose, the authors design a simple binary classification strategy via multiclass categorisation (2vM) to make use of sub-class partition information, which can lead to better performance over the traditional binary classification. The proposed binary classification strategy is then applied to enhance ECOC.  
    \\ 
    \specialcell{Findings} & 
    In ECOC, sub-class partition information of positive and negative classes is available but ignored even though it has value for binary classification. No one algorithm can win on every dataset and situation.
    \\
    \specialcell{Challenges} & 
    The authors plan to conduct more experiments on more datasets. Also, they assume that the proposed method may be useful for non-text applications. Also, the authors plan to explore other local search algorithms to improve the proposed strategy further.
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Jin2010}}} \\
    \specialcell{Details} &
	The authors create a new classification which is based on prototype learning, where training data is represented as a set of points (prototypes) in a feature space. A test point \textit{x} is then classified to the class of the closest prototype. To this purpose, the authors investigate the loss functions of prototype learning algorithms and aim to improve the classification performance using a new form of the loss function with regularisation term.  
    \\ 
    \specialcell{Findings} & 
	The authors observed that the proposed method produces larger average hypothesis margin than the other prototype learning algorithms.
    \\ 
    \specialcell{Challenges} & 
	The authors highlight that the method as a learning criterion can also be applied to other classifier structures based on gradient descent, such as neural networks and quadratic discriminant functions.
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Xia2009}}} \\ 
    \specialcell{Details} &
    The work explores the linear classification approach, i.e. during the training process, is computed the matrix of scores (the contribution table). In the row are placed terms and in the columns are placed the classes of documents. For each word and class is compute a value of how the term contributes to the class. For each term in a test document, the authors look up the ratings it contributes to all categories, and combine the scores of different terms for the same class and then classify the document into the group with the largest score combination.  
    \\ 
    \specialcell{Findings} & 
	The proposed method has a less time complexity, thus is more efficient. The method do not need to know the semantic contribution of a term makes to a document it occurs in.
    \\ 
    \specialcell{Challenges} & 
	The work authors gives for all terms in feature space different weights, but they notice that by making use of more knowledge of the field of natural language processing, improvements can still be made. 
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Larochelle2008}}} \\ 
    \specialcell{Details} &
    The authors incorporate labels into the training process of the Restricted Boltzmann Machines (RBMS). The authors propose two models: (1) Discriminative Restricted Boltzmann Machines (DRBMs), and (2) Hybrid Discriminative Restricted Boltzmann Machines (HDRBMs). These discriminative versions of RBMs integrate the process of discovering features of inputs with their use in classification, without relying on a separate classifier. This ensures that the learned features are discriminative and facilitates model selection.     
    \\ 
    \specialcell{Findings} & 
    The authors argued that RBMs can and should be used as stand-alone non-linear classifiers alongside other standard and more popular classifiers, instead of merely being considered as simple feature extractors. The analysis of the target weights indicate that RBMs would be good at capturing the conditional statistical relationship between multiple tasks or in the components in a complex target space.
    \\ 
    \specialcell{Challenges} & 
    The authors want to investigate the use of discriminative versions of RBMs in more challenging settings such as in multi-task or structured output problems. The authors note that exact computation of the conditional distribution for the target is not tractable anymore, but there exist promising techniques such as mean-field approximations that could estimate that distribution. The authors wan to intend to explore ways to introduce generative learning in RBMs and HDRBMs, which would be less computationally expensive when the input vectors are large but sparse.  
    \\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Genkin2007}}} \\
    \specialcell{Details} &
    The authors propose to use a Laplace prior regularisation term in Bayesian logistic regression approach. Thanks to that they achieve the classification model which produces a sparseness model, i.e. the model which has many zero parameters so that it can avoid overfitting. Furthermore, the authors describe an optimisation method of the proposed method - the optimisation solution bases on the Cyclic coordinate descent (CLG) algorithm. The algorithm was adopted for the Laplace prior regularisation term.   
    \\ 
    \specialcell{Findings} & 
	Feature selection often (although not always) improved the effectiveness of ridge logistic regression. In no case did additional feature selection improve the macro averaged F1 of lasso logistic regression. The authors noted that feature selection sometimes improved the results of SVMs models with default thresholds, but never improved the results of SVMs models with tuned thresholds. So, SVMs built-in regularization is quite effective. The authors found a strong correlation between the number of positive training examples and the number of features chosen.
    \\
    \specialcell{Challenges} & 
    The authors noted that there are the limitations of the lasso as a feature selection algorithm. So, in future work, they will explore alternatives such as Meinshausen's \textit{relaxed lasso}. The authors highlighted that several algorithms that estimate \textit{regularization paths} have emerged in recent years (see, e.g., Park and Hastie 2006). These algorithms have the attractive property of estimating coefficients for all possible hyperparameter values. However, scaling such algorithms to huge applications remains challenging.
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Qian2007}}} \\
    \specialcell{Details} &
	The work explores the Associative Text Categorization (ATC) concept to produce semantic-aware classifier, which includes understandable rules for text categorization. For this purpose, the authors use hyperclique patterns rather than frequent patterns/frequent itemset and thanks to that they achieve a strong connection between the overall similarity of a set of objects/documents and the itemset/words in which they are involved.  
    \\ 
    \specialcell{Findings} & 
	The work confirms an observation from earlier research~\cite{Joachims1997} that text categorization can benefit from larger vocabulary size. Also, this is true for associative text categorization. The improvement of micro-BEP is at the cost of losing efficiency, and this drawback can be resolved thanks to the proposed method, i.e. a vertical rule-pruning method, which can greatly help reduce the computational cost.
    \\ 
    \specialcell{Challenges} & 
	The authors do not highlighted challenges or open problems.
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Gliozzo2005}}} \\
    \specialcell{Details} &
	The authors create the algorithm that can be applied to any categorization problem in which categories are described by initial sets of discriminative features and an unlabeled training data set is provided. The proposed algorithm utilizes a generalized similarity measure based on Latent Semantic Spaces and a Gaussian Mixture algorithm as a principled method to scale similarity scores into probabilities. Both techniques address inherent limitations of the Intensional Learning (IL) setting, and leverage unsupervised information from an unlabeled corpus.     
    \\
    \specialcell{Findings} & 
	The authors observed that we can obtain competitive performance using only the category names as initial seeds. This minimal information per category, when exploited by the IL algorithm, is shown to be equivalent to labelling about 70-160 training documents per-category for the state of the art extensional learning.
    \\ 
    \specialcell{Challenges} & 
	The authors want to investigate optimal procedures for collecting seed features and find out whether additional seeds might still contribute to better performance. The authors note that the interesting topic is an exploration of optimal combinations of intensional and extensional supervision, provided by the user in the forms of seed features and labelled examples.
	\\
		
	& \multicolumn{1}{c}{\textbf{~\citet{Zhang2005}}} \\ 
    \specialcell{Details} &
	The authors assumed that it makes more sense to view document feature vectors as points in a Riemannian manifold, rather than in the much larger Euclidean space. So, they studied kernel on the multinomial manifold that enables Support Vector Machines (SVMs) to effectively exploit the intrinsic geometric structure of text data that leads to improving text classification accuracy.    
    \\ 
    \specialcell{Findings} & 
	The authors proved that the Negative Geodesic Distance (NGD) on the multinomial manifold is a conditionally positive definite (cpd) kernel, and it leads to accuracy improvements over kernels assuming Euclidean geometry for text classification. Linear kernel and TF-IDF with L2 regularisation achieve the second result.
    \\ 
    \specialcell{Challenges} & 
    The authors want to extend Negative Geodesic Distance (NGD) kernel to other manifolds (particularly for multimedia tasks) and apply it in other kernel methods for pattern analysis, such as kernel PCA, kernel Spectral Clustering.  
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Li2004}}} \\
    \specialcell{Details} &
    The authors note that the class distribution in a training set is not even and some classes may have more samples than others, especially when we have an extremely skewed dataset. Furthermore, they highlight that the performance of traditional k-nn algorithms is susceptible to the choice of the parameter k and it is very likely that a fixed k value will result in a bias for large categories, and will not make full use of the information in the training set. The authors propose an improved and adaptive k-NN strategy to deal with these problems.   
    \\ 
    \specialcell{Findings} & 
	The proposed methods are less sensitive to the parameter k than the traditional ones, and can adequately classify documents belonging to smaller classes with a large k. The proposed strategy is especially applicable and promising for cases where estimating the parameter k via cross-validation is not possible, and the class distribution of a training set is skewed.
    \\ 
    \specialcell{Challenges} & 
    The authors want to conduct experiments to deal with the multi-label categorisation problem. Also, they want to examine another interesting topic: the question of how to evaluate a dataset for text categorisation. This research may give some guidelines on how to build a useful training collection for text categorisation.
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Rennie2003}}} \\
    \specialcell{Details} &
	The work explores how the given validation method performs on a selection of regularisation parameter of classification method called Regularized Least Squares Classification (RLSC).      
    \\ 
    \specialcell{Findings} & 
	The author found that for RLSC the LOOCV selects consistently a regularization parameter that is too large.
    \\
    \specialcell{Challenges} & 
    The author limited experiments to a single text classification data set, and higlight that these results may be are uncharacteristic of other domains and possibly other text data sets.
	\\
	
    \hline
     \label{tab:cm}
    \end{longtable}%
  %\label{tab:addlabel}%
%\end{table}%
%\end{landscape}