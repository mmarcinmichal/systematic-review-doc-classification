\begin{landscape}
% Table generated by Excel2LaTeX from sheet 'Sheet 1'
%\begin{table}[htbp]
% \centering
 % \caption{Evaluation and benchmarking works.}
    \begin{longtable}{lp{.3\textwidth}p{.8\textwidth}}
    \caption{Evaluation and benchmarking works.} \\
    \hline    
    Reference & \multicolumn{1}{c}{Aspect of work} & \multicolumn{1}{c}{Description} \\
	\hline
	
    \multirow{3}[10]{*}{~\citep{Arras2017}} & 
    \specialcell{Technical and algorithmic \\ aspect of the work} & 
    The authors demonstrated, based on two classification methods, such as Convolutional neural network (CNN) and a bag-of-words Support Vector Machine (SVM), that understanding of how and why the given classification method classify can be achieved by tracing the classification decision back to individual words using layer-wise relevance propagation (LRP). LRP method is a recently developed technique for explaining predictions of complex non-linear classifiers.   
    \\ & 
    \specialcell{Findings/recommendations \\ of the research} & 
	The authors proposed a novel measure of model explanatory power which does not depend on a classification performance change, but only on the word relevances. The results demonstrate qualitatively that the CNN model produces better explanations than the BoW/SVM classifier. The LRP decomposition method provides better explanations than gradient-based sensitivity analysis (SA). The CNN models present some advantages over the SVM in that their computational costs. The CNN can take advantage of the word similarity information encoded in the distributed word embeddings.	
	\\ & 
	\specialcell{Highlighted challenges \\ or open problems} & 
	The authors expect that proposed technique to be also suitable for types of applications that are based on other neural network architectures such as character-based or recurrent network classifiers, or other types of classification problems (e.g. sentiment analysis). Such expectation requires more research.  
	\\
    
	\multirow{3}[0]{*}{~\citep{Sun2009}} & 
    \specialcell{Technical and algorithmic \\ aspect of the work} &
    The authors compare the performance of state of the art strategies addressing imbalanced text classification using SVM classifiers. The authors summarised the strategies in taxonomy in the context of text classification. Based on the taxonomy, they gave a survey on the techniques proposed for imbalanced classification including resampling, instance weighting and thresholding.  
    \\ & 
    \specialcell{Findings/recommendations \\ of the research} & 
    The authors find that the standard SVM often learn the best decision surface in most test cases. For the classification tasks involving high imbalance ratios, it is therefore more critical to find an appropriate threshold that adjusts decision thresholds of a classifier to balance the precision and recall, than applying any of the resampling or instance weighting strategies. 
    \\ & 
    \specialcell{Highlighted challenges \\ or open problems} & 
    The authors suggest to look deep into thresholding strategies, which may consider the data distribution, the information obtained during the classifier training, and user feedback if available. Another research direction is to improve the SVM learning objective function to consider the data imbalance in learning the decision surface such that the default threshold could be easily adopted.
    \\
    
	\multirow{3}[0]{*}{~\citep{Bramesh2019}} & 
    \specialcell{Technical and algorithmic \\ aspect of the work} &
    The authors compare the performance of state of the art classification methods that utilise vector space where terms are weighting thanks to using the state of the art feature weighting methods.      
    \\ & 
    \specialcell{Findings/recommendations \\ of the research} & 
    The authors noted that the decision tree (C5.0) classifier has performed well when compared with Na√Øve Bayes, neural network, SVM, and KNN on all the stated datasets.
    \\ & 
    \specialcell{Highlighted challenges \\ or open problems} & 
    The authors noted that the study can be also extended to include other feature selection methods and to use the k-fold validation procedure. 
	\\
	
	\multirow{3}[0]{*}{~\citep{Mazyad2017}} & 
    \specialcell{Technical and algorithmic \\ aspect of the work} &
    the authors compare the performance of the different state of the art classification methods that utilise vector space where terms are weighting thanks to using the state of the art feature weighting methods.  
    \\ & 
    \specialcell{Findings/recommendations \\ of the research} & 
    The authors find that the superiority of supervised term weighting methods over unsupervised methods is still not clear. 
    \\ & 
    \specialcell{Highlighted challenges \\ or open problems} & 
    The authors do not highlighted challenges or open problems.
	\\
	
    \hline
     \label{tab:ebm}
    \end{longtable}%
  %\label{tab:addlabel}%
%\end{table}%
\end{landscape}