%\begin{sidewaystable}
%\begin{landscape}
% Table generated by Excel2LaTeX from sheet 'Sheet 1'
%\begin{table}[htbp]
% \centering
 % \caption{Add caption}
    \begin{longtable}{p{.15\textwidth}p{.85\textwidth}}
    \caption{Known and used learning method.} \\
    \hline    
    \specialcell{\textbf{Aspect of work}} & \multicolumn{1}{c}{\textbf{Reference/Description}} \\
	\hline
	    
	& \multicolumn{1}{c}{\textbf{~\citet{Kim2019}}} \\ 	
    \specialcell{Details} & 
    The authors extend the standard co-training learning method, which is one of the several approaches to realise semi-supervised learning. The authors proposed the method where in order to increase the variety of feature sets for classification, they transform a document using three different document representation methods. 
    \\  
    \specialcell{Findings} & 
    The proposed multi-co-training (MCT) method achieve a superior and more robust classification performance than the traditional supervised learning or self-training learning methods even when documents are transformed into a very-low-dimensional vector and the labelled documents are very few.
	\\  
	\specialcell{Challenges} & 
	The proposed method should be tested under different scenarios of class imbalance. The authors highlight that the method can be burdensome in terms of computational complexity. Hence, efficient learning methods, such as early stopping or removing redundant examples, should be explored. 
	\\
    
	
	& \multicolumn{1}{c}{\textbf{~\citet{Pavlinek2017}}} \\ 	
    \specialcell{Details} &
    The authors propose a new self-training solution which is one of the several approaches to realise semi-supervised learning. The proposed solution, called Self-Training with Latent Dirichlet Allocation (ST LDA) utilises inductive learning, where the evaluation is carried out on a validation set. 
    \\  
    \specialcell{Findings} & 
    The authors noted that when is added only a few instances in the early training stage of the classification model, the model may be over-fitted. The proposed ST LDA requires a minimal amount of training labelled data to out-perform other models.
    \\  
    \specialcell{Challenges} & 
    The authors want to extend the method with components that will be able to suggest unlabeled instances that are the most appropriate for labelling to prepare a better initial labelled set. Also, the  computation time should be reduced.
    \\
	
    
	& \multicolumn{1}{c}{\textbf{~\citet{Cai2012}}} \\ 	
    \specialcell{Details} &
    The authors noted that most of the previous active learning approaches explore either the euclidean or data-independent nonlinear structure of the data space and do not take into account the intrinsic manifold structure. So, the authors proposed an approach that explicitly takes into account the intrinsic manifold structure. 
    \\  
    \specialcell{Findings} & 
    When the size of the initial labelled set is small, the methods which select the most representative points are usually better than the methods which select the most uncertain data points. When the size of the initial labelled set is large, the methods which select the most uncertain data points can outperform the methods which select the most representative points. The authors suggested to combine these two actives. 
    \\  
    \specialcell{Challenges} & 
    The computational complexity of all the kernel-based techniques scales with the number of data points, so the proposed method may not be applied to large-scale data sets. The authors suggested using clustering techniques such as K-means to group the data points into clusters and select some representative points from each cluster. 
	\\
    
    \hline
     \label{tab:lm}
    \end{longtable}%
  %\label{tab:addlabel}%
%\end{table}%
%\end{landscape}
%\end{sidewaystable}