%\begin{sidewaystable}
%\begin{landscape}
% Table generated by Excel2LaTeX from sheet 'Sheet 1'
%\begin{table}[htbp]
% \centering
 % \caption{Add caption}
    \begin{longtable}{p{.15\textwidth}p{.8\textwidth}}
    \caption{Known and used learning methods.} \\
    \hline    
    \specialcell{\textbf{Aspect of work}} & \multicolumn{1}{c}{\textbf{Reference/Description}} \\
	\hline
	
	& \multicolumn{1}{c}{\textbf{~\citet{Shen2020}}} \\ 	
    \specialcell{Details} & 
    The authors propose applying learning vector quantization (LVQ) classifiers to the online scenario with stochastic gradient optimization for updating prototypes. They present two efficient clustering-based methods for extracting information from unlabeled data. They use different criteria to update prototypes for labelled and unlabeled data.
    \\  
    \specialcell{Findings} & 
    We can use both the maximum conditional likelihood criterion and the clustering criteria, such as Gaussian mixture or neural gas, alternatively based on the availability of label information. By doing so, we can fully leverage both supervised and unsupervised data to enhance performance.
	\\  
	\specialcell{Challenges} & 
	The authors failed to highlight any challenges or open problems. 
	\\
		    
	& \multicolumn{1}{c}{\textbf{~\citet{Kim2019}}} \\ 	
    \specialcell{Details} & 
    The authors extend the standard co-training learning method. In order to increase the variety of feature sets for classification, they transform a document using three different document representation methods. 
    \\  
    \specialcell{Findings} & 
    The proposed multi-co-training (MCT) method achieves superior classification performance, even when the documents are transformed into a very-low-dimensional vector and the labelled documents are very few.
	\\  
	\specialcell{Challenges} & 
	The work points out that, (1) different scenarios of class imbalance should be explored, and (2) the computational complexity should be improved. 
	\\
    	
	& \multicolumn{1}{c}{\textbf{~\citet{Pavlinek2017}}} \\ 	
    \specialcell{Details} &
    The authors propose a new self-training solution, known as Self-Training with Latent Dirichlet Allocation (ST-LDA), which utilizes inductive learning in which evaluation is conducted on a validation set. 
    \\  
    \specialcell{Findings} & 
    Only a few instances in the early training stage caused the model to be over-fitted. ST-LDA requires a minimal amount of training labelled data to outperform other models.
    \\  
    \specialcell{Challenges} & 
    The work points out that, (1) a better initial labeled set should be established, and (2) the computation time required should also be reduced.
    \\
	    
	& \multicolumn{1}{c}{\textbf{~\citet{Cai2012}}} \\ 	
    \specialcell{Details} &
    The authors propose an approach that explicitly considers the intrinsic manifold structure of data. 
    \\  
    \specialcell{Findings} & 
    The authors suggest combining the methods that select the most uncertain data points, and those that select the most representative points. 
    \\  
    \specialcell{Challenges} & 
    The work points out that, the computational complexity should be improved. 
	\\
    
    \hline
    \label{tab:lm}
    \end{longtable}%
  %\label{tab:addlabel}%
%\end{table}%
%\end{landscape}
%\end{sidewaystable}
