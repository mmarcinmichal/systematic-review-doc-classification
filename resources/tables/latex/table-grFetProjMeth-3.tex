%\begin{landscape}
% Table generated by Excel2LaTeX from sheet 'Sheet 1'
%\begin{table}[htbp]
% \centering
 % \caption{Known and used feature projection methods.}
    \begin{longtable}{p{.15\textwidth}p{.85\textwidth}}
    \caption{Known and used feature projection methods.} \\
    \hline    
    \specialcell{\textbf{Aspect of work}} & \multicolumn{1}{c}{\textbf{Reference/Description}} \\
	\hline
	
	& \multicolumn{1}{c}{\textbf{~\citet{Li2020}}} \\ 
    \specialcell{Details} &
	A proposed representation scheme known as Bag-of-Concepts (BoC) automatically acquires useful conceptual knowledge from an external knowledge base. A second representation model, known as Bag-of-Concept-Clusters (BoCCl), improves BoC representation further.     
    \\ 
    \specialcell{Findings} & 
	Bag-of-words (BoW) is a solid baseline for document classification tasks. BoC and BoCCl can effectively capture the concept-level information of documents. They also offer high interpretability.
    \\
    \specialcell{Challenges} & 
    Sentence-level-based representation should be considered. Conceptual knowledge should be incorporated into deep neural networks.
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Gao2018}}} \\ 
    \specialcell{Details} &
    An innovative latent relation-enhanced word embedding model increases the semantic relatedness of words in the corpus. The authors discover more useful relations between words, and add them to word embeddings.      
    \\ 
    \specialcell{Findings} & 
    Word embedding representation is a powerful tool, as it served various systems as a reliable input.
    \\ 
    \specialcell{Challenges} & 
    The contextual information, as the unique distributions to generate word embeddings should be analyzed carefully.
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Chen2017}}} \\ 
    \specialcell{Details} &
    Competitive learning is introduced during an autoencoder training phase. Due to the competition between neurons, each becomes specialized, and the overall model can learn meaningful representations of textual data.     
    \\
    \specialcell{Findings} & 
    The proposed autoencoder, known as KATE, can learn better representation than traditional autoencoders, and outperforms deep generative models, probabilistic topic models, and even word representation models. 
    \\ 
    \specialcell{Challenges} & 
    KATE should be evaluated on more domain-specific datasets. The scalability and effectiveness of the approach should be improved.
    \\
		
    & \multicolumn{1}{c}{\textbf{~\citet{Hu2017}}} \\ 
    \specialcell{Details} & 
    The authors developed a new regularized Restricted Boltzmann Machines (RBMs), which accounts for class information. 
    \\ 
    \specialcell{Findings} & 
	The features extracted by the proposed method have strong discriminant power. The improved performance of the method comes at the cost of high computational demands. The effect of the inter-class repulsion regularization component obtained by the models is imperceptible — features of different groups cannot be effectively separated.
	\\ 
	\specialcell{Challenges} & 
	A new inter-class repulsion regularization should be used to improve the performance of the method. 
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Kesiraju2016}}} \\
    \specialcell{Details} &
	A Subspace Multinomial Model (SMM), in which modification, i.e. regularization of terms creates a compact and continuous representation for the documents.
    \\ 
    \specialcell{Findings} & 
	The classification accuracy of the SMM increases with the dimensionality of the latent variable, which is not the case with Sparse Topical Coding (STC) or Probabilistic Topic Models (PTM).
    \\
    \specialcell{Challenges} & 
    An in-depth exploration of different optimization techniques should be performed. This should involve exploring discriminative SMMs, and fully Bayesian modelling of SMMs.
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Li2016a}}} \\ 
    \specialcell{Details} &
	A novel hybrid model known as Mixed Word Embedding (MWE) combines two variants of Word2Vec seamlessly by sharing a common encoding structure. Moreover, the model incorporates a global text vector in order to capture more semantic information. 
    \\ 
    \specialcell{Findings} & 
	MWE achieves highly competitive performance. MWE preserves the same time complexity as the Skip-Gram model. 
    \\ 
    \specialcell{Challenges} & 
	MWE should be improved by incorporating more external corpora, and giving consideration to proximity and ambiguity among words.
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Zheng2016}}} \\ 
    \specialcell{Details} &
	A Bidirectional Hierarchical Skip-Gram model (BHSG), models text topic embedding, and considers a whole sentence or document as a special word to capture the semantic relationship between the words and the global context word.    
    \\ 
    \specialcell{Findings} &	
	BHSG utilizes negative sampling; thus, it is highly suitable for large scale data.
    \\ 
    \specialcell{Challenges} & 
    BHSG should be extended to implement more topic-related tasks, such as keyword extraction and text summarization. 
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Rodrigues2014}}} \\ 
    \specialcell{Details} &
	The proposed method is based on the Incremental Naive Bayes Clustering (INBC) algorithm, which was initially designed for continuous inputs, and so is considered an extension of it.    
    \\ 
    \specialcell{Findings} & 
	A single pass over the training data is required to achieve an impressive classification result. As more data is presented, the model can be improved.
    \\ 
    \specialcell{Challenges} & 
    Feature selection should be performed. Other properties, such as similarity criteria, should be verified.
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Cai2012}}} \\ 
    \specialcell{} & See Table~\ref{tab:lm} \\
    %\specialcell{Details} &
	%See Table~\ref{tab:lm}
    %\\ 
    %\specialcell{Findings} & 
	%See Table~\ref{tab:lm}  
    %\\ 
    %\specialcell{Challenges} & 
	%See Table~\ref{tab:lm}
	%\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Li2011}}} \\ 
    \specialcell{Details} &
	A Concise Semantic Analysis (CSA) technique extracts a few concepts (a new N-dimensional space, in which each concept represents a new dimension) based on class labels. It then implements a concise interpretation of words and documents in this new space.      
    \\ 
    \specialcell{Findings} & 
	The CSA helps with dimension-sensitive learning algorithms, such as k-nearest neighbors, to eliminate the \textit{Curse of Dimensionality}~\citep{bouveyron2019,Aggarwal2016}. K-nearest neighbors in the new concept space performs comparably with SVMs. CSA performs equally well in the Chinese and English languages, and incurs a very low computational cost. 
    \\ 
    \specialcell{Challenges} & 
    CSA should adopted to perform on large taxonomies of text categorization. The technique should be also parallelized.
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Salakhutdinov2009}}} \\ 
    \specialcell{Details} &
    The authors propose a method that creates a separate Restricted Boltzmann Machines (RBM) for each document, with as many Softmax units as there are words in the document. The authors also present efficient learning and inference algorithms for the model.   
    \\ 
    \specialcell{Findings} & 
    The model’s learning is easy and stable. It may be scaled up to classify billions of documents. This is in contrast to directed topic models, in which most of the existing inference algorithms are designed to be run in a batch mode. The proposed model can generalize much better than Latent Dirichlet Allocation (LDA).
    \\ 
    \specialcell{Challenges} & 
    Label information should be added to the modelling. The document-specific metadata observed should be incorporated into the model’s learning. More layers should be added to create a Deep Belief Network~\citep{Hinton2006}.
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Yan2008}}} \\ 
    \specialcell{} & See Table~\ref{tab:fsm} \\
    %\specialcell{Details} &
	%See Table~\ref{tab:fsm}
    %\\ 
    %\specialcell{Findings} & 
	%See Table~\ref{tab:fsm}  
    %\\ 
    %\specialcell{Challenges} & 
	%See Table~\ref{tab:fsm}
	%\\
	
    \hline
    \label{tab:fpm}
    \end{longtable}%
  %\label{tab:addlabel}%
%\end{table}%
%\end{landscape}
