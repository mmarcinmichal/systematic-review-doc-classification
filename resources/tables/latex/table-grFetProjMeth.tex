\begin{landscape}
% Table generated by Excel2LaTeX from sheet 'Sheet 1'
%\begin{table}[htbp]
% \centering
 % \caption{Known and used feature projection method.}
    \begin{longtable}{lp{.3\textwidth}p{.8\textwidth}}
    \caption{Known and used feature projection method.} \\
    \hline    
    Reference & \multicolumn{1}{c}{Aspect of work} & \multicolumn{1}{c}{Description} \\
	\hline
	
	\multirow{3}[0]{*}{~\citep{Li2020}} & 
    \specialcell{Technical and algorithmic \\ aspect of the work} &
	The authors propose a novel document representation scheme called Bag-of-Concepts (BoC), which automatically acquires useful conceptual knowledge from an external knowledge base, then conceptualizes words and phrases in the document into higher-level semantics (i.e. concepts) in a probabilistic manner, and eventually represents a document as a distributed vector in the learned concept space. By utilizing background knowledge from a knowledge base, BoC representation can provide more semantic and conceptual information of texts, as well as better interpretability for human understanding. Also, the authors propose a second representation model called Bag-of-Concept-Clusters (BoCCl), which clusters semantically similar concepts together and performs entity sense disambiguation to improve BoC representation further.     
    \\ & 
    \specialcell{Findings/recommendations \\ of the research} & 
	The authors found that the classification accuracy of BoW (with L2 normalized tf-idf weighting) outperforms many other document representations, indicating that BoW is a solid baseline for document classification tasks. Experiment results show that BoC and BoCCl can effectively capture the concept-level information of documents and improve the performance of document classification. BoC and BoCCl provide great interpretability which allows human to have a deeper understanding of the document and a clearer operation logic for further reasoning and decision making. Backward Maximum Matching (BMM)~\citep{}  proposed originally for Chinese word segmentation is also efficient to recognize English phrases which are composed of many words.
    \\ & 
    \specialcell{Highlighted challenges \\ or open problems} & 
    The authors want to study BoC/BoCCl for sentence-level representation. The authors  plan to incorporate conceptual knowledge into deep neural networks for better semantic understandings of natural language while preserving model interpretability.
	\\
	
	\multirow{3}[0]{*}{~\citep{Gao2018}} & 
    \specialcell{Technical and algorithmic \\ aspect of the work} &
    The authors propose an innovative latent relation-enhanced word embedding model to increase the semantic relatedness of words in the corpus. The authors discover more useful relations between words and add them to word embedding. The authors take into account pattern and conceptual links between words during the construction of word embedding using by CBOW and Skip-gram methods adequately. Instead of learning embedding vectors merely based on context information (traditional state-of-the-art CBOW and the Skip-gram based models), the authors incorporate latent relations into the training process.      
    \\ & 
    \specialcell{Findings/recommendations \\ of the research} & 
    The authors found that word embedding representation is a powerful tool for various systems as a reliable input. In the paper, the authors treated the importance of the context information equally to predict a word. 
    \\ & 
    \specialcell{Highlighted challenges \\ or open problems} & 
    The authors want to carefully analysis the difference between the context information as some unique distributions to generate word embedding, and the context distributions can be treated as attentions in multi-layer networks, especially related to specific applications. 
	\\
	
	\multirow{3}[0]{*}{~\citep{Chen2017}} & 
    \specialcell{Technical and algorithmic \\ aspect of the work} &
    The authors propose to add competitive learning during an autoencoder training phase. Due to the competition between the neurons in the hidden layer, each neuron becomes specialized in recognizing specific data patterns, and overall the model can learn meaningful representations of textual data.    
    \\ & 
    \specialcell{Findings/recommendations \\ of the research} & 
    A comprehensive set of experiments show that KATE can learn better representations than traditional autoencoders including denoising, contractive, variational, and k-sparse autoencoders. Also, the proposed model outperforms deep generative models, probabilistic topic models, and even word representation models (e.g., Word2Vec) in terms of several downstream tasks such as document classification, regression, and retrieval. The authors found that across tasks such as document classification, multi-label classification, regression and document retrieval, KATE clearly outperforms competing methods or obtains close to the best results. The KATE is also able to learn semantically meaningful representations of words, documents and topics, which we evaluated via both quantitative and qualitative studies. 
    \\ & 
    \specialcell{Highlighted challenges \\ or open problems} & 
    The authors want to evaluate KATE on more domain-specific datasets, such as bibliographic networks, for example, for topic induction and scientific publication retrieval. The authors also want to improve the scalability and effectiveness of our approach on much larger text collections by developing parallel and distributed implementations.
    \\
		
    \multirow{3}[10]{*}{~\citep{Hu2017}} & 
    \specialcell{Technical and algorithmic \\ aspect of the work} & 
    The authors developed a new regularized Restricted Boltzmann Machines (RBM), which takes into account the class information. The authors imposed two constraints on RBM to make the class information clearly reflected in extracted features. One constraint decreases the distance between the features of the same class, and the other one increases the distance between the features of different classes. The two constraints introduce class information to RBM and make the extracted features contain more category information which contributes to a better classification result.   
    \\ & 
    \specialcell{Findings/recommendations \\ of the research} & 
	The authors found that the features extracted by proposed CPr-RBM have stronger discriminant compared to the state-of-the-art RBM variant models. The cpu time of training CPr-RBM is longer than other RBM modelsâ€™, which also results from the two introduced regularization terms. The improved performance of CPr-RBM is at the cost of high computational. The effect of inter-class repulsion regularization part obtained by the proposed models is not obvious, namely, features of different groups can not be well separated.  	
	\\ & 
	\specialcell{Highlighted challenges \\ or open problems} & 
	The authors want to consider a new inter-class repulsion regularization to improve the performance of CPr-RBM. 
	\\
	
	\multirow{3}[0]{*}{~\citep{Kesiraju2016}} & 
    \specialcell{Technical and algorithmic \\ aspect of the work} &
	The authors propose to use subspace multinomial model (SMM) which modification that based on adding the regularization terms to the basic SMM objective function, for obtaining a compact and continuous representation for the documents, which are further used for document classification and clustering tasks.   
    \\ & 
    \specialcell{Findings/recommendations \\ of the research} & 
	In the work the authors observed that the classification accuracy of SMM increases with the increasing dimensionality of the latent variable which is not the case with sparse topical coding (STC) or probabilistic topic models (PTM).
    \\ & 
    \specialcell{Highlighted challenges \\ or open problems} & 
    The work suggests deep exploration of different optimization techniques for $l_1$ regularized objective functions. The future work involves, exploring discriminative SMM and fully Bayesian modelling of SMM.
	\\
	
	\multirow{3}[0]{*}{~\citep{Li2016a}} & 
    \specialcell{Technical and algorithmic \\ aspect of the work} &
	The authors propose a novel hybrid model called mixed word embedding (MWE) based on the well-known word2vec toolbox - "(...) the proposed MWE model combines the two variants of word2vec, i.e., SKIP-GRAM and CBOW, in a seamless way via sharing a common encoding structure, which is able to capture the syntax information of words more accurately. Furthermore, it incorporates a global text vector into the CBOW variant so as to capture more semantic information." Furthermore, the proposed MWE preserves the same time complexity as the SKIP-GRAM.    
    \\ & 
    \specialcell{Findings/recommendations \\ of the research} & 
	The authors found that we may achieve very competitive performance as compared with other state-of-the-art methods thanks to using the learned latent representations.
    \\ & 
    \specialcell{Highlighted challenges \\ or open problems} & 
	The authors want to improve the mixed word embedding model by incorporating more external corpus and considering proximity and ambiguity among words.
	\\
	
	\multirow{3}[0]{*}{~\citep{Zheng2016}} & 
    \specialcell{Technical and algorithmic \\ aspect of the work} &
	The authors propose a Bidirectional Hierarchical Skip-Gram model (BHSG) based on a skip-gram model to model text topic embedding. The basic idea of a general skip-gram model is to predict the surrounding words within a certain distance based on the current one. Words occurring in similar contexts tend to have similar meanings. Thanks to that, we can capture the semantic relationship between the words. The authors in the paper propose to consider the whole sentence/document as a  special word, called global context word, and apply the skip-gram to capture the semantic relationship between the words and the global context word. Then, the words can contain the global context information and the global context word capture the basic topic information of the given text. So, the proposed hierarchical skip-gram model to learn the semantic relationship between different words and enhance the topic relationship between words and document.    
    \\ & 
    \specialcell{Findings/recommendations \\ of the research} &	
	The proposed model is based on the negative sampling manner, which has high efficiency and it is also an unsupervised method. Hence BHSG is very suitable for the large scale data.
    \\ & 
    \specialcell{Highlighted challenges \\ or open problems} & 
    The authors want to extend BHSG to more topic-related tasks such as: keywords extraction and text summarization. 
	\\
	
	\multirow{3}[0]{*}{~\citep{Rodrigues2014}} & 
    \specialcell{Technical and algorithmic \\ aspect of the work} &
	The proposed method is based on the Incremental Naive Bayes Clustering (INBC) algorithm, which was initially designed for continuous inputs, and so is considered an extension to it. After training, the resulting model is composed of a set of clusters which are used to categorize documents. The authors' significant changes made over that model regarding the kind of probability distribution used to model the data and the similarity criterion between clusters and vectors. The first modification was achieved using Multinomial distributions and the second using the Jensen-Shannon distance.     
    \\ & 
    \specialcell{Findings/recommendations \\ of the research} & 
	The authors observed that only a single pass over the training data is needed to achieve good classification result. Also, the authors notice that as more data is presented, the model can be improved.
    \\ & 
    \specialcell{Highlighted challenges \\ or open problems} & 
    The work suggests the works regarding the selection of features used when comparing documents and clusters. As future work, an online extension to this technique could be applied in the proposed approach, improving its feature selection capabilities.  Also, the authors consider other similarity criteria to clusters and documents, preferably ones that could give confidence estimates to the decision.
	\\
	
	\multirow{3}[0]{*}{~\citep{Cai2012}} & 
    \specialcell{Technical and algorithmic \\ aspect of the work} &
	The authors noted that most of the previous active learning approaches explore either the euclidean or data-independent nonlinear structure of the data space and do not take into account the intrinsic manifold structure which may be useful for improving the learning performance. So, the authors proposed an approach that explicitly takes into account the intrinsic manifold structure. The nearest neighbour graph captures the local geometry of the data. The graph Laplacian is incorporated into the manifold adaptive kernel space in which active learning is then performed.    
    \\ & 
    \specialcell{Findings/recommendations \\ of the research} & 
	When the size of the initial labelled set is small, the methods which select the most representative points are usually better than the methods which select the most uncertain data points. When the size of the initial labelled set is large, the methods which select the most uncertain data points can outperform the methods which select the most representative points. With a large number of labelled points, the initial model can be relatively accurate. The authors suggested to combine these two active learning directions: one can select the most representative data points if the size of the initial labelled set is small. As the size of the labelled set increases, one can switch to the methods that select the most uncertain data points.
    \\ & 
    \specialcell{Highlighted challenges \\ or open problems} & 
    The authors highlighted that the computational complexity of all the kernel-based techniques scales with the number of data points, so the proposed method may not be applied to large-scale data sets. The authors suggested using clustering techniques such as K-means to group the data points into clusters and select some representative points from each cluster. After that, the method can be applied only to the representative points. In the work, the authors chose several numbers of the number of selected samples (the algorithm parameter k). This parameter may be different for different datasets, and tunning of them may be inconvenient to optimisation. The author suggested that an acceptable error rate may be fixed and the goal is to minimise the number of selected samples.
	\\
	
	\multirow{3}[0]{*}{~\citep{Li2011}} & 
    \specialcell{Technical and algorithmic \\ aspect of the work} &
	The authors propose a concise semantic analysis (CSA) technique for dimension reduction. CSA extracts a few concepts (a new N-dimensional space, where each concept represents a new dimension) based on category/class labels and then implements a concise interpretation of words and documents in this new space. The created space of concepts is small in quantity and great in generality and tightly related to the category labels. Therefore, CSA preserves necessary information for classifiers with very low computing cost.        
    \\ & 
    \specialcell{Findings/recommendations \\ of the research} & 
	In the work the authors observed that the CSA helps for dimension sensitive learning algorithms such as k-nearest neighbor (kNN) to eliminate the \textit{Curse of Dimensionality}~\citep{}. The kNN in new concept space reaches a comparable performance with support vector machine (SVM) in text categorization applications. In addition, CSA is language independent and performs equally well both in Chinese and English.
    \\ & 
    \specialcell{Highlighted challenges \\ or open problems} & 
    In future work, the authors want to create a implementation of CSA on large taxonomies text categorization will be studied. The authors will create a parallelize version of CSA to satisfy the requirement of large scale text categorization applications.
	\\
	
	\multirow{3}[0]{*}{~\citep{Salakhutdinov2009}} & 
    \specialcell{Technical and algorithmic \\ aspect of the work} &
    The authors propose the method that, for each document, creates a separate Restricted Boltzmann Machines (RBM) with as many softmax units as there are words in the document. The proposed approach ignores the order of the words, and all softmax units can share the same set of weights, connecting them to the hidden binary units. So, the weights can be shared by the whole family of different-sized RBM's that are created for documents of different lengths, and for this reason, the method is called the "Replicated Softmax" model. Also, the authors present efficient learning and inference algorithms for this model and show how a Monte-Carlo based method, Annealed Importance Sampling, can be used to produce an accurate estimate of the log-probability the model assigns to test data. Finally, the authors demonstrate that the proposed model can generalise much better compared to Latent Dirichlet Allocation in terms of both the log-probability of held-out documents and retrieval accuracy.  
    \\ & 
    \specialcell{Findings/recommendations \\ of the research} & 
    In the work, the authors observed that the learning is easy and stable, it can model documents of different lengths, and computing the posterior distribution over the latent topic values is easy. When is using stochastic gradient descent, scaling up learning to billions of documents would not be particularly tricky. This is in contrast to directed topic models, where most of the existing inference algorithms are designed to be run in a batch mode. Therefore the authors would have to make further approximations, for example, by using particle filtering~\citep{Canini2009}. The authors demonstrate that the proposed model can generalise much better than LDA in terms of both the log-probability on held-out documents and retrieval accuracy.
    \\ & 
    \specialcell{Highlighted challenges \\ or open problems} & 
    The authors want to add to modelling the information output label. The authors want to define a conditional Replicated Softmax model, where the observed document-specific metadata, such as author, references, can be used to influence the states of the latent topic units Finally, the authors conclude that a single layer of binary features may not the best way to capture the complex structure in the count data. Once the Replicated Softmax has been trained, they suggest adding more layers to create a Deep Belief Network~\citep{Hinton2006}, which could potentially produce a better generative model and further improve retrieval accuracy.
	\\
	
	\multirow{3}[0]{*}{~\citep{Yan2008}} & 
    \specialcell{Technical and algorithmic \\ aspect of the work} &
    See table XXX 
    \\ & 
    \specialcell{Findings/recommendations \\ of the research} & 
    See table XXX 
    \\ & 
    \specialcell{Highlighted challenges \\ or open problems} & 
    See table XXX 
	\\
	
    \hline
     \label{tab:fpm}
    \end{longtable}%
  %\label{tab:addlabel}%
%\end{table}%
\end{landscape}