%\begin{landscape}
% Table generated by Excel2LaTeX from sheet 'Sheet 1'
%\begin{table}[htbp]
% \centering
 % \caption{Known and used classification methods.}
    \begin{longtable}{p{.15\textwidth}p{.85\textwidth}}
    \caption{Known and used classification methods.} \\
    \hline    
    \specialcell{\textbf{Aspect of work}} & \multicolumn{1}{c}{\textbf{Reference/Description}} \\
	\hline
	
	& \multicolumn{1}{c}{\textbf{~\citet{Tiwari2019}}} \\
    \specialcell{Details} &
	A novel classification method known as a Quantum-Inspired Binary Classifier (QIBC) resolves a binary classification problem. It is inspired by quantum detection theory. 
    \\ 
    \specialcell{Findings} & 
	QIBC can outperform the baselines in several categories. Some results, however, remain unsatisfactory in some categories. 
    \\ 
    \specialcell{Challenges} & 
	An in-depth error classification analysis should be performed. Multi-label classification problems should be addressed.
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Berge2019}}} \\ 
    \specialcell{Details} &
	A Tsetlin Machine learns propositional formulae, such as IF ``rash'' AND ``reaction'' AND ``penicillin'' THEN Allergy, to represent the particular facets of each category.    
    \\
    \specialcell{Findings} & 
	The proposed method captures categories using simple propositional formulae that are readable to humans. The explanatory power of Tsetlin Machine-produced clauses seems to equal that of decision trees.
    \\
    \specialcell{Challenges} & 
	A utilization of word embeddings should be considered. A combination of different data views should be applied. Datasets with more complicated structures should be considered.
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Unnikrishnan2019}}} \\
    \specialcell{Details} &
	This article proposes a new approach to sparse classification, and presents a comparative study of different sparse classification strategies for text classification. 
    \\
    \specialcell{Findings} & 
	The minimum reconstruction error criterion is suitable for the problem of text classification. The computational bottle-neck can be resolved using the proposed dictionary refinement procedure.
    \\
    \specialcell{Challenges} & 
    The authors failed to highlight any challenges or open problems.
	\\
		
	& \multicolumn{1}{c}{\textbf{~\citet{Pappagari2018}}} \\
    \specialcell{Details} &
	A new multi-scale Convolutional Neural Network (CNN) architecture that uses raw text as input. It contains parallel convolutional layers, and jointly optimises a new objective function, which, in turn, optimizes two tasks simultaneously.     
    \\ 
    \specialcell{Findings} & 
	The objective function, which integrates the verification and identification tasks, improves the results of the identification tasks. This approach does not use text pre-processing to achieve better document classification performance.
    \\ 
    \specialcell{Challenges} & 
    The sequence dynamics modeling with Long Short-Term Memory (LSTM) should be incorporated into the proposed model.
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{AlSalemi2018}}} \\
	\specialcell{} & See Table~\ref{tab:fsm} \\
    %\specialcell{Details} &
	%See Table~\ref{tab:fsm}
    %\\ 
    %\specialcell{Findings} & 
	%See Table~\ref{tab:fsm}  
    %\\ 
    %\specialcell{Challenges} & 
	%See Table~\ref{tab:fsm}
	%\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Feng2017}}} \\
    \specialcell{Details} &
    The authors consider the overfitting problem and propose a quantitative measurement, rate of overfitting, denoted as RO. They also propose an algorithm known as AdaBELM.
    \\ 
    \specialcell{Findings} & 
    Extreme Learning Machines (ELMs) suffer from a significant overfitting problem. The proposed model, AdaBELM, resolves this drawback and has high generalizability, which is demonstrated by its high performance.
    \\ 
    \specialcell{Challenges} & 
    The authors failed to highlight any challenges or open problems.
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Sharma2017a}}} \\ 
    \specialcell{Details} &
    The article proposes a new hierarchical sparse-based classifier, exploring the concept of sparse coding for text classification, and seeding the dictionary used the principal components.      
    \\
    \specialcell{Findings} & 
	The proposed hierarchical classifier works better than flat sparse-based classifiers. Principal Component Analysis (PCA) may be used to create an overcomplete dictionary.
    \\ 
    \specialcell{Challenges} & 
	More research with other datasets should be conducted. The semantic information should be considered.
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Benites2017}}} \\
    \specialcell{Details} &
    The work explores a scalable extension—a Hierarchical Adaptive Resonance Associative Map (HARAM)—to a fuzzy Adaptive Resonance Associative Map (ARAM) neural network for quick classification of high-dimensional and large data.
    \\ 
    \specialcell{Findings} & 
	HARAM is faster than ARAM. A voting classification procedure increases its accuracy. Adaptive Resonance Theory (ART) neural networks are highly parallelized.
    \\ 
    \specialcell{Challenges} & 
	The authors noted that the details of implementation could be an issue.
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Sharma2016}}} \\
    \specialcell{Details} &
    The work explores the idea of sparse coding for text classification and seeding the dictionary using principal components. The article also explores the use of Support Vector Machines (SVMs) with frequency-based kernels.    
    \\ 
    \specialcell{Findings} & 
    PCA may be utilized to create an overcomplete dictionary. SVMs with Hellinger’s kernel, and without PCA, produces the best results. A voting classification procedure improves the outcomes.  
    \\ 
    \specialcell{Challenges} & 
    The semantic information should be taken into account. Better strategies for combining the classifiers must be explored.
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Jin2016}}} \\
    \specialcell{Details} &
	The authors built text classifier by using a Naive Bayes model, utilizing a new structure called bag-of-embeddings probabilities.
    \\ 
    \specialcell{Findings} & 
	The model is conceptually simple; the only parameters being embedding vectors, trained using a variation of the Skip-gram method. The proposed model outperforms state-of-the-art methods for both balanced and imbalanced data.   
    \\ 
    \specialcell{Challenges} & 
	Leveraging unlabeled data for semi-supervised learning should be considered. Other neural document models should be exploited to achieve higher accuracy.
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{AlSalemi2016}}} \\
    \specialcell{} & See Table~\ref{tab:fsm} \\
    %\specialcell{Details} &
	%See Table~\ref{tab:fsm}
    %\\ 
    %\specialcell{Findings} & 
	%See Table~\ref{tab:fsm}  
    %\\ 
    %\specialcell{Challenges} & 
	%See Table~\ref{tab:fsm}
	%\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Pang2015}}} \\ 
    \specialcell{Details} &
	A new classification method called CenKNN combines the strengths of two widely-used text classification techniques, k-nearest neighbors and Centroid.
    \\ 
    \specialcell{Findings} & 
	CenKNN overcomes the drawbacks of k-nearest neighbors classifiers. CenKNN works better than Centroid. The proposed method is appropriate for highly imbalanced corpora with a low number of classes. SVM is a better choice for large balanced corpora.  
    \\ 
    \specialcell{Challenges} & 
	CenKNN should be improved to handle sub-clusters and/or a larger number of classes. Multi-label classification problems should be addressed.
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Kusner2015}}} \\
    \specialcell{Details} &
    A distance function, Word Mover's Distance (WMD) measures the dissimilarity between two text documents. This is an instance of the Earth Mover's Distance (EMD).        
    \\ 
    \specialcell{Findings} & 
    The metric method leads to low error rates across all investigated data sets. WMD is also the among the slowest metrics to compute (a solution for speeding up the computations is presented in the article).
    \\ 
    \specialcell{Challenges} & 
    The interpretability of the method should be explored. The document structure should be considered using a distance function. 
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Feng2015}}} \\ 
    \specialcell{} & See Table~\ref{tab:fsm} \\
    %\specialcell{Details} &
	%See Table~\ref{tab:fsm}
    %\\ 
    %\specialcell{Findings} & 
	%See Table~\ref{tab:fsm}  
    %\\ 
    %\specialcell{Challenges} & 
	%See Table~\ref{tab:fsm}
	%\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Gomez2014}}} \\
    \specialcell{Details} &
    Classification inference is based on the reconstruction errors of each classification model for each class, i.e. measuring the difference between the set of reconstructed documents and the original one. 
    \\ 
    \specialcell{Findings} & 
	The proposed method creates a model that generalizes the classification problem well. Its performance depends on the number of principal components. The method performs better than the rest of the classifiers when a dataset has select properties.
    \\
    \specialcell{Challenges} & 
	Other text classification tasks should be explored. The output prediction of the model should be combined with other classifiers to refine the final prediction.
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Lo2012}}} \\ 
    \specialcell{Details} &
	The article explores the background net~\citep{Chen2011, Lo2011}, and a set of different reasoning methods created on top of the net to resolve a document classification task.    
    \\ 
    \specialcell{Findings} & 
	The method produces impressive performance without demanding significant effort in preprocessing. 
    \\
    \specialcell{Challenges} & 
    The authors study how to obtain fuzzy association between terms based on granules of articles to achieve a more flexible and powerful approach.
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Sainath2010}}} \\ 
    \specialcell{Details} & 
    The article compares three frameworks used to produce sparse coding solutions with different vocabulary sizes to generate a classification decision.
    \\ 
    \specialcell{Findings} & 
	All training documents not only increase the size of the dictionary significantly, but also enforce a stronger need for sparseness on the coefficients. Sparse coding methods offer slight, but promising results over a Naive Bayes classifier.  	
	\\ 
	\specialcell{Challenges} & 
	Feature selection techniques should be incorporated. Comparison with other learning methods should be performed. 
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Li2010}}} \\
    \specialcell{Details} &
    The authors improve multi-class text classification using Error-Correcting Output Coding (ECOC) with sub-class partitions.  
    \\ 
    \specialcell{Findings} & 
    In ECOC, sub-class partition information of positive and negative classes is available, but ignored, even though it has a value for binary classification. No single algorithm can win on every dataset and situation.
    \\
    \specialcell{Challenges} & 
    More experiments on more datasets should be performed. Non-text applications should be considered. Local search algorithms should be explored to improve the proposed strategy.
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Jin2010}}} \\
    \specialcell{Details} &
	The authors create a new classification based on prototype learning, in which training data is represented as a set of points (prototypes) in a feature space.  
    \\ 
    \specialcell{Findings} & 
	The authors observed that the proposed method produces a larger average hypothesis margin than other prototype learning algorithms.
    \\ 
    \specialcell{Challenges} & 
	The method can also be applied as a learning criterion to other classifier structures based on gradient descent, such as neural networks and quadratic discriminant functions.
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Xia2009}}} \\ 
    \specialcell{Details} &
    The article explores the linear classification approach – a matrix of scores (the contribution table) is computed during the training process and a document is classified into the group with the largest score combination.  
    \\ 
    \specialcell{Findings} & 
	The method has lower time complexity, and does not need to know the semantic contribution of a term makes to a document in which it occurs.
    \\ 
    \specialcell{Challenges} & 
	Different feature weights should be considered. 
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Larochelle2008}}} \\ 
    \specialcell{Details} &
    The authors incorporate labels into the training process of Restricted Boltzmann Machines (RBMS), and propose two models: (1) Discriminative Restricted Boltzmann Machines (DRBMs), and (2) Hybrid Discriminative Restricted Boltzmann Machines (HDRBMs).    
    \\ 
    \specialcell{Findings} & 
    RBMs can and should be used as standalone non-linear classifiers. RBMs are effective at capturing the conditional statistical relationship between multiple tasks, or between the components in a complex target space.
    \\ 
    \specialcell{Challenges} & 
    More challenging settings, such as multi-task or structured output problems should be considered. Mean-field approximations should be applied. For large, but sparse input vectors, less computationally expensive learning should be introduced.  
    \\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Genkin2007}}} \\
    \specialcell{Details} &
    A Laplace prior regularization term is used within a Bayesian logistic regression approach. An optimization method is also proposed.   
    \\ 
    \specialcell{Findings} & 
	The classification results depend on feature selection and configuration of the classification method. The authors found a strong correlation between the number of positive training examples and the number of features chosen.
    \\
    \specialcell{Challenges} & 
    Other LASSO-based feature selection algorithms should be explored. Scaling algorithms that estimate \textit{regularization paths} to huge applications remain challenging.
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Qian2007}}} \\
    \specialcell{Details} &
	The article employs the Associative Text Categorization (ATC) concept to produce a semantic-aware classifier, which includes understandable rules for text categorization.  
    \\ 
    \specialcell{Findings} & 
	The article confirms an observation from earlier research of~\cite{Joachims1997}. A vertical rule-pruning method can greatly help reduce computational cost.
    \\ 
    \specialcell{Challenges} & 
	The authors fail to highlight any challenges or open problems.
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Gliozzo2005}}} \\
    \specialcell{Details} &
	The proposed algorithm utilizes a generalized similarity measure based on latent semantic spaces and a Gaussian Mixture algorithm to scale similarity scores into probabilities.     
    \\
    \specialcell{Findings} & 
	Competitive performance can be achieved only by using the category names as initial seeds.
    \\ 
    \specialcell{Challenges} & 
	The optimal procedures for collecting seed features should be investigated. The contribution of additional seed performance should be explored. Optimal combinations of intensional and extensional supervision should be investigated.
	\\
		
	& \multicolumn{1}{c}{\textbf{~\citet{Zhang2005}}} \\ 
    \specialcell{Details} &
	The authors studied kernels on the multinomial manifold that enables Support Vector Machines (SVMs) to effectively exploit the intrinsic geometric structure of text data.    
    \\ 
    \specialcell{Findings} & 
	Negative Geodesic Distance (NGD) on the multinomial manifold is a conditionally positive definite (CPD) kernel, and leads to improvements in accuracy over kernels assuming Euclidean geometry. Linear kernel and TF-IDF with $l_2$ regularization achieve second result.
    \\ 
    \specialcell{Challenges} & 
    The NGD kernel should be extended to other manifolds (particularly for multimedia tasks). Other kernel methods should be considered.  
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Li2004}}} \\
    \specialcell{Details} &
    The authors propose an improved and adaptive k-nearest neighbors strategy to resolve its problems.   
    \\ 
    \specialcell{Findings} & 
	The proposed methods are less sensitive to the parameter k, and can adequately classify documents belonging to smaller classes with larger values of k. The proposed strategy is adequate for cases in which estimating the parameter k via cross-validation is impossible, and the class distribution of the training set is skewed.
    \\ 
    \specialcell{Challenges} & 
    Multi-label classification problems should be addressed. The question of how to evaluate a dataset for text categorization should be addressed. A guideline on how to build a useful training collection for text categorization should be developed.
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Rennie2003}}} \\
    \specialcell{Details} &
	The work explores how the given validation method performs on a selection of regularization parameters of a classification method called Regularized Least Squares Classification (RLSC).      
    \\ 
    \specialcell{Findings} & 
	For RLSC, leave-one-out cross validation (LOOCV) consistently selects a regularization parameter that is too large.
    \\
    \specialcell{Challenges} & 
    Other text datasets should be considered.
	\\
	
    \hline
    \label{tab:cm}
    \end{longtable}%
  %\label{tab:addlabel}%
%\end{table}%
%\end{landscape}
