%\begin{landscape}
% Table generated by Excel2LaTeX from sheet 'Sheet 1'
%\begin{table}[htbp]
% \centering
 % \caption{Known and used feature selection method.}
    \begin{longtable}{p{.15\textwidth}p{.8\textwidth}}
    \caption{Known and used feature selection methods.} \\
    \hline    
    \specialcell{\textbf{Aspect of work}} & \multicolumn{1}{c}{\textbf{Reference/Description}} \\
	\hline
	
	& \multicolumn{1}{c}{\textbf{~\citet{Brockmeier2018}}} \\
    \specialcell{Details} &
    The feature selection proposed utilizes a method known as descriptive clustering, which involves automatically organizing data instances into clusters, and generating a descriptive summary for each cluster.    
    \\ 
    \specialcell{Findings} & 
    The proposed method performs accurately, and yields feature subsets that are indicative of the cluster content. 
    \\
    \specialcell{Challenges} & 
    The work points out that, the more complex features, including multi-word expressions, named entities, and clusters of the features themselves should be investigated.
    \\
	
	& \multicolumn{1}{c}{\textbf{~\citet{AlSalemi2018}}} \\
    \specialcell{Details} &
	Seven feature ranking methods are applied in order to improve the performance of the RFBoost classification method ~\citep{AlSalemi2016}. An accelerated version of RFBoost, called RFBoost1, is introduced. 
    \\ 
    \specialcell{Findings} &
	RFBoost is an improved and accelerated version of AdaBoost. MH. There is no overall best feature ranking method. The performance of the feature ranking methods depends on the nature of the datasets. RFBoost1 has fast performance.
    \\
    \specialcell{Challenges} & 
    The authors wish to investigate the use of other feature selection methods to improve both RFBoost and RFBoost1.
    \\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Hassaine2017}}} \\
    \specialcell{Details} &
    The proposed approach extracts keywords in hierarchical order of importance using a hyper rectangle tree.  
    \\
    \specialcell{Findings} & 
    The hyper rectangle algorithm provides discriminating features that are almost independent of the chosen weights. The logistic regression classifier outperforms random forests due to better handling of a large number of features.
    \\
    \specialcell{Challenges} & 
    The work points out that, the other tasks, such as anomaly detection, sentiment analysis, and document indexing and ranking should be considered.
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Rehman2017}}} \\
    \specialcell{Details} &
	A method known as Normalized Difference Measure (NDM) utilizes the true positive rate (tpr) and false positive rate (fpr) to create a feature ranking metric.    
    \\
    \specialcell{Findings} & 
	A term occurring with different document frequencies in positive and negative classes is relatively more discriminative than one that has similar document frequencies in both classes. NDM boosts terms that are rare in both classes. 
    \\
    \specialcell{Challenges} & 
	The authors failed to highlight any challenges or open problems.
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Tang2016}}} \\ 
    \specialcell{Details} &
    The authors use Baggenstoss’s PDF Project Theorem (PPT) to reformulate Bayes’ decision rule for classification with selected class-specific features.      
    \\ 
    \specialcell{Findings} & 
    An improvement is achieved in a small number of the features. When more features are selected, the classification performance of all methods considered improve, leading to a minor improvement to the overall approach. 
    \\
    \specialcell{Challenges} & 
    The authors failed to highlight any challenges or open problems. 
    \\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Tang2016a}}} \\
    \specialcell{Details} &
    Based on JMH-divergence, the authors developed two efficient feature selection methods for text categorization, termed maximum discrimination (MD) and $\chi^2$ methods. 
    \\
    \specialcell{Findings} & 
    Almost all of the filter-based feature selection approaches use binary variables. These filter approaches only by exploring the intrinsic characteristics of the data.
    \\
    \specialcell{Challenges} & 
    The work points out that, (1) feature dependence should be utilized in order to maximize discriminative capacity, and (2) enhancement of the learning for rare categories should be considered.
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Li2016}}} \\
    \specialcell{Details} &
	The authors propose a formula that convert the values of a feature selection method’s parameters from integers to real values.    
    \\
    \specialcell{Findings} & 
	The proposed method assists the Chi-square ($\chi^2$) and Information Gain (IG) metrics to obtain better results, especially when fewer features are used on imbalanced datasets.
    \\
    \specialcell{Challenges} & 
    The work points out that, (1) other datasets should be considered for evaluation, and (2) the proposed strategy should be applied to revising other feature selection methods.
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{AlSalemi2016}}} \\
    \specialcell{Details} &
	The RFBoost algorithm proposed is based on filtering a low, fixed number of ranked features, rather than using all features. Two methods for ranking features are proposed: (1) One Boosting Round (OBR); and (2) Labeled Latent Dirichlet Allocation (LLDA).
    \\
    \specialcell{Findings} & 
	RFBoost, with the new weighting policies and the LLDA-based feature ranking, significantly outperformed all other algorithms evaluated. OBR-based feature ranking yielded the worst performance overall.
    \\
    \specialcell{Challenges} & 
    The work points out that, (1) multi-label classification problems should be considered, and (2) other feature ranking methods, as it is the core concept for improving the effectiveness of RFBoost, should be considered.
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Wang2016}}} \\
    \specialcell{Details} &
	The proposed method, known as Categorical Document Frequency Divided by Categorical Number (CDFDC), involves adding information about categories to a given term in the original formula of Categorical Document Frequency (CDF), to increase the discrimination of the terms.    
    \\
    \specialcell{Findings} & 
	The high computational complexity might not enable high precision or recall rate, and stable and predictable time efficiency is necessary.
    \\
    \specialcell{Challenges} & 
    The work points out that, (1) other, more extensive datasets should be evaluated, and (2) the connection between algorithm complexity and document-time-efficiency should be explored.
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Zong2015}}} \\
    \specialcell{Details} &
	The method established, known as Discriminative Feature Selection with Similarity (DFS+Similarity), selects features with strong discriminative power, and considers the semantic similarity between features and documents.    
    \\
    \specialcell{Findings} & 
	DFS, DFS+Similarity, Chi Square ($\chi2$) statistic, Information Gain (IG), and Mutual Information (MI) produce the worst results when the number of features is the lowest (1000). MI and IG perform relatively poorer than the others, and they are sensitive to the number of features.
    \\
    \specialcell{Challenges} & 
    The work points out that, (1) multi-label classification problems should be considered, and (2) the characteristics of feature distribution in each category of documents should be considered. 
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Feng2015}}} \\
    \specialcell{Details} &
	The authors develop an optimal set of features that is characterized by global and local section indices for group and single features, respectively. The author also proposes a Latent Selection Augmented Naive (LSAN) Bayes classifier to enable a suitable fit to the data.
    \\
    \specialcell{Findings} &	
	Feature selection and feature weighting can be combined organically in the classifier proposed. The high dimension can be reduced in this model when working with not only the feature selection indices, but also future predictions.
    \\
    \specialcell{Challenges} & 
    The work points out that, (1) other feature selection and weighting methods and parameters should be examined, (2) corresponding laws for feature selection should be explored, and (3) a statistically in-depth analysis should be performed.
	\\
	
    & \multicolumn{1}{c}{\textbf{~\citet{Li2015}}} \\
    \specialcell{Details} & 
    The method, known as Weighted Document Frequency (WDF), creates a feature ranking based on information about how a feature is essential for a document.
    \\
    \specialcell{Findings} & 
	The method outperforms the document frequency (DF) approach, but there is no difference between the proposed method and the Chi-Square ($\chi2$) method.  	
	\\
	\specialcell{Challenges} & 
	The work points out that, (1) more research with other datasets is needed, (2) other text mining applications should be considered for evaluation, and (3) the influence of feature-weighting schema should be studied more deeply.
	\\
    	
	& \multicolumn{1}{c}{\textbf{~\citet{Rehman2015}}} \\ 
    \specialcell{Details} &
    A new feature ranking metric called Relative Discrimination Criterion (RDC) enhances the ranking of the terms present in only one class, or those for which the term counts in a single class are relatively higher than in the other.
    \\
    \specialcell{Findings} & 
    The method selects suitable features. The RDC measure, however, requires somewhat more computation than the other feature ranking metrics.
    \\
    \specialcell{Challenges} & 
    The work points out that, (1) more research with other datasets is needed, and (2) tuning of the parameters should be considered.
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Yan2008}}} \\ 
    \specialcell{Details} &
    The proposed optimization framework integrates feature selection and feature extraction. A novel feature selection algorithm called Trace Oriented Feature Analysis (TOFA) optimizes the feature extraction objective function in the solution spaces of feature selection algorithms.     
    \\
    \specialcell{Findings} & 
	Many commonly used algorithms are special cases of the proposed unified objective function. The optimal solution, according to its objective function, can be achieved. TOFA is suitable for large-scale text data. The solution can handle both unsupervised and semi-supervised cases.
    \\
    \specialcell{Challenges} & 
    The work points out that, (1) the relationship between data distribution and the framework's parameters should be established, and (2) calibration of the optimal setting should be performed.
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Tesar2006}}} \\ 
    \specialcell{Details} &
    A new suffix-tree-based algorithm discovers itemsets which contains different and valuable words.
    \\
    \specialcell{Findings} & 
	Bigrams seem to be more suitable for text classification. A feature subset selection approach should be determined on the basis of the principle of the classifier used. The extended bag-of-words (BoW) failed to overcome the best results achieved by the simple BoW approach.
    \\
    \specialcell{Challenges} & 
	The work points out that, a more in-depth experiment and evaluation should be performed to establish what types of activity influence text classification performance significantly.
	\\

    \hline
    \label{tab:fsm}
    \end{longtable}%
  %\label{tab:addlabel}%
%\end{table}%
%\end{landscape}
