\begin{landscape}
% Table generated by Excel2LaTeX from sheet 'Sheet 1'
%\begin{table}[htbp]
% \centering
 % \caption{Add caption}
    \begin{longtable}{lp{.3\textwidth}p{.8\textwidth}}
    \caption{Known and used learning method.} \\
    \hline    
    Reference & \multicolumn{1}{c}{Aspect of work} & \multicolumn{1}{c}{Description} \\
	\hline
	
    \multirow{3}[10]{*}{~\citep{Kim2019}} & 
    \specialcell{Technical and algorithmic \\ aspect of the work} & 
    The authors extend the standard co-training learning method, which is one of the several approaches to realise semi-supervised learning. The authors proposed the method where in order to increase the variety of feature sets for classification, they transform a document using three document representation methods: term frequency-inverse document frequency (TF-IDF) based on the bag-of-words scheme, topic distribution based on latent Dirichlet allocation (LDA), and neural-network-based document embedding known as document to vector (Doc2Vec). 
    \\ & 
    \specialcell{Findings/recommendations \\ of the research} & 
    The proposed multi-co-training (MCT) method achieve a superior and more robust classification performance than the traditional supervised learning or self-training learning methods even when documents are transformed into a very-low-dimensional vector and the labelled documents are very few.
	\\ & 
	\specialcell{Highlighted challenges \\ or open problems} & 
	The proposed method should be tested under different scenarios of class imbalance. The authors highlight that the method can be burdensome in terms of computational complexity. Hence, efficient learning methods, such as early stopping or removing redundant examples, should be explored. 
	\\
    
	\multirow{3}[0]{*}{~\citep{Pavlinek2017}} & 
    \specialcell{Technical and algorithmic \\ aspect of the work} &
    The authors propose a new self-training solution which is one of the several approaches to realise semi-supervised learning. The proposed solution, called Self-Training with Latent Dirichlet Allocation (ST LDA)  utilises inductive learning, where the evaluation is carried out on a validation set (a part of data that is excluded from the available training data before the semi-supervised training phase). The ST LDA method consists of two major parts: a self-training algorithm for enlarging the initial labelled set, and a model for the setting of the algorithm's parameters. The parameter estimation model is included in the ST LDA method in order to provide a fully automated text classification method without the need for manually tuning parameters for each new document collection. 
    \\ & 
    \specialcell{Findings/recommendations \\ of the research} & 
    The authors noted that when is added only a few instances in the early training stage of the classification model, the model may be over-fitted. The proposed ST LDA requires a minimal amount of training labelled data to out-perform other models.
    \\ & 
    \specialcell{Highlighted challenges \\ or open problems} & 
    The authors highlighted that the main drawback of ST LDA is the time needed to build a classification model, where most of the execution time is consumed on a self-training algorithm. So, it is an open problem how this time may be reduced. The authors want to extend the method with components that will be able to suggest unlabeled instances that are the most appropriate for labelling to prepare a better initial labelled set. 
    \\
	
    \multirow{3}[0]{*}{~\citep{Cai2012}} & 
    \specialcell{Technical and algorithmic \\ aspect of the work} &
    The authors noted that most of the previous active learning approaches explore either the euclidean or data-independent nonlinear structure of the data space and do not take into account the intrinsic manifold structure which may be useful for improving the learning performance. So, the authors proposed an approach that explicitly takes into account the intrinsic manifold structure. 
    \\ & 
    \specialcell{Findings/recommendations \\ of the research} & 
    When the size of the initial labelled set is small, the methods which select the most representative points are usually better than the methods which select the most uncertain data points. When the size of the initial labelled set is large, the methods which select the most uncertain data points can outperform the methods which select the most representative points. The authors suggested to combine these two active learning directions: one can select the most representative data points if the size of the initial labelled set is small. As the size of the labelled set increases, one can switch to the methods that select the most uncertain data points. 
    \\ & 
    \specialcell{Highlighted challenges \\ or open problems} & 
    The computational complexity of all the kernel-based techniques scales with the number of data points, so the proposed method may not be applied to large-scale data sets. The authors suggested using clustering techniques such as K-means to group the data points into clusters and select some representative points from each cluster. After that, the method can be applied only to the representative points. The authors chose several numbers of the number of selected samples (the algorithm parameter \textit{k}). This parameter may be different for different datasets, and tunning of them may be inconvenient to optimisation. The author suggested that an acceptable error rate may be fixed and the goal is to minimise the number of selected samples. 
	\\
    
    \hline
     \label{tab:lm}
    \end{longtable}%
  %\label{tab:addlabel}%
%\end{table}%
\end{landscape}