%\begin{landscape}
% Table generated by Excel2LaTeX from sheet 'Sheet 1'
%\begin{table}[htbp]
% \centering
 % \caption{Known and used feature selection method.}
    \begin{longtable}{p{.15\textwidth}p{.85\textwidth}}
    \caption{Known and used feature selection method.} \\
    \hline    
    \specialcell{\textbf{Aspect of work}} & \multicolumn{1}{c}{\textbf{Reference/Description}} \\
	\hline
	
	& \multicolumn{1}{c}{\textbf{~\citet{Brockmeier2018}}} \\
    \specialcell{Details} &
    The proposed feature selection utilises a method called descriptive clustering that consists of automatically organizing data instances into clusters and generating a descriptive summary for each cluster.    
    \\ 
    \specialcell{Findings} & 
    The proposed method performs accurately and yield feature subsets that are indicative of the cluster content. 
    \\
    \specialcell{Challenges} & 
    The more complex features, including multi-word expressions, named entities, and clusters of features themselves should be investigated.
    \\
	
	& \multicolumn{1}{c}{\textbf{~\citet{AlSalemi2018}}} \\
    \specialcell{Details} &
	Seven feature ranking methods are applied in order to improve the performance of classification method called RFBoost~\citep{AlSalemi2016}. An accelerated version of RFBoost, called RFBoost1, is introduced. 
    \\ 
    \specialcell{Findings} &
	RFBoost is an improved and accelerated version of AdaBoost.MH. There is no best feature ranking method in general. The performances of the feature ranking methods depend on the nature of the datasets. RFBoost1 has fast performance.
    \\
    \specialcell{Challenges} & 
    The authors want to investigate the use of other existing feature selection methods for improving both RFBoost and RFBoost1.
    \\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Hassaine2017}}} \\
    \specialcell{Details} &
    The proposed approach extracts keywords in hierarchical importance order by a hyper rectangle tree.  
    \\
    \specialcell{Findings} & 
    The hyper rectangle algorithm provides discriminating features which are almost independent of the chosen weights. The logistic regression classifier outperforms random forests due to better handling of a large number of features.
    \\
    \specialcell{Challenges} & 
    The other tasks, such as anomaly detection, sentiment analysis or document indexing and ranking should be considered.
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Rehman2017}}} \\
    \specialcell{Details} &
	A proposed method called Normalized Difference Measure (NDM) utilises the true positive rate (tpr) and false positive rate (fpr) to create a feature ranking metric.    
    \\
    \specialcell{Findings} & 
	A term occurring with different document frequencies in positive and negative classes is relatively more discriminative than a term having similar document frequencies in both the classes. NDM boosts terms which are rare in both classes. 
    \\
    \specialcell{Challenges} & 
	The authors do not highlighted challenges or open problems.
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Tang2016}}} \\ 
    \specialcell{Details} &
    The authors use Baggenstoss’s PDF Project Theorem (PPT) to reformulate Bayes decision rule for classification with the selected class-specific features.      
    \\ 
    \specialcell{Findings} & 
    An improvement is achieved for a small number of features. When more features are selected, the classification performances of all considered methods are improved, leading to a small improvement of the proposed approach. 
    \\
    \specialcell{Challenges} & 
    The authors do not highlighted challenges or open problems. 
    \\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Tang2016a}}} \\
    \specialcell{Details} &
    Based on the JMH-divergence, the authors developed two efficient feature selection methods, termed maximum discrimination (MD) and $\chi^2$ methods, for text categorisation. 
    \\
    \specialcell{Findings} & 
    Almost all of the filter based feature selection approaches use binary variables. Also, these filter approaches only exploring the intrinsic characteristics of data.
    \\
    \specialcell{Challenges} & 
    The feature dependence should be used to maximize the discriminative capacity. To enhance the learning for rare categories should be considered.
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Li2016}}} \\
    \specialcell{Details} &
	The authors propose the formula that change the values of the feature selection methods parameters from integers to real values.    
    \\
    \specialcell{Findings} & 
	The proposed method helps the Chi-square ($\chi^2$) and Information Gain (IG) metrics obtain better results, especially when using fewer features on imbalanced datasets.
    \\
    \specialcell{Challenges} & 
    Other datasets should be considered to evaluate. To apply the proposed strategy into revising other existing feature selection methods.
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{AlSalemi2016}}} \\
    \specialcell{Details} &
	The proposed RFBoost algorithm is based on filtering a small fixed number of ranked features rather than using all features. Two methods for ranking of features are proposed: (1) One Boosting Round (OBR), and (2) Labeled Latent Dirichlet Allocation (LLDA).
    \\
    \specialcell{Findings} & 
	RFBoost with the new weighting policies and the LLDA-based feature ranking significantly outperformed all other evaluated algorithms. OBR-based feature ranking yielded the worst performance overall.
    \\
    \specialcell{Challenges} & 
    Multi-label classification problems should be considered. Other feature ranking methods, as feature ranking is the core idea for improving RFBoost’s effectiveness, should be taken into account.
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Wang2016}}} \\
    \specialcell{Details} &
	The proposed method called Categorical Document Frequency Divided by Categorical Number (CDFDC) bases on adding information about categories including a given term in the original formula of Categorical Document Frequency (CDF) to increase terms' discrimination.    
    \\
    \specialcell{Findings} & 
	The high computational complexity may not bring high precision or recall rate, except that we need stable and predictable time efficiency.
    \\
    \specialcell{Challenges} & 
    Other more extensive data set should be taken into evaluation. The connection between algorithm complexity and documents-time-efficiency should be found.
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Zong2015}}} \\
    \specialcell{Details} &
	The created method called Discriminative Features Selection with Similarity (DFS+Similarity) selects features with strong discriminative power and considers the semantic similarity between features and documents.    
    \\
    \specialcell{Findings} & 
	DFS, DFS+Similarity, Chi Square ($\chi2$) statistic, Information Gain (IG) and Mutual Information (MI) produce worst results when the number of features is the lowest (1000). MI and IG perform relatively poorer than others and they are sensitive to the number of features.
    \\
    \specialcell{Challenges} & 
    Multi-label classification problems should be considered. The characteristics of features distribution in each category of documents should be considered. 
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Feng2015}}} \\
    \specialcell{Details} &
	The authors create the optimal set of features that is characterized by global and local section index for a group and single feature respectively. Also, the author propose a Latent Selection Augmented Naive (LSAN) Bayes classifier to provide a good fit to the data.
    \\
    \specialcell{Findings} &	
	Feature selection and feature weighting can be combined organically in the proposed classifier. The high dimension can be reduced in this model when dealing with not only the feature selection indexes, but also the future prediction.
    \\
    \specialcell{Challenges} & 
    Other feature selection and weighting methods and paramteres should be examined. The corresponding laws for feature selection should be discovered. A statistically in-depth analysis should be performed.
	\\
	
    & \multicolumn{1}{c}{\textbf{~\citet{Li2015}}} \\
    \specialcell{Details} & 
    The method called Weighted Document Frequency (WDF) creates a feature ranking bases on the information about how the feature is essential for a document.
    \\
    \specialcell{Findings} & 
	The method outperforms the document frequency (DF) approach, but there is no difference between the proposed method and Chi-Square ($\chi2$) method.  	
	\\
	\specialcell{Challenges} & 
	More research with other datasets is needed. Other text mining applications should be considered for evaluation. The influence of feature weighting schema should be more studied.
	\\
    	
	& \multicolumn{1}{c}{\textbf{~\citet{Rehman2015}}} \\ 
    \specialcell{Details} &
    A new feature ranking metric termed as Relative Discrimination Criterion (RDC) enhances ranks of the terms present in only one class or the terms for which the term counts in one class are relatively high then the other class.
    \\
    \specialcell{Findings} & 
    The method chooses good features. However, the RDC measure requires somewhat more computations than the other compared feature ranking metrics.
    \\
    \specialcell{Challenges} & 
    More research with other datasets is needed. A tuning of different parameters should be considered.
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Yan2008}}} \\ 
    \specialcell{Details} &
    The proposed optimisation framework integrates feature selection and feature extraction. Also, a novel feature selection algorithm called Trace Oriented Feature Analysis (TOFA) optimises the feature extraction objective function in the solution space of feature selection algorithms.     
    \\
    \specialcell{Findings} & 
	Many commonly used algorithms are special cases of the proposed unified objective function. The optimal solution, according to its objective function, can be achieved. TOFA is suitable for large scale text data. The solution can handle unsupervised and semi-supervised cases.
    \\
    \specialcell{Challenges} & 
    The relationship between data distribution and the framework's parameter should be established. Calibration of the optimal setting should be performed.
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Tesar2006}}} \\ 
    \specialcell{Details} &
    A new suffix tree based algorythm discovers itemset where itemset contains different and valulable words.
    \\
    \specialcell{Findings} & 
	The bigrams seems to be more suitable for text classification. A feature subset selection approach should be determined on the basis of the principle of the classifier which is used. The extend bag-of-words (BoW) did not overcome the best results achieved by the simple BoW approach.
    \\
    \specialcell{Challenges} & 
	A more in-depth experiment and evaluation should be performed to establish what type of activities influence on text classification performance significantly.
	\\

    \hline
     \label{tab:fsm}
    \end{longtable}%
  %\label{tab:addlabel}%
%\end{table}%
%\end{landscape}