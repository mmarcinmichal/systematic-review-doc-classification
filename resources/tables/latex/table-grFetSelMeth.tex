\begin{landscape}
% Table generated by Excel2LaTeX from sheet 'Sheet 1'
%\begin{table}[htbp]
% \centering
 % \caption{Known and used feature selection method.}
    \begin{longtable}{lp{.3\textwidth}p{.8\textwidth}}
    \caption{Known and used feature selection method.} \\
    \hline    
    Reference & \multicolumn{1}{c}{Aspect of work} & \multicolumn{1}{c}{Description} \\
	\hline
	
	\multirow{3}[0]{*}{~\citep{Brockmeier2018}} & 
    \specialcell{Technical and algorithmic \\ aspect of the work} &
    The authors for the feature selection utilise method called descriptive clustering that consists of automatically organizing data instances into clusters and generating a descriptive summary for each cluster. Based on the each cluster the more discriminative features are extracted.    
    \\ & 
    \specialcell{Findings/recommendations \\ of the research} & 
    The results showed that the proposed method performs accurately and yield feature subsets that are indicative of the cluster content. 
    \\ & 
    \specialcell{Highlighted challenges \\ or open problems} & 
    In future work the authors want to investigate the propsoed method using more complex features including multi-word expressions, named entities, and clusters of features themselves.
    \\
	
	\multirow{3}[0]{*}{~\citep{AlSalemi2018}} & 
    \specialcell{Technical and algorithmic \\ aspect of the work} &
	The authors investigate seven feature ranking methods (information gain, chi-square, GSS-coefficient, mutual information, odds ratio, F1 score, and accuracy) in order to improve performance of classification method called RFBoost~\cite{AlSalemi2016}. Moreover, the authors introduce an accelerated version of RFBoost, called RFBoost1. The proposed algorithm in contrast to RFBoost rather than filtering a subset of the highest-ranked features selects only one feature, based on its weight, to build a new weak hypothesis. Furthermore, the authors use a document representation called BoWT~\cite{AlSalemi2015}, which is a hybrid representation obtained by merging the top-ranked words and the topics into one representation model, which is processed by the feature selection methods.    
    \\ & 
    \specialcell{Findings/recommendations \\ of the research} & 
	RFBoost is an improved and accelerated version of AdaBoost.MH which proves to be effective and efficient for text categorization. Among the evaluated feature ranking methods, MI proved to be a good choice for improving RFBoost’s performance. However, the results show that there is no best feature ranking method in general, as the performances of the feature ranking methods depend on the nature of the datasets. The experimental results show that RFBoost1 accelerates the weak learning without penalizing the classification performance. The results also show that RFBoost1’s performance is not significantly different from that of AdaBoost.MH, while RFBoost achieves the best performance overall.  The main feature of RFBoost1 is its fast performance. It is approximately four times faster than AdaBoost.MH, making it a more efficient algorithm and a right choice when computation time is an issue.
    \\ & 
    \specialcell{Highlighted challenges \\ or open problems} & 
    Future works can consider improving RFBoost1, and the authors want to investigate the use of other existing feature selection methods for improving both RFBoost and RFBoost1.
	\\
	
	\multirow{3}[0]{*}{~\citep{Hassaine2017}} & 
    \specialcell{Technical and algorithmic \\ aspect of the work} &
    The authors represent the corpus of documents by a binary relation (or context, binary matrix) in which the objects correspond to the documents, and the attributes correspond to words. After that, the proposed approach extracts keywords in hierarchical importance order. The authors construct the optimal set of the hierarchical features for each category of documents using by a hyper rectangle tree. Furthermore, the method allows using different weighting metrics that yield different orderings which imply different sets of keywords. Finally, the lists of keywords (corresponding to the categories of documents) are merged and fed into a classification algorithm for predicting the class of each document.  
    \\ & 
    \specialcell{Findings/recommendations \\ of the research} & 
    The hyper rectangle algorithm in itself provides discriminating features which are almost independent of the chosen weights. The logistic regression classifier outperforms random forests on all tested datasets due to better handling of a large number of features. The proposed weights achieve similar performance which means that the strength of our method lies in the algorithm itself rather than the weighting metrics.
    \\ & 
    \specialcell{Highlighted challenges \\ or open problems} & 
    Future works can consider applying the proposed method for other tasks such as anomaly detection within large textual corporate as well as sentiment analysis for multiple applications. Also, the author planned to test the method of information retrieval tasks, such as document indexing and ranking.
	\\
	
	\multirow{3}[0]{*}{~\citep{Rehman2017}} & 
    \specialcell{Technical and algorithmic \\ aspect of the work} &
	The most of feature ranking methods use a document frequency (a number of a term occurrence in a document) for the determination of term rank. The document frequency of a term in positive class is the number of true positives (tp), while the document frequency in the negative class is the number of false positives (fp). The authors observed that considering only the difference between tp and fp can be misleading for text data and argued that a term whose tp or fp is close to zero along with a high |tp - fp| value is relatively more important. Furthermore, the proposed method called Normalized Difference Measure (NDM) elevates the rank of a term having either the true positive rate (tpr) or false positive rate (fpr), value closer to zero, among the terms having equal |tpr - fpr| values. The proposed feature ranking metric assigns higher ranks to the terms frequent in one class and infrequent in the other class. To achieve this criterion, the authors normalise the difference by dividing |tpr - fpr| by the minimum of the two (tpr, fpr).    
    \\ & 
    \specialcell{Findings/recommendations \\ of the research} & 
	For text datasets, document frequency determines the degree of discrimination power of a term. A term occurring with different document frequencies in positive and negative classes is relatively more discriminative than a term having similar document frequencies in both the classes. For large datasets, NDM may boost terms which are rare in both classes. The authors observed that NDM shows similar or lower performance compared to the other seven feature ranking metrics for highly skewed or large datasets like RE0 and 20 newsgroups datasets. Skew increases further when one-against-all settings is used. 20 newsgroups is a large dataset having 20 classes and all classes contain almost equal number of documents. In one- against-all settings average class skew of 20 newsgroups becomes high (1:19) due to large number of classes i.e. documents in one class against all documents in the rest of 19 classes.
    \\ & 
    \specialcell{Highlighted challenges \\ or open problems} & 
	The authors do not highlighted challenges or open problems.
	\\
	
	\multirow{3}[0]{*}{~\citep{Tang2016}} & 
    \specialcell{Technical and algorithmic \\ aspect of the work} &
    The authors, instead of using the combination operation to select a global feature subset for all classes, select a specific feature subset for each category, namely class-specific features. The authors still apply existing feature importance in the proposed approach to select candidate features for each class. Because such feature selection gives different feature spaces, the authors use Baggenstoss’s PDF Project Theorem (PPT) to reforulate Bayes decision rule for classification with these selected class-specific features.      
    \\ & 
    \specialcell{Findings/recommendations \\ of the research} & 
    It has been seen that the classification performance increases with more features are selected. The comparison results indicate significant improvements of the proposed approach for a small number of features, when Information Gain, Relevance Score, and Maximum Discrimination feature selection criteria are used. Also, the authors notice that, when more features are selected, the classification performances of all considered methods are improved, leading to a small improvement of the proposed approach. In contrast to the conventional feature selection methods, the proposed method allows to choose the most important features for each class. The one important advantage of the method is that many existing feature selection criteria can be easily incorporated. 
    \\ & 
    \specialcell{Highlighted challenges \\ or open problems} & 
    The authors do not highlighted challenges or open problems. 
	\\
	
	\multirow{3}[0]{*}{~\citep{Tang2016a}} & 
    \specialcell{Technical and algorithmic \\ aspect of the work} &
    In the paper is proposed a novel and efficient feature selection framework based on the Information Theory. The framework aims to rank the features with their discriminative capacity for classification. The authors revisited two information measures: Kullback-Leibler divergence and Jeffreys divergence for binary hypothesis testing, and analyse their asymptotic properties relating to type I and type II errors of a Bayesian classifier. Next, the authors introduced a new divergence measure, called Jeffreys-Multi-Hypothesis (JMH) divergence, to measure multi-distribution divergence for multi-class classification. Based on the JMH-divergence, they developed two efficient feature selection methods, termed maximum discrimination (MD) and $\chi^2$ methods, for text categorisation. 
    \\ & 
    \specialcell{Findings/recommendations \\ of the research} & 
    The authors note that almost all of the filter based feature selection approaches that utilise the information theory measures use binary variables, e.g., the presence or the absence of a term/word in a document, and a document belonging to a category or not. Also, these existing filter approaches rank the features by only exploring the intrinsic characteristics of data based on the feature relevancy without considering their discriminative information in classifiers. The proposed approach overcomes these drawbacks and achieves promising classification results.
    \\ & 
    \specialcell{Highlighted challenges \\ or open problems} & 
    The authors consider exploring how the feature dependence may be used to improve the proposed solution and develop feature selection algorithms by weighting each feature, aiming to maximize the discriminative capacity. The authors want to incorporate the feature selection approaches into other advanced machine learning algorithms such as imbalanced learning [43] [44] and partial learning model [45] [46] to enhance the learning for rare categories.
	\\
	
	\multirow{3}[0]{*}{~\citep{Li2016}} & 
    \specialcell{Technical and algorithmic \\ aspect of the work} &
	The work explores how to enhance the feature selection methods such as Chi-square and Information Gain by considering the importance of a feature in a document. The authors propose the formula that change the values of the feature selection methods parameters from integers to real values, which are thought to carry more fine-grained information. Thus, derived feature selection metchods are expected to perform better.    
    \\ & 
    \specialcell{Findings/recommendations \\ of the research} & 
	The proposed importance weighted feature selection strategy helps the traditional Chi-square and Information Gain metrics obtain better results, especially when using fewer features on imbalanced datasets.
    \\ & 
    \specialcell{Highlighted challenges \\ or open problems} & 
    The work suggests more research with other datasets and apply the proposed importance weighted feature selection strategy into revising other existing feature selection metchods. The authors consider exploring how to better determine the importance of a feature in a document.
	\\
	
	\multirow{3}[0]{*}{~\citep{AlSalemi2016}} & 
    \specialcell{Technical and algorithmic \\ aspect of the work} &
	The proposed RFBoost algorithm is based on filtering a small fixed number of ranked features in each boosting round rather than using all features, as AdaBoost.MH does. The authors proposed two methods for ranking the features: (1) One Boosting Round (OBR), and (2) Labeled Latent Dirichlet Allocation (LLDA), a supervised topic model based on Gibbs sampling. Additionally, the authors investigated the use of LLDA as a feature selection method for reducing the feature space based on the maximal conditional probabilities of words across labels.     
    \\ & 
    \specialcell{Findings/recommendations \\ of the research} & 
	RFBoost is an improved and accelerated version of AdaBoost.MH and MPBoost which proves to be effective and efficient for text categorization. In terms of the classification performance, the results demonstrate that RFBoost with the new weighting policies and the LLDA-based feature ranking significantly outperformed all other evaluated algorithms.  OBR-based feature ranking yielded the worst performance overall. However, the advantage of OBR-based feature ranking is that the features are ranked within the weak learner, and there is no need to use external ranking methods.
    \\ & 
    \specialcell{Highlighted challenges \\ or open problems} & 
    Future works can investigate the use of RFBoost to solve other multi-label classification problems. In addition, the authors want to investigate the use of other feature ranking methods, as feature ranking is the core idea for improving RFBoost’s effectiveness.
	\\
	
	\multirow{3}[0]{*}{~\citep{Wang2016}} & 
    \specialcell{Technical and algorithmic \\ aspect of the work} &
	The authors propose a new feature selection method called Categorical Document Frequency Divided by Categorical Number (CDFDC), which improves other feature selection method called Categorical Document Frequency (CDF). The idea of the proposed method bases on adding information about categories including a given term/word in the original formula of CDF to increase terms' discrimination.    
    \\ & 
    \specialcell{Findings/recommendations \\ of the research} & 
	The authors found that CDFP VM method~\citep{Li2014a} does not perform significantly better than the other feature selection methods in English date set, especially not better than CDF. The high computation complexity may not bring high precision or recall rate, except that we need stable and predictable time efficiency.
    \\ & 
    \specialcell{Highlighted challenges \\ or open problems} & 
    In the future work the authors want to  to experiment on larger data set and find the connection between algorithm complexity and documents-time-efficiency.
	\\
	
	\multirow{3}[0]{*}{~\citep{Zong2015}} & 
    \specialcell{Technical and algorithmic \\ aspect of the work} &
	The authors create two stages feature selection method called DFS+Similarity for text categorization. First, the proposed method selects features with strong discriminative power. In the second stage the method considers the semantic similarity between features and documents based on the term frequency-inverse document frequency (TF-IDF) weight which is also modified by calculated semantic similarity finally. Finally, on the top of the method, Support Vector Machines utilise the reweighted and pruned feature space to learn a classification model.    
    \\ & 
    \specialcell{Findings/recommendations \\ of the research} & 
	On the 20-Newsgroups dataset, all the five methods Discriminative features selection (DFS), "DFS+Similarity", Chi Square ($\chi2$) statistic, information gain (IG) and mutual information (MI)) produce their worst results when the number of features is the lowest (1000). Amongst the five methods, MI and IG perform relatively poorer than others. Also, MI and IG are also sensitive to the number of features. Some text categories in the 20-Newsgroups dataset are very similar. Examples of such similarities include the ""comp.sys.ibm.pc.hardware"" and ""comp.sys.mac.hardware"" categories, and the ""talk.politics.misc"" and ""talk.religion.misc"" categories. This obviously makes the features between different categories more difficult to distinguish."
    \\ & 
    \specialcell{Highlighted challenges \\ or open problems} & 
    Future work can consider the characteristics of features distribution, such as the position and compactness of features in each category of documents. Also, the other research direction may take into account the multi-label text categorization problem in feature selection.
	\\
	
	\multirow{3}[0]{*}{~\citep{Feng2015}} & 
    \specialcell{Technical and algorithmic \\ aspect of the work} &
	The authors create the optimal set of features that is characterized by global and local section index for a group and single feature respectively. Under the naive Bayes assumption, the Global Selection Index is factorized into the Local Selection Indexes, which can in turn be explicitly calculated by the posterior feature selection probability in its own dimension. So, as a result, the high dimension can be reduced to the dimension of each feature itself. Finally, the $2^p$ possible feature subset selection models can be pruned by applying a cutoff for the LSIs to balance the bias and variance, and the prediction function using Bayesian model averaging can be obtained as a product of much less than p single feature model averages. After modeling the uncertainty of feature subset selection, the author propose a latent selection augmented naive (LSAN) Bayes classifier to provide a good fit to the data.     
    \\ & 
    \specialcell{Findings/recommendations \\ of the research} & 
	Comparing with the original naive Bayes classifier, the LSAN classifier keeps the promptness of the computation, improves the classification accuracy distinctly, and is competitive with state-of-the-art classifiers such as the SVM. Feature selection and feature weighting can be combined organically in the proposed classifier. The authors conclude that, the proposed model calculates the strength of relevance to the label for each feature, and gives a probabilistic weighted classifier based on Bayesian model averaging. Furthermore, the high dimension can be reduced in this model when dealing with not only the feature selection indexes, but also the future prediction.
    \\ & 
    \specialcell{Highlighted challenges \\ or open problems} & 
    Future works can consider feature selection and feature weighting under the multinomial naive Bayes, multivariate Poisson model, and combined models (NB with SVM) to improve performance. Future work should be focus on to find the optimal value of the LSI truncation threshold, and discover the corresponding laws for feature selection in text classification. A possible way is to test the statistical significance of the LSAN classifier as suggested to test the significance of the naive Bayes classifier.
	\\
	
    \multirow{3}[10]{*}{~\citep{Li2015}} & 
    \specialcell{Technical and algorithmic \\ aspect of the work} & 
    The authors propose method called Weighted Document Frequency (WDF)) to create a feature ranking. The ranking bases on the information about how the feature is essential for a document. The importance of a feature in the document is based on two weighting schema TF and TF-IDF.   
    \\ & 
    \specialcell{Findings/recommendations \\ of the research} & 
	The proposed method outperforms a baseline method called document frequency (DF), but there is no difference between the proposed method and Chi-Square method.  	
	\\ & 
	\specialcell{Highlighted challenges \\ or open problems} & 
	The work suggests more research with other datasets and applies the proposed feature selection metric into other text mining applications, e.g. text clustering, sentiment analysis. The authors plan to explore further why WDF based on TF-IDF weight does not perform better than WDF, which based on TF weight schema. 
	\\
    	
	\multirow{3}[0]{*}{~\citep{Rehman2015}} & 
    \specialcell{Technical and algorithmic \\ aspect of the work} &
    The work explores a new feature ranking metric termed as relative discrimination criterion (RDC). The metric takes document frequencies for each term count of a term into account while estimating the usefulness of a term. The authors noted two things (1) existing feature ranking metrics for text data determine the rank of a term by comparing its document frequencies in the positive and negative classes, and (2) metrics ignore term counts and consider only presence or absence of a term in a document. So, they proposed the metric enhances ranks of the terms present in only one class or the terms for which the term counts in one class are relatively high then the other class. The metric considers not only the document frequency but also the term counts to determine the rank of a term.    
    \\ & 
    \specialcell{Findings/recommendations \\ of the research} & 
    The proposed feature selection strategy helps to choose better features as RDC generates the highest micro and macro F1 values in the most experiment cases. The RDC measure requires somewhat more computations than the other compared feature ranking metrics. However, the authors highlight that these computations can be run independently of each other in a parallel way.
    \\ & 
    \specialcell{Highlighted challenges \\ or open problems} & 
    The work suggests more research with other datasets varying in size, class skew and sparseness. The authors consider investigation to make the term counts more effective in contributing to determining the term rank. Also, the author highlight that RDC’s performance can be enhanced by tuning different parameters. So, finding the optimal set of parameters for RDC can be another research direction. The authors wnt to work on modifications needed for the application of RDC on non-text datasets and compare the performance of RDC with other feature ranking measure for non-text datasets.
	\\
	
	\multirow{3}[0]{*}{~\citep{Yan2008}} & 
    \specialcell{Technical and algorithmic \\ aspect of the work} &
    The authors integrate the two algorithm categories, i.e. feature selection and feature extraction, by using the created optimisation framework. Under this framework, the authors develop a novel feature selection algorithm called Trace Oriented Feature Analysis (TOFA).  The novel feature selection algorithm optimises the feature extraction objective function in the solution space of feature selection algorithms.     
    \\ & 
    \specialcell{Findings/recommendations \\ of the research} & 
	The authors showed that many commonly used algorithms in several previous works are special cases of the proposed unified objective function.  In contrast to many of the previous greedy feature selection algorithms, the proposed feature selection algorithm can achieve the optimal solution according to its objective function. TOFA is suitable for large scale text data, and it can handle the unsupervised and semi-supervised cases which cannot be solved by traditional algorithms such as Information Gain (IG).
    \\ & 
    \specialcell{Highlighted challenges \\ or open problems} & 
    In the future, the authors want to analyze the relationship between data distribution and framework's parameter and give information about how to select the optimal setting, i.e. select an optimal objective function according to the data distribution without cross-validation.
	\\
	
	\multirow{3}[0]{*}{~\citep{Tesar2006}} & 
    \specialcell{Technical and algorithmic \\ aspect of the work} &
    The authors propose a suffix tree based algorythm to discover itemset where itemset contains different and valulable words. In the work, the authors focus attention on the comparison between using word n-grams and using itemsets in text categorisation task mainly in terms of classification accuracy.     
    \\ & 
    \specialcell{Findings/recommendations \\ of the research} & 
	The bigrams seems to be more suitable for text classification and all feature selection methods. Generally, for any data collection, a feature subset selection approach should be determined on the basis of the principle of the classifier which is used. The employed many bigrams and 2-itemsets to extend the BOW document model representation did not overcome the best results achieved by the simple BOW approach.
    \\ & 
    \specialcell{Highlighted challenges \\ or open problems} & 
    The authors suppose that for a more succesfull text classifier (e.g. SVM) the improvements obtained by incorporating bigrams or 2-itemsets will be generally less noticeable and not very contributing. The authors expect that due to the statistical sparseness, high level n-grams and itemsets cannot affect the text classification performance significantly.
	\\

    \hline
     \label{tab:fsm}
    \end{longtable}%
  %\label{tab:addlabel}%
%\end{table}%
\end{landscape}