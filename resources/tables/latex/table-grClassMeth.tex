\begin{landscape}
% Table generated by Excel2LaTeX from sheet 'Sheet 1'
%\begin{table}[htbp]
% \centering
 % \caption{Known and used classification method.}
    \begin{longtable}{lp{.3\textwidth}p{.8\textwidth}}
    \caption{Known and used classification method.} \\
    \hline    
    Reference & \multicolumn{1}{c}{Aspect of work} & \multicolumn{1}{c}{Description} \\
	\hline
	
	\multirow{3}[0]{*}{~\citep{Tiwari2019}} & 
    \specialcell{Technical and algorithmic \\ aspect of the work} &
	The authors propose a novel classification method called Quantum-Inspired Binary Classifier (QIBC). This classifier can resolve a binary classification problem, i.e. the classification case where we have two labels. Following by the authors' description - the QIBC for each topic (category) supposes that each training sample is about the class or not. For a given category and the set of training samples, the method uses the projector, i.e. a detection operator which must be the one whose eigenvalues are 0 and 1, for each category to identify whether the test sample is about the class or not. To determine whether the test sample is about the class, 1 is examined against a vectorial representation of the test sample.  
    \\ & 
    \specialcell{Findings/recommendations \\ of the research} & 
	QIBC can outperform the baselines for several categories. So, the quantum-based approach seems to be adopted to solve a classification problem. However, some results are still unsatisfactory for some categories (classes). 
    \\ & 
    \specialcell{Highlighted challenges \\ or open problems} & 
	The authors want to conduct in-depth error classification analysis, i.e. they want to understand better the reason as to why the proposed model fails for some classes and performs better for other classes. The authors addressed binary classification problem. In future, they want to try to implement a multi-classifier inspired by quantum detection theory for multi-class classification problems. The authors highlight that they are working on multi-label classification tasks to come up with multi-label classifiers.
	\\
	
	\multirow{3}[0]{*}{~\citep{Berge2019}} & 
    \specialcell{Technical and algorithmic \\ aspect of the work} &
	The authors experimented with the text representation where the terms of a text are treated as propositional variables. From these variables, the authors captured categories using simple propositional formulae, such as: IF ``rash'' AND ``reaction'' AND ``penicillin'' THEN Allergy. The Tsetlin Machine learns these formulae from the labelled text, utilizing conjunctive clauses to represent the particular facets of each category. Therefore, the absence of terms (negated features) also can be used for categorization purposes.    
    \\ & 
    \specialcell{Findings/recommendations \\ of the research} & 
	The study shows that the proposed method captures categories using simple propositional formulae that are also easy to interpret for humans. The authors observed that the explanatory power of the Tsetlin Machine-produced clauses seems to equal that of decision trees.
    \\ & 
    \specialcell{Highlighted challenges \\ or open problems} & 
	The authors plan to examine how to use the Tsetlin Machine for unsupervised learning of word embeddings. The authors want to investigate how the sparse structure of documents can be taken advantage of to speed up learning. The authors plan to leverage local context windows to learn structure in paragraphs and sentences for more precise text representation. The authors are interested in how structured medical data can be combined with the medical narrative to support precision medicine. The authors plan to do experiments with the Tsetlin Machine on datasets that are unsolvable for linear algorithms.
	\\
	
	\multirow{3}[0]{*}{~\citep{Unnikrishnan2019}} & 
    \specialcell{Technical and algorithmic \\ aspect of the work} &
	The work presents a comparative study of different sparse classification strategies for text classification with an empirical demonstration of the effectiveness of this class of algorithms on text data. The work proposes a dynamic class-wise dictionary selection algorithm and a dictionary refinement algorithm which addresses the computational complexity aspect along with improving the classification performance of the best performing algorithm as observed in the comparative study.    
    \\ & 
    \specialcell{Findings/recommendations \\ of the research} & 
	The study shows the minimum reconstruction error criterion (SRC-3) is the suitable one for the problem of text classification and the authors observed that when the minimum reconstruction error criterion is used with classwise dictionaries (SRC-4), the results are even better. The computational bottle-neck can be resolved thanks to a dictionary refinement procedure. The proposed refinement procedure produces a more compact dictionary which more effectively represents the class characteristics and hence yields better and more effective sparse codes as evidenced by the empirical outcomes.
    \\ & 
    \specialcell{Highlighted challenges \\ or open problems} & 
    The authors do not highlighted challenges or open problems.
	\\
		
	\multirow{3}[0]{*}{~\citep{Pappagari2018}} & 
    \specialcell{Technical and algorithmic \\ aspect of the work} &
	The work explores and examines the new multi-scale Convolutional Neural Network (CNN) architecture for document classification using raw text as a input, i.e. the authors use one-hot encoding and do not ustilise any pre-created embeding such as word2vec or Glove. The proposed architecture contains parallel convolutional layers and optimises jointly the new objectctive function. The new objective function puts into account and optimises at the same time two tasks: (1) verification task where is deciding whether two given documents are from the same class, and (2) document classification tasks. Thanks to that, the authors provide gains in the document classification performance.     
    \\ & 
    \specialcell{Findings/recommendations \\ of the research} & 
	The objective function that integrating the verification and identification tasks improves identification task. The proposed approach works on raw text and do not consider any kind of ad-hoc techniques such as feature weighting, or vocabulary selection for achive better document classification performance.
    \\ & 
    \specialcell{Highlighted challenges \\ or open problems} & 
    The authors want to explore incorporating sequence dynamics modeling with Long short-term memory (LSTM) into this framework.
	\\
	
	\multirow{3}[0]{*}{~\citep{AlSalemi2018}} & 
    \specialcell{Technical and algorithmic \\ aspect of the work} &
	See Table~\ref{}
    \\ & 
    \specialcell{Findings/recommendations \\ of the research} & 
	See Table~\ref{}  
    \\ & 
    \specialcell{Highlighted challenges \\ or open problems} & 
	See Table~\ref{}
	\\
	
	\multirow{3}[0]{*}{~\citep{Feng2017}} & 
    \specialcell{Technical and algorithmic \\ aspect of the work} &
    The authors consider the overfitting problem and propose a quantitative measurement, rate of overfitting, denoted as RO, where the parameters, such as testing error, training error, training size or iteration time are taken into account to establish and measure how the given method is overfitted. Furthermore, the authors address the problem of overfitting of the extreme learning machine (ELM). They propose an algorithm called AdaBELM, which is based on the idea of weak classifiers compilation from AdaBoost to resolve this drawback. They show that thanks to such approach the ELM is not overfitted and better than baseline ELM.  
    \\ & 
    \specialcell{Findings/recommendations \\ of the research} & 
    The authors noted that ELM suffers from a significant overfitting problem, which is due to the output weights being computed by the Moore-Penrose generalized inverse, which is a least-squares minimization issue. The proposed model AdaBELM resolve the drawback mentioned above. The proposed model has good generalizability, as revealed by its high performance on balanced, unbalanced, and real application data.
    \\ & 
    \specialcell{Highlighted challenges \\ or open problems} & 
    The authors do not highlighted challenges or open problems.
	\\
	
	\multirow{3}[0]{*}{~\citep{Sharma2017a}} & 
    \specialcell{Technical and algorithmic \\ aspect of the work} &
    The work explores the idea of sparse coding for text classification and seeding the dictionary using the principal components of all the training documents. The work investigates how the proposed hierarchical classifier outperforms other solutions. The paper checks how the weighted decomposition principal component analysis technique: (1) performs in construction of the dictionary, and (2) influences on the discrimination among confusing classes.     
    \\ & 
    \specialcell{Findings/recommendations \\ of the research} & 
	The principal components analysis may be used to create an overcomplete dictionary for text classification purpose. Hierarchical classification based on the sparse coding classifiers which the weighted decomposition principal component analysis to construct dictionary technique works better than shallow/flat classification based on the sparse coding classifier because of confusing documents of similar type of classes affects the performance of parse coding classifier, but after grouping such classes, that problem gets resolved. The weighted decomposition principal component analysis works well in the field of document classification, but this approach needs a very good grouping technique as the weighted decomposition principal component analysis is used to discriminate between similar type of documents.
    \\ & 
    \specialcell{Highlighted challenges \\ or open problems} & 
	The work suggests more research with other datasets. Possibly using the semantic information may also improve the performance of text classification.
	\\
	
	\multirow{3}[0]{*}{~\citep{Benites2017}} & 
    \specialcell{Technical and algorithmic \\ aspect of the work} &
    The work explores a scalable extension to the fuzzy Adaptive Resonance Associative Map (ARAM) neural network, which was specially developed for the quick classification of high-dimensional and large data. This extension aims at increasing the classification speed by adding an extra layer for clustering learned prototypes into large clusters. This enables the activation of only one or a few clusters, i.e. a small fraction of all prototypes, reducing the classification time significantly. Further, the authors introduce two methods to adopt this extension to a multi-label classification task.   
    \\ & 
    \specialcell{Findings/recommendations \\ of the research} & 
	The study shows that in comparison with ARAM, Hierarchical ARAM (HARAM) is faster. The performacnce in term of accuracy may increase when we use a voiting classification procedure. A major advantage of Adaptive Resonance Theory (ART) neural networks nowadays is that they can be highly parallelized using GPUs and multi-core CPUs.
    \\ & 
    \specialcell{Highlighted challenges \\ or open problems} & 
	The authors noted that the details of implementation can be an issue.
	\\
	
	\multirow{3}[0]{*}{~\citep{Sharma2016}} & 
    \specialcell{Technical and algorithmic \\ aspect of the work} &
    The work explores the idea of sparse coding for text classification and seeding the dictionary using the principal components of all the training documents. Also, the work explores frequency-based kernels such as HIK, Chi-kernel and Hellinger’s kernel for text classification using Support Vector Machines. The paper introduces a simple technique for combining the classifiers for text classification, i.e. presents the voting scheme.    
    \\ & 
    \specialcell{Findings/recommendations \\ of the research} & 
    The principal component analysis can be used to create the overcomplete dictionary, and based on such the dictionary, we may achieve good classification results comparable to the method without PCA. Support Vector Machines with Hellinger’s kernel and without PCA transformation of data set made the best results. The study demonstrates that effectiveness in text classification may be increased by combining sparse representation classification strategies and Support Vector Machines based classifiers, i.e. voting procedure. The work observes that misclassification in 20 Newsgroup corpus can be attributed to the presence of highly confusing classes. It is also found that complicated classes are also semantically related.
    \\ & 
    \specialcell{Highlighted challenges \\ or open problems} & 
    The work suggests the classifier built using the semantic information may improve the performance of text classification.  The approach explored for combining the classifiers can be extended to any the classifiers. Better strategies for combining the classifiers need to be explored.
	\\
	
	\multirow{3}[0]{*}{~\citep{Jin2016}} & 
    \specialcell{Technical and algorithmic \\ aspect of the work} &
	The authors built a text classifier by using a Naive Bayes model with utilising a new structure called bag-of-embeddings probabilities. The structure is a set of word embeddings created based on text classes. The authors assume that words exhibit different distributional characteristics under different text classes. Based on this assumption, they train distributional word representations for different text classes. The text class, for a new document, is predicted by maximising the probabilities of embedding vectors of its words under the category. The authors note that compared with bag-of-words models (vector representation of documents), the bag-of-embeddings model exploits contextual information by deriving the probabilities of class sensitive embedding vectors from their inner product with context words.
    \\ & 
    \specialcell{Findings/recommendations \\ of the research} & 
	The created model is conceptually simple, with only parameters being embedding vectors, trained using a variation of the skip-gram method. Experiments on two standard datasets showed that the proposed model outperforms state-of-the-art methods for both balanced and imbalanced data.   
    \\ & 
    \specialcell{Highlighted challenges \\ or open problems} & 
	The authors want to consider leveraging unlabeled data for semi-supervised learning. The authors want to exploit neural documents models, which contains richer parameters spaces by capturing word relations, for potentially better accuracies.
	\\
	
	\multirow{3}[0]{*}{~\citep{AlSalemi2016}} & 
    \specialcell{Technical and algorithmic \\ aspect of the work} &
    See Table~\ref{}
    \\ & 
    \specialcell{Findings/recommendations \\ of the research} & 
	See Table~\ref{}
    \\ & 
    \specialcell{Highlighted challenges \\ or open problems} & 
	See Table~\ref{}
	\\
	
	\multirow{3}[0]{*}{~\citep{Pang2015}} & 
    \specialcell{Technical and algorithmic \\ aspect of the work} &
	The authors propose a new classification method called CenKNN. The approach combines the strengths of two widely used text classification techniques, k-Nearest Neighbours (k-NN) and centroid based (Centroid) classifiers. First, CenKNN projects high-dimensional (often hundreds of thousands) documents into a low-dimensional (normally a few dozen) space spanned by class centroids. Second, the method uses the k-d tree structure to find K nearest neighbours to classify a new example efficiently.
    \\ & 
    \specialcell{Findings/recommendations \\ of the research} & 
	The study shows that the CenKNN overcomes two issues related to existing KNN text classifiers, i.e., sensitivity to imbalanced class distributions and irrelevant or noisy term features. CenKNN creates and works on the projected low-dimensional data, thanks to that CenKNN reduces the expensive computation time in k-NN substantially. Also, CenKNN works better than Centroid since it uses all the class centroids to define similarity and works well on complex data, i.e., non-linearly separable data and data with local patterns within each class. The proposed method is also empirically preferable to another well-known classifier, Support Vector Machines, on highly imbalanced corpora with a small number of classes. SVM is a better choice for large balanced corpora in terms of classification.  
    \\ & 
    \specialcell{Highlighted challenges \\ or open problems} & 
	The authors want to improve CenKNN to handle sub-clusters and/or a large number of classes, e.g., through the use of hierarchical classification and class hierarchy. Also, the authors want to extend the work to multi-label text classification.
	\\
	
	\multirow{3}[0]{*}{~\citep{Kusner2015}} & 
    \specialcell{Technical and algorithmic \\ aspect of the work} &
    The authors created a distance function (Word Mover’s Distance, WMD) which measures the dissimilarity between two text documents as the minimum amount of distance that the embedded words of one document need to ``travel'' to reach the embedded words of another document. The authors showed that this distance metric could be cast as an instance of the Earth Mover’s Distance, a well-studied transportation problem for which several highly efficient solvers have been developed. The advantage of the proposed method is that it has no hyperparameters and is straight-forward to implement.      
    \\ & 
    \specialcell{Findings/recommendations \\ of the research} & 
    The proposed metric method leads to low error rates across all investigated data sets. The proposed WMD yields by far the most accurate classification results, the authors fair to say that it is also the slowest metric to compute. However, they also show the solution to speed up the proposed distance computations.
    \\ & 
    \specialcell{Highlighted challenges \\ or open problems} & 
    The authors want to explore the interpretability of the method. The authors noted that document distances could be dissected into sparse distances between words, which can be visualized and explained to humans. Also, they want to incorporate document structure into the distances between words by adding penalty terms if two words occur in different sections of similarly structured documents. If, for example, the WMD metric is used to measure the distance between academic papers, it might make sense to penalize word movements between the introduction and method section more than word movements from one introduction to another. 
	\\
	
	\multirow{3}[0]{*}{~\citep{Feng2015}} & 
    \specialcell{Technical and algorithmic \\ aspect of the work} &
    See Table~\ref{}
    \\ & 
    \specialcell{Findings/recommendations \\ of the research} & 
	See Table~\ref{} 
    \\ & 
    \specialcell{Highlighted challenges \\ or open problems} & 
	See Table~\ref{}
	\\
	
	\multirow{3}[0]{*}{~\citep{Gomez2014}} & 
    \specialcell{Technical and algorithmic \\ aspect of the work} &
    The authors create a new classification algorithm that utilises the property of the Principal Component Analysis (PCA). During the training phase, the documents of each class are used to create a model that can transform the documents to lower space and for this purpose is used PCA. Thanks to that we receive PCA model for each class. During testing, given a new unseen document, each model projects such document to lower space and, then reconstructs the document using each one of the models again and finally computes the reconstruction errors, by measuring the Frobenius norm of the difference between the set of reconstructed documents and the original one. The model which produces the smallest error indicates the category to be assigned to the new document. Shortly, the process of inferring about the document's belonging to a class (the classification model) involves finding the smallest reconstruction error value.  
    \\ & 
    \specialcell{Findings/recommendations \\ of the research} & 
	The authors observed that for 20 newgroup dataset the skewness is low, but the entropy is high, meaning the distribution of documents along the categories is soft, but the uncertainty of such distribution is high. The proposed method creates a model that well generalise the classification problem, i.e. the created model works well when we have a limited number of the training examples. Results have shown that the mRE classifier performs well in terms of accuracy (or micro-F1) and macro-F1. The selection of the number of principal components is an essential issue for the mRE classifier. The authors from the experiments observed that for datasets with high entropy and especially for datasets with high skewness, the mRE classifier generally performs better than the rest of the classifiers.
    \\ & 
    \specialcell{Highlighted challenges \\ or open problems} & 
	The authors want to apply the mRE method to other text classification tasks. In particular, they are interested in using it for hierarchical classification in large datasets. The authors suggest that the mRE could be used inside the taxonomy to model the internal categories. It would be interesting as well, to combine the output prediction of the mRE with other classifiers to refine the final prediction.
	\\
	
	\multirow{3}[0]{*}{~\citep{Lo2012}} & 
    \specialcell{Technical and algorithmic \\ aspect of the work} &
	The work explores the background net~\citep{Chen2011, Lo2011} and a set of different reasoning methods created on top of the net to resolve a document classification task. The main idea is to create a structure called bacground net (a graph presetns contextual association between terms) and after learning of such structure is performed probabilistic reasoning  to classify new document.    
    \\ & 
    \specialcell{Findings/recommendations \\ of the research} & 
	The proposed method gives a good performance without requiring a great effort in preprocessing. 
    \\ & 
    \specialcell{Highlighted challenges \\ or open problems} & 
    The authors shall further study how to obtain fuzzy association between terms based on granules of articles to achieve a more flexible and powerful approach.
	\\
	
	\multirow{3}[10]{*}{~\citep{Sainath2010}} & 
    \specialcell{Technical and algorithmic \\ aspect of the work} & 
    The goal of the work is to show that a proposed solution applies the idea of sparse coding for text classification without focuses on any feature selection techniques may achieve promising results. The work explores seeding the dictionary using all training documents. The work compares three different frameworks of using the sparse coding solution to generate an actual classification decision. The work explores the sensitivity of the sparse coding method to the vocabulary size of the training documents. The authors investigate how the accuracy of the sparse coding technique changes with varied vocabulary size.   
    \\ & 
    \specialcell{Findings/recommendations \\ of the research} & 
	The study shows that using all training documents not only makes the size of the dictionary much larger but also enforces a stronger need for sparseness on coefficients. The study shows that sparse coding method offers slight but promising results over the Naive Bayes classifier, thus introducing an alternative class of methods that work as well as the standard Naive Bayes approach often used for text categorization.  	
	\\ & 
	\specialcell{Highlighted challenges \\ or open problems} & 
	The work proposes the exploration of feature selection techniques, and comparison the achieved results to Support Vector Machines. 
	\\
	
	\multirow{3}[0]{*}{~\citep{Li2010}} & 
    \specialcell{Technical and algorithmic \\ aspect of the work} &
    The authors improve multiclass text classification thanks to using error-correcting output coding with sub-class partitions. Error-Correcting Output Coding (ECOC) is a general framework for multiclass text classification with a set of binary classifiers. The authors note that, when training such a binary classifier, sub-class distribution within positive and negative classes is neglected and they try to utilise this information to improve a binary classifier. To this purpose, the authors design a simple binary classification strategy via multiclass categorisation (2vM) to make use of sub-class partition information, which can lead to better performance over the traditional binary classification. The proposed binary classification strategy is then applied to enhance ECOC.  
    \\ & 
    \specialcell{Findings/recommendations \\ of the research} & 
    In ECOC, sub-class partition information of positive and negative classes is available but ignored even though it has value for binary classification. No one algorithm can win on every dataset and situation.
    \\ & 
    \specialcell{Highlighted challenges \\ or open problems} & 
    The authors plan to conduct more experiments on more datasets. Also, they assume that the proposed method may be useful for non-text applications, but it needs to be verified with extensive experiments. Also, the authors plan to explore other local search algorithms to improve the proposed strategy further.
	\\
	
	\multirow{3}[0]{*}{~\citep{Jin2010}} & 
    \specialcell{Technical and algorithmic \\ aspect of the work} &
	The authors create a new classification which is based on prototype learning, where training data is represented as a set of points (prototypes) in a feature space. A test point \textit{x} is then classified to the class of the closest prototype. To this purpose, the authors investigate the loss functions of prototype learning algorithms and aim to improve the classification performance using a new form of the loss function with regularisation term.  
    \\ & 
    \specialcell{Findings/recommendations \\ of the research} & 
	The authors observed that the proposed method produces larger average hypothesis margin than the other prototype learning algorithms.
    \\ & 
    \specialcell{Highlighted challenges \\ or open problems} & 
	The authors highlight that the method as a learning criterion can also be applied to other classifier structures based on gradient descent, such as neural networks and quadratic discriminant functions.
	\\
	
	\multirow{3}[0]{*}{~\citep{Xia2009}} & 
    \specialcell{Technical and algorithmic \\ aspect of the work} &
    The work explores the linear classification approach, i.e. during the training process, is computed the matrix of scores (the contribution table). In the row are placed terms and in the columns are placed the classes of documents. For each word and class is compute a value of how the term contributes to the class. For each term in a test document, the authors look up the ratings it contributes to all categories, and combine the scores of different terms for the same class and then classify the document into the group with the largest score combination.  
    \\ & 
    \specialcell{Findings/recommendations \\ of the research} & 
	The proposed method has a less time complexity, thus is more efficient. The method do not need to know the semantic contribution of a term makes to a document it occurs in.
    \\ & 
    \specialcell{Highlighted challenges \\ or open problems} & 
	The work authors gives for all terms in feature space different weights, but they notice that by making use of more knowledge of the field of natural language processing, improvements can still be made. 
	\\
	
	\multirow{3}[0]{*}{~\citep{Larochelle2008}} & 
    \specialcell{Technical and algorithmic \\ aspect of the work} &
    The authors incorporate labels into the training process of the Restricted Boltzmann Machines (RBMS). The authors propose two models: (1) Discriminative Restricted Boltzmann Machines (DRBMs), i.e. RBMs that are trained more specifically to be good classification models, and (2) Hybrid Discriminative Restricted Boltzmann Machines (HDRBMs) which explore the space between discriminative and generative learning and can combine their advantages. These discriminative versions of RBMs integrate the process of discovering features of inputs with their use in classification, without relying on a separate classifier. This ensures that the learned features are discriminative and facilitates model selection.     
    \\ & 
    \specialcell{Findings/recommendations \\ of the research} & 
    The authors argued that RBMs can and should be used as stand-alone non-linear classifiers alongside other standard and more popular classifiers, instead of merely being considered as simple feature extractors. The analysis of the target weights for the 20-newsgroup dataset seems to indicate that RBMs would be good at capturing the conditional statistical relationship between multiple tasks or in the components in a complex target space.
    \\ & 
    \specialcell{Highlighted challenges \\ or open problems} & 
    The authors want to investigate the use of discriminative versions of RBMs in more challenging settings such as in multi-task or structured output problems.  The authors note that exact computation of the conditional distribution for the target is not tractable anymore, but there exist promising techniques such as mean-field approximations that could estimate that distribution. The authors wan to intend to explore ways to introduce generative learning in RBMs and HDRBMs, which would be less computationally expensive when the input vectors are large but sparse.  
    \\
	
	\multirow{3}[0]{*}{~\citep{Genkin2007}} & 
    \specialcell{Technical and algorithmic \\ aspect of the work} &
    The authors propose to use a Laplace prior regularisation term in Bayesian logistic regression approach. Thanks to that they achieve the classification model which produces a sparseness model, i.e. the model which has many zero parameters so that it can avoid overfitting. Furthermore, the authors describe an optimisation method of the proposed method - the optimisation solution bases on the Cyclic coordinate descent (CLG) algorithm. The algorithm was adopted for the Laplace prior regularisation term.   
    \\ & 
    \specialcell{Findings/recommendations \\ of the research} & 
	Feature selection often (although not always) improved the effectiveness of ridge logistic regression. In no case did additional feature selection improve the macro averaged F1 of lasso logistic regression. The authors noted that feature selection sometimes improved the results of SVM models with default thresholds, but never improved the results of SVM models with tuned thresholds. So, SVMs built-in regularization is quite effective. The authors found a strong correlation between the number of positive training examples and the number of features chosen.
    \\ & 
    \specialcell{Highlighted challenges \\ or open problems} & 
    The authors noted that there are the limitations of the lasso as a feature selection algorithm. So, in future work, they will explore alternatives such as Meinshausen's \textit{relaxed lasso}. The authors highlighted that several algorithms that estimate \textit{regularization paths} have emerged in recent years (see, e.g., Park and Hastie 2006). These algorithms have the attractive property of estimating coefficients for all possible hyperparameter values. However, scaling such algorithms to huge applications remains challenging.
	\\
	
	\multirow{3}[0]{*}{~\citep{Qian2007}} & 
    \specialcell{Technical and algorithmic \\ aspect of the work} &
	The work explores the Associative Text Categorization (ATC) concept to produce semantic-aware classifier, which includes understandable rules for text categorization. For this purpose, the authors use hyperclique patterns rather than frequent patterns/frequent itemset and thanks to that they achieve a strong connection between the overall similarity of a set of objects/documents and the itemset/words in which they are involved.  
    \\ & 
    \specialcell{Findings/recommendations \\ of the research} & 
	The work confirms an observation from earlier research~\cite{Joachims1997} that text categorization can benefit from larger vocabulary size. Also, this is true for associative text categorization. The improvement of micro-BEP is at the cost of losing efficiency, and this drawback can be resolved thanks to the proposed method, i.e. a vertical rule-pruning method, which can greatly help reduce the computational cost.
    \\ & 
    \specialcell{Highlighted challenges \\ or open problems} & 
	The authors do not highlighted challenges or open problems.
	\\
	
	\multirow{3}[0]{*}{~\citep{Gliozzo2005}} & 
    \specialcell{Technical and algorithmic \\ aspect of the work} &
	The authors create the algorithm that can be applied to any categorization problem in which categories are described by initial sets of discriminative features and an unlabeled training data set is provided. The proposed algorithm utilizes a generalized similarity measure based on Latent Semantic Spaces and a Gaussian Mixture algorithm as a principled method to scale similarity scores into probabilities. Both techniques address inherent limitations of the (IL) setting, and leverage unsupervised information from an unlabeled corpus.     
    \\ & 
    \specialcell{Findings/recommendations \\ of the research} & 
	The authors observed that we can obtain competitive performance using only the category names as initial seeds. This minimal information per category, when exploited by the IL algorithm, is shown to be equivalent to labelling about 70-160 training documents per-category for the state of the art extensional learning.
    \\ & 
    \specialcell{Highlighted challenges \\ or open problems} & 
	The authors want to investigate optimal procedures for collecting seed features and find out whether additional seeds might still contribute to better performance. The authors note that the interesting topic is an exploration of optimal combinations of intensional and extensional supervision, provided by the user in the forms of seed features and labelled examples.
	\\
		
	\multirow{3}[0]{*}{~\citep{Zhang2005}} & 
    \specialcell{Technical and algorithmic \\ aspect of the work} &
	The authors assumed that it makes more sense to view document feature vectors as points in a Riemannian manifold, rather than in the much larger Euclidean space. So, they studied kernel on the multinomial manifold that enables SVMs to effectively exploit the intrinsic geometric structure of text data that leads to improving text classification accuracy.    
    \\ & 
    \specialcell{Findings/recommendations \\ of the research} & 
	The authors proved that the Negative Geodesic Distance (NGD) on the multinomial manifold is a conditionally positive definite (cpd) kernel, and it leads to accuracy improvements over kernels assuming Euclidean geometry for text classification. Linear kernel and TF-IDF with L2 regularisation achieve the second result.
    \\ & 
    \specialcell{Highlighted challenges \\ or open problems} & 
    The authors want to extend Negative Geodesic Distance (NGD) kernel to other manifolds (particularly for multimedia tasks) and apply it in other kernel methods for pattern analysis, such as kernel PCA, kernel Spectral Clustering.  
	\\
	
	\multirow{3}[0]{*}{~\citep{Li2004}} & 
    \specialcell{Technical and algorithmic \\ aspect of the work} &
    The authors note that the class distribution in a training set is not even and some classes may have more samples than others, especially when we have an extremely skewed dataset. Furthermore, they highlight that the performance of traditional k-nn algorithms is susceptible to the choice of the parameter k and it is very likely that a fixed k value will result in a bias for large categories, and will not make full use of the information in the training set. The authors propose an improved and adaptive k-NN strategy to deal with these problems. More precisely, they propose two new solutions/decision functions called $ADPT_1$ and $ADPT_2$. The improved strategy takes into account different numbers of nearest neighbours for different categories instead of a fixed number across all categories. The numbers of nearest neighbours selected for different categories are adaptive to their sample size in the training set.   
    \\ & 
    \specialcell{Findings/recommendations \\ of the research} & 
	The proposed methods are less sensitive to the parameter k than the traditional ones, and can adequately classify documents belonging to smaller classes with a large k. The proposed strategy is especially applicable and promising for cases where estimating the parameter k via cross-validation is not possible, and the class distribution of a training set is skewed.
    \\ & 
    \specialcell{Highlighted challenges \\ or open problems} & 
    The authors want to conduct experiments to deal with the multi-label categorisation problem. Also, they want to examine another interesting topic: the question of how to evaluate a dataset for text categorisation. This research may give some guidelines on how to build a useful training collection for text categorisation.
	\\
	
	\multirow{3}[0]{*}{~\citep{Rennie2003}} & 
    \specialcell{Technical and algorithmic \\ aspect of the work} &
	The work explores how the given validation method performs on a selection of regularisation parameter of classification method called Regularized Least Squares Classification (RLSC).      
    \\ & 
    \specialcell{Findings/recommendations \\ of the research} & 
	The author found that for RLSC the LOOCV selects consistently a regularization parameter that is too large.
    \\ & 
    \specialcell{Highlighted challenges \\ or open problems} & 
    The author limited experiments to a single text classification data set, and higlight that these results may be are uncharacteristic of other domains and possibly other text data sets.
	\\
	
    \hline
     \label{tab:cm}
    \end{longtable}%
  %\label{tab:addlabel}%
%\end{table}%
\end{landscape}