%\begin{landscape}
% Table generated by Excel2LaTeX from sheet 'Sheet 1'
%\begin{table}[htbp]
% \centering
 % \caption{Known and used feature projection method.}
    \begin{longtable}{p{.15\textwidth}p{.85\textwidth}}
    \caption{Known and used feature projection method.} \\
    \hline    
    \specialcell{\textbf{Aspect of work}} & \multicolumn{1}{c}{\textbf{Reference/Description}} \\
	\hline
	
	& \multicolumn{1}{c}{\textbf{~\citet{Li2020}}} \\ 
    \specialcell{Details} &
	The authors propose a novel document representation scheme called Bag-of-Concepts (BoC), which automatically acquires useful conceptual knowledge from an external knowledge base. Also, the authors propose a second representation model called Bag-of-Concept-Clusters (BoCCl), which clusters semantically similar concepts together and performs entity sense disambiguation to improve BoC representation further.     
    \\ 
    \specialcell{Findings} & 
	The classification accuracy of bag-of-words (BOW) (with L2 normalized TF-IDF weighting) outperforms many other document representations, indicating that BOW is a solid baseline for document classification tasks. BoC and BoCCl can effectively capture the concept-level information of documents and improve the performance of document classification. Also, they are great interpretability which allows human to have a deeper understanding of the document and a clearer operation logic for further reasoning and decision making.
    \\
    \specialcell{Challenges} & 
    The authors want to study BoC/BoCCl for sentence-level representation. The authors plan to incorporate conceptual knowledge into deep neural networks for better semantic understandings of natural language while preserving model interpretability.
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Gao2018}}} \\ 
    \specialcell{Details} &
    The authors propose an innovative latent relation-enhanced word embedding model to increase the semantic relatedness of words in the corpus. The authors discover more useful relations between words and add them to word embedding. The authors take into account pattern and conceptual links between words during the construction of word embedding.      
    \\ 
    \specialcell{Findings} & 
    The authors found that word embedding representation is a powerful tool for various systems as a reliable input. In the paper, the authors treated the importance of the context information equally to predict a word. 
    \\ 
    \specialcell{Challenges} & 
    The authors want to carefully analysis the difference between the context information as some unique distributions to generate word embedding, and the context distributions can be treated as attentions in multi-layer networks, especially related to specific applications. 
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Chen2017}}} \\ 
    \specialcell{Details} &
    The authors propose to add competitive learning during an autoencoder training phase. Due to the competition between the neurons in the hidden layer, each neuron becomes specialized in recognizing specific data patterns, and overall the model can learn meaningful representations of textual data.    
    \\
    \specialcell{Findings} & 
    A comprehensive set of experiments show that the proposed autoencoder called KATE can learn better representation than traditional autoencoders. Also, the proposed model outperforms deep generative models, probabilistic topic models, and even word representation models (e.g., Word2Vec) in terms of several downstream tasks such as document classification, regression, and retrieval. The KATE is also able to learn semantically meaningful representations of words, documents and topics, which we evaluated via both quantitative and qualitative studies. 
    \\ 
    \specialcell{Challenges} & 
    The authors want to evaluate KATE on more domain-specific datasets, such as bibliographic networks, for example, for topic induction and scientific publication retrieval. The authors also want to improve the scalability and effectiveness of the approach.
    \\
		
    & \multicolumn{1}{c}{\textbf{~\citet{Hu2017}}} \\ 
    \specialcell{Details} & 
    The authors developed a new regularized Restricted Boltzmann Machines (RBM), which takes into account the class information. The authors imposed two constraints on RBM to make the class information clearly reflected in extracted features. One constraint decreases the distance between the features of the same class, and the other one increases the distance between the features of different classes. 
    \\ 
    \specialcell{Findings} & 
	The features extracted by proposed method have stronger discriminant compared to the state-of-the-art RBM variant models. The cpu time of training CPr-RBM is longer than other RBM modelsâ€™, which also results from the two introduced regularization terms. The improved performance of the method is at the cost of high computational. The effect of inter-class repulsion regularization part obtained by the proposed models is not obvious, namely, features of different groups can not be well separated.
	\\ 
	\specialcell{Challenges} & 
	The authors want to consider a new inter-class repulsion regularization to improve the performance of the method. 
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Kesiraju2016}}} \\
    \specialcell{Details} &
	The authors propose to use Subspace Multinomial Model (SMM) which modification that based on adding the regularization terms to the basic SMM objective function, for obtaining a compact and continuous representation for the documents, which are further used for document classification and clustering tasks.
    \\ 
    \specialcell{Findings} & 
	In the work the authors observed that the classification accuracy of SMM increases with the increasing dimensionality of the latent variable which is not the case with Sparse Topical Coding (STC) or probabilistic topic models (PTM).
    \\
    \specialcell{Challenges} & 
    The work suggests deep exploration of different optimization techniques for $l_1$ regularized objective functions. The future work involves, exploring discriminative SMM and fully Bayesian modelling of SMM.
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Li2016a}}} \\ 
    \specialcell{Details} &
	The authors propose a novel hybrid model called Mixed Word Embedding (MWE) that combines two variants of word2vec in a seamless way via sharing a common encoding structure, which is able to capture the syntax information of words more accurately. Furthermore, it incorporates a global text vector so as to capture more semantic information. 
    \\ 
    \specialcell{Findings} & 
	The authors found that we may achieve very competitive performance as compared with other state-of-the-art methods thanks to using the learned latent representations. Furthermore, the proposed MWE preserves the same time complexity as the SKIP-GRAM. 
    \\ 
    \specialcell{Challenges} & 
	The authors want to improve the mixed word embedding model by incorporating more external corpus and considering proximity and ambiguity among words.
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Zheng2016}}} \\ 
    \specialcell{Details} &
	The authors propose a Bidirectional Hierarchical Skip-Gram model (BHSG) based on a skip-gram model to model text topic embedding. The authors consider the whole sentence/document as a special word, called global context word, and apply the skip-gram to capture the semantic relationship between the words and the global context word. So, the proposed hierarchical skip-gram model learns the semantic relationship between different words and enhance the topic relationship between words and document.    
    \\ 
    \specialcell{Findings} &	
	The proposed model is based on the negative sampling manner, which has high efficiency and it is also an unsupervised method. Hence BHSG is very suitable for the large scale data.
    \\ 
    \specialcell{Challenges} & 
    The authors want to extend BHSG to more topic-related tasks, such as keywords extraction and text summarization. 
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Rodrigues2014}}} \\ 
    \specialcell{Details} &
	The proposed method is based on the Incremental Naive Bayes Clustering (INBC) algorithm, which was initially designed for continuous inputs, and so is considered an extension to it. After training, the resulting model is composed of a set of clusters which are used to categorize documents. The authors' significant changes made over that model regarding the kind of probability distribution used to model the data and the similarity criterion between clusters and vectors.     
    \\ 
    \specialcell{Findings} & 
	The authors observed that only a single pass over the training data is needed to achieve good classification result. Also, the authors notice that as more data is presented, the model can be improved.
    \\ 
    \specialcell{Challenges} & 
    The work suggests the works regarding the selection of features used when comparing documents and clusters. As future work, an online extension to this technique could be applied in the proposed approach, improving its feature selection capabilities.  Also, the authors consider other similarity criteria to clusters and documents, preferably ones that could give confidence estimates to the decision.
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Cai2012}}} \\ 
    \specialcell{} & See Table~\ref{tab:lm} \\
    %\specialcell{Details} &
	%See Table~\ref{tab:lm}
    %\\ 
    %\specialcell{Findings} & 
	%See Table~\ref{tab:lm}  
    %\\ 
    %\specialcell{Challenges} & 
	%See Table~\ref{tab:lm}
	%\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Li2011}}} \\ 
    \specialcell{Details} &
	The authors propose a Concise Semantic Analysis (CSA) technique for dimension reduction. CSA extracts a few concepts (a new N-dimensional space, where each concept represents a new dimension) based on category/class labels and then implements a concise interpretation of words and documents in this new space. The created space of concepts is small in quantity and great in generality and tightly related to the category labels.        
    \\ 
    \specialcell{Findings} & 
	The CSA helps for dimension sensitive learning algorithms such as k-nearest neighbor (kNN) to eliminate the \textit{Curse of Dimensionality}~\citep{}. The kNN in new concept space reaches a comparable performance with support vector machine (SVM) in text categorization applications. In addition, CSA is language independent and performs equally well both in Chinese and English. Also, CSA preserves necessary information for classifiers with very low computing cost. 
    \\ 
    \specialcell{Challenges} & 
    In future work, the authors want to create a implementation of CSA on large taxonomies text categorization. The authors will create a parallelize version of CSA to satisfy the requirement of large scale text categorization applications.
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Salakhutdinov2009}}} \\ 
    \specialcell{Details} &
    The authors propose the method that, for each document, creates a separate Restricted Boltzmann Machines (RBM) with as many softmax units as there are words in the document. The proposed approach ignores the order of the words, and all softmax units can share the same set of weights, connecting them to the hidden binary units. So, the weights can be shared by the whole family of different-sized RBM's that are created for documents of different lengths, and for this reason, the method is called the "Replicated Softmax" model. Also, the authors present efficient learning and inference algorithms for this model and show how a Monte-Carlo based method, Annealed Importance Sampling, can be used to produce an accurate estimate of the log-probability the model assigns to test data. Finally, the authors demonstrate that the proposed model can generalise much better compared to Latent Dirichlet Allocation in terms of both the log-probability of held-out documents and retrieval accuracy.  
    \\ 
    \specialcell{Findings} & 
    In the work, the authors observed that the learning is easy and stable, it can model documents of different lengths, and computing the posterior distribution over the latent topic values is easy. When is using stochastic gradient descent, scaling up learning to billions of documents would not be particularly tricky. This is in contrast to directed topic models, where most of the existing inference algorithms are designed to be run in a batch mode. Therefore the authors would have to make further approximations, for example, by using particle filtering~\citep{Canini2009}. The authors demonstrate that the proposed model can generalise much better than LDA in terms of both the log-probability on held-out documents and retrieval accuracy.
    \\ 
    \specialcell{Challenges} & 
    The authors want to add to modelling the information output label. The authors want to define a conditional Replicated Softmax model, where the observed document-specific metadata, such as author, references, can be used to influence the states of the latent topic units Finally, the authors conclude that a single layer of binary features may not the best way to capture the complex structure in the count data. Once the Replicated Softmax has been trained, they suggest adding more layers to create a Deep Belief Network~\citep{Hinton2006}, which could potentially produce a better generative model and further improve retrieval accuracy.
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Yan2008}}} \\ 
    \specialcell{} & See Table~\ref{tab:fsm} \\
    %\specialcell{Details} &
	%See Table~\ref{tab:fsm}
    %\\ 
    %\specialcell{Findings} & 
	%See Table~\ref{tab:fsm}  
    %\\ 
    %\specialcell{Challenges} & 
	%See Table~\ref{tab:fsm}
	%\\
	
    \hline
     \label{tab:fpm}
    \end{longtable}%
  %\label{tab:addlabel}%
%\end{table}%
%\end{landscape}