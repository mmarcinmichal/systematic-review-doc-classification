%\begin{landscape}
% Table generated by Excel2LaTeX from sheet 'Sheet 1'
%\begin{table}[htbp]
% \centering
 % \caption{Known and used feature projection method.}
    \begin{longtable}{p{.15\textwidth}p{.85\textwidth}}
    \caption{Known and used feature projection method.} \\
    \hline    
    \specialcell{\textbf{Aspect of work}} & \multicolumn{1}{c}{\textbf{Reference/Description}} \\
	\hline
	
	& \multicolumn{1}{c}{\textbf{~\citet{Li2020}}} \\ 
    \specialcell{Details} &
	A proposed representation scheme called Bag-of-Concepts (BoC) automatically acquires useful conceptual knowledge from an external knowledge base. A second representation model called Bag-of-Concept-Clusters (BoCCl) improves BoC representation further.     
    \\ 
    \specialcell{Findings} & 
	Bag-of-words (BoW) is a solid baseline for document classification tasks. BoC and BoCCl can effectively capture the concept-level information of documents. Also, they are great interpretability.
    \\
    \specialcell{Challenges} & 
    A sentence-level based representation should be considered. A conceptual knowledge should be incorporate into deep neural networks.
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Gao2018}}} \\ 
    \specialcell{Details} &
    An innovative latent relation-enhanced word embedding model increases the semantic relatedness of words in the corpus. The authors discover more useful relations between words and add them to word embedding.      
    \\ 
    \specialcell{Findings} & 
    Word embedding representation is a powerful tool for various systems as a reliable input.
    \\ 
    \specialcell{Challenges} & 
    The context information as some unique distributions to generate word embedding should be analysed carefully.
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Chen2017}}} \\ 
    \specialcell{Details} &
    Competitive learning is added during an autoencoder training phase. Due to the competition between neurons, each neuron becomes specialized, and overall the model can learn meaningful representations of textual data.     
    \\
    \specialcell{Findings} & 
    The proposed autoencoder called KATE can learn better representation than traditional autoencoders and outperforms deep generative models, probabilistic topic models, and even word representation models. 
    \\ 
    \specialcell{Challenges} & 
    KATE should be evaluated on more domain-specific datasets. The scalability and effectiveness of the approach should be improved.
    \\
		
    & \multicolumn{1}{c}{\textbf{~\citet{Hu2017}}} \\ 
    \specialcell{Details} & 
    The authors developed a new regularized Restricted Boltzmann Machines (RBM), which takes into account the class information. 
    \\ 
    \specialcell{Findings} & 
	The extracted features by the proposed method have strong discriminant power. The improved performance of the method is at the cost of high computational. The effect of inter-class repulsion regularization part obtained by the proposed models is not obvious, namely, features of different groups can not be well separated.
	\\ 
	\specialcell{Challenges} & 
	A new inter-class repulsion regularization should be used to improve the performance of the method. 
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Kesiraju2016}}} \\
    \specialcell{Details} &
	A Subspace Multinomial Model (SMM) which modification, i.e. regularization terms creates a compact and continuous representation for the documents.
    \\ 
    \specialcell{Findings} & 
	The classification accuracy of SMM increases with the increasing dimensionality of the latent variable which is not the case with Sparse Topical Coding (STC) or Probabilistic Topic Models (PTM).
    \\
    \specialcell{Challenges} & 
    An in-depth exploration of different optimization techniques should be performed. Exploring discriminative SMM and fully Bayesian modelling of SMM should be involved.
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Li2016a}}} \\ 
    \specialcell{Details} &
	A novel hybrid model called Mixed Word Embedding (MWE) combines two variants of word2vec in a seamless way via sharing a common encoding structure. Furthermore, it incorporates a global text vector so as to capture more semantic information. 
    \\ 
    \specialcell{Findings} & 
	MWE achieves very competitive performance. MWE preserves the same time complexity as the SKIP-GRAM. 
    \\ 
    \specialcell{Challenges} & 
	MWE should be improved by incorporating more external corpus and considering proximity and ambiguity among words.
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Zheng2016}}} \\ 
    \specialcell{Details} &
	A Bidirectional Hierarchical Skip-Gram model (BHSG) bases on a skip-gram model to model text topic embedding and considers the whole sentence/document as a special word to capture the semantic relationship between the words and the global context word.    
    \\ 
    \specialcell{Findings} &	
	BHSG utilise the negative sampling manner. Hence it is very suitable for the large scale data.
    \\ 
    \specialcell{Challenges} & 
    BHSG should be extended to realise more topic-related tasks, such as keywords extraction and text summarization. 
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Rodrigues2014}}} \\ 
    \specialcell{Details} &
	The proposed method is based on the Incremental Naive Bayes Clustering (INBC) algorithm, which was initially designed for continuous inputs, and so is considered an extension to it.    
    \\ 
    \specialcell{Findings} & 
	A single pass over the training data is needed to achieve good classification result. As more data is presented, the model can be improved.
    \\ 
    \specialcell{Challenges} & 
    A feature selection should be performed. The other properties, such as similarity criteria should be checked.
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Cai2012}}} \\ 
    \specialcell{} & See Table~\ref{tab:lm} \\
    %\specialcell{Details} &
	%See Table~\ref{tab:lm}
    %\\ 
    %\specialcell{Findings} & 
	%See Table~\ref{tab:lm}  
    %\\ 
    %\specialcell{Challenges} & 
	%See Table~\ref{tab:lm}
	%\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Li2011}}} \\ 
    \specialcell{Details} &
	A Concise Semantic Analysis (CSA) technique extracts a few concepts (a new N-dimensional space, where each concept represents a new dimension) bases on class labels. It then implements a concise interpretation of words and documents in this new space.      
    \\ 
    \specialcell{Findings} & 
	The CSA helps for dimension sensitive learning algorithms such as k-nearest neighbor (k-NN) to eliminate the \textit{Curse of Dimensionality}~\citep{bouveyron2019,Aggarwal2016}. The kNN in new concept space reaches a comparable performance with Support Vector Machines (SVMs). CSA performs equally well both in Chinese and English. CSA has very low computing cost. 
    \\ 
    \specialcell{Challenges} & 
    CSA should adopted to perform on large taxonomies text categorization. CSA should be parallelized.
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Salakhutdinov2009}}} \\ 
    \specialcell{Details} &
    The authors propose the method that, for each document, creates a separate Restricted Boltzmann Machines (RBM) with as many softmax units as there are words in the document. Also, the authors present efficient learning and inference algorithms for this model.   
    \\ 
    \specialcell{Findings} & 
    The model learning is easy and stable. We may scaling up learning to billions of documents. This is in contrast to directed topic models, where most of the existing inference algorithms are designed to be run in a batch mode. The proposed model can generalise much better than Latent Dirichlet Allocation (LDA).
    \\ 
    \specialcell{Challenges} & 
    The label information should be added to modelling. The observed document-specific metadata should be incorporated to model learning. The more layers should be added to create a Deep Belief Network~\citep{Hinton2006}.
	\\
	
	& \multicolumn{1}{c}{\textbf{~\citet{Yan2008}}} \\ 
    \specialcell{} & See Table~\ref{tab:fsm} \\
    %\specialcell{Details} &
	%See Table~\ref{tab:fsm}
    %\\ 
    %\specialcell{Findings} & 
	%See Table~\ref{tab:fsm}  
    %\\ 
    %\specialcell{Challenges} & 
	%See Table~\ref{tab:fsm}
	%\\
	
    \hline
     \label{tab:fpm}
    \end{longtable}%
  %\label{tab:addlabel}%
%\end{table}%
%\end{landscape}