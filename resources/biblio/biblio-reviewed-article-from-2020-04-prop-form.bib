@article{Unnam2020,
   author = {Narendra Babu Unnam and P. Krishna Reddy},
   doi = {10.1007/s41060-019-00200-5},
   issn = {2364-415X},
   issue = {1},
   journal = {International Journal of Data Science and Analytics},
   month = {6},
   pages = {49-64},
   publisher = {Springer Science and Business Media LLC},
   title = {A document representation framework with interpretable features using pre-trained word embeddings},
   volume = {10},
   url = {http://link.springer.com/10.1007/s41060-019-00200-5},
   year = {2020},
}
@article{Chen2020,
   author = {C. L. Philip Chen and Shuang Feng},
   doi = {10.1109/TCYB.2018.2869902},
   issn = {2168-2267},
   issue = {5},
   journal = {IEEE Transactions on Cybernetics},
   month = {5},
   pages = {2237-2248},
   publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
   title = {Generative and Discriminative Fuzzy Restricted Boltzmann Machine Learning for Text and Image Classification},
   volume = {50},
   url = {https://ieeexplore.ieee.org/document/8478774/},
   year = {2020},
}
@article{Guo2021,
   author = {Shun Guo and Nianmin Yao},
   doi = {10.1109/TKDE.2019.2961343},
   issn = {1041-4347},
   issue = {8},
   journal = {IEEE Transactions on Knowledge and Data Engineering},
   month = {8},
   pages = {3062-3074},
   publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
   title = {Document Vector Extension for Documents Classification},
   volume = {33},
   url = {https://ieeexplore.ieee.org/document/8938709/},
   year = {2021},
}
@article{Guo2020,
   author = {Shun Guo and Nianmin Yao},
   doi = {10.1007/s00521-019-04541-x},
   issn = {0941-0643},
   issue = {14},
   journal = {Neural Computing and Applications},
   month = {7},
   pages = {10087-10108},
   publisher = {Springer Science and Business Media LLC},
   title = {Generating word and document matrix representations for document classification},
   volume = {32},
   url = {http://link.springer.com/10.1007/s00521-019-04541-x},
   year = {2020},
}
@article{Zhou2020,
   abstract = {<p> Feature representation and feature extraction are two crucial procedures in text mining. Convolutional Neural Networks (CNN) have shown overwhelming success for text-mining tasks, since they are capable of efficiently extracting <italic>n</italic> -gram features from source data. However, vanilla CNN has its own weaknesses on feature representation and feature extraction. A certain amount of filters in CNN are inevitably duplicate and thus hinder to discriminatively represent a given text. In addition, most existing CNN models extract features in a fixed way (i.e., max pooling) that either limit the CNN to local optimum nor without considering the relation between all features, thereby unable to learn a contextual <italic>n</italic> -gram features adaptively. In this article, we propose a discriminative CNN with context-aware attention to solve the challenges of vanilla CNN. Specifically, our model mainly encourages discrimination across different filters via maximizing their earth mover distances and estimates the salience of feature candidates by considering the relation between context features. We validate carefully our findings against baselines on five benchmark datasets of classification and two datasets of summarization. The results of the experiments verify the competitive performance of our proposed model. </p>},
   author = {Yuxiang Zhou and Lejian Liao and Yang Gao and Heyan Huang and Xiaochi Wei},
   doi = {10.1145/3397464},
   issn = {2157-6904},
   issue = {5},
   journal = {ACM Transactions on Intelligent Systems and Technology},
   month = {10},
   pages = {1-21},
   publisher = {Association for Computing Machinery (ACM)},
   title = {A Discriminative Convolutional Neural Network with Context-aware Attention},
   volume = {11},
   url = {https://dl.acm.org/doi/10.1145/3397464},
   year = {2020},
}
@inproceedings{Chiu2020,
   author = {Billy Chiu and Sunil Kumar Sahu and Neha Sengupta and Derek Thomas and Mohammady Mahdy},
   city = {New York, NY, USA},
   doi = {10.1145/3397271.3401203},
   isbn = {9781450380164},
   journal = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
   month = {7},
   pages = {1685-1688},
   publisher = {ACM},
   title = {Attending to Inter-sentential Features in Neural Text Classification},
   url = {https://dl.acm.org/doi/10.1145/3397271.3401203},
   year = {2020},
}
@book_section{,
   author = {Marcin Białas and Marcin Michał Mirończuk and Jacek Mańdziuk},
   doi = {10.1007/978-3-030-58112-1_30},
   journal = {Parallel Problem Solving from Nature  PPSN XVI},
   pages = {433-447},
   publisher = {Springer International Publishing},
   title = {Biologically Plausible Learning of Text Representation with Spiking Neural Networks},
   url = {http://link.springer.com/10.1007/978-3-030-58112-1_30},
   year = {2020},
}
@inproceedings{Yang2020,
   author = {Liang Yang and Fan Wu and Junhua Gu and Chuan Wang and Xiaochun Cao and Di Jin and Yuanfang Guo},
   city = {New York, NY, USA},
   doi = {10.1145/3366423.3380102},
   isbn = {9781450370233},
   journal = {Proceedings of The Web Conference 2020},
   month = {4},
   pages = {144-154},
   publisher = {ACM},
   title = {Graph Attention Topic Modeling Network},
   url = {https://dl.acm.org/doi/10.1145/3366423.3380102},
   year = {2020},
}
@article{Kesiraju2020,
   author = {Santosh Kesiraju and Oldrich Plchot and Lukas Burget and Suryakanth V. Gangashetty},
   doi = {10.1109/TASLP.2020.3012062},
   issn = {2329-9290},
   journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
   pages = {2319-2332},
   publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
   title = {Learning Document Embeddings Along With Their Uncertainties},
   volume = {28},
   url = {https://ieeexplore.ieee.org/document/9149686/},
   year = {2020},
}
@article{Shen2020,
   author = {Yuan-Yuan Shen and Yan-Ming Zhang and Xu-Yao Zhang and Cheng-Lin Liu},
   doi = {10.1016/j.neucom.2020.03.025},
   issn = {09252312},
   journal = {Neurocomputing},
   month = {7},
   pages = {467-478},
   publisher = {Elsevier BV},
   title = {Online semi-supervised learning with learning vector quantization},
   volume = {399},
   url = {https://linkinghub.elsevier.com/retrieve/pii/S0925231220303672},
   year = {2020},
}
@inproceedings{Suneera2020,
   author = {C M Suneera and Jay Prakash},
   doi = {10.1109/INDICON49873.2020.9342208},
   isbn = {978-1-7281-6916-3},
   journal = {2020 IEEE 17th India Council International Conference (INDICON)},
   month = {12},
   pages = {1-6},
   publisher = {IEEE},
   title = {Performance Analysis of Machine Learning and Deep Learning Models for Text Classification},
   url = {https://ieeexplore.ieee.org/document/9342208/},
   year = {2020},
}
@inproceedings{Wei2020,
   author = {Xinde Wei and Hai Huang and Longxuan Ma and Ze Yang and Liutong Xu},
   doi = {10.1109/ICSESS49938.2020.9237709},
   isbn = {978-1-7281-6578-3},
   journal = {2020 IEEE 11th International Conference on Software Engineering and Service Science (ICSESS)},
   month = {10},
   pages = {91-97},
   publisher = {IEEE},
   title = {Recurrent Graph Neural Networks for Text Classification},
   url = {https://ieeexplore.ieee.org/document/9237709/},
   year = {2020},
}
@article{Chen2020,
   author = {Gang Chen and Sargur N. Srihari},
   doi = {10.1016/j.patrec.2020.10.006},
   issn = {01678655},
   journal = {Pattern Recognition Letters},
   month = {12},
   pages = {214-221},
   publisher = {Elsevier BV},
   title = {Revisiting hierarchy: Deep learning with orthogonally constrained prior for classification},
   volume = {140},
   url = {https://linkinghub.elsevier.com/retrieve/pii/S0167865520303810},
   year = {2020},
}
@article{Aler2020,
   author = {Ricardo Aler and José M. Valls and Henrik Boström},
   doi = {10.1016/j.eswa.2020.113264},
   issn = {09574174},
   journal = {Expert Systems with Applications},
   month = {7},
   pages = {113264},
   publisher = {Elsevier BV},
   title = {Study of Hellinger Distance as a splitting metric for Random Forests in balanced and imbalanced classification datasets},
   volume = {149},
   url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417420300890},
   year = {2020},
}
@article{Jiang2020,
   author = {Haiyun Jiang and Deqing Yang and Yanghua Xiao and Wei Wang},
   doi = {10.1007/s11280-020-00806-x},
   issn = {1386-145X},
   issue = {4},
   journal = {World Wide Web},
   month = {7},
   pages = {2429-2447},
   publisher = {Springer Science and Business Media LLC},
   title = {Understanding a bag of words by conceptual labeling with prior weights},
   volume = {23},
   url = {https://link.springer.com/10.1007/s11280-020-00806-x},
   year = {2020},
}
@inproceedings{Wagh2021,
   author = {Vedangi Wagh and Snehal Khandve and Isha Joshi and Apurva Wani and Geetanjali Kale and Raviraj Joshi},
   doi = {10.1109/TENCON54134.2021.9707465},
   isbn = {978-1-6654-9532-5},
   journal = {TENCON 2021 - 2021 IEEE Region 10 Conference (TENCON)},
   month = {12},
   pages = {732-737},
   publisher = {IEEE},
   title = {Comparative Study of Long Document Classification},
   url = {https://ieeexplore.ieee.org/document/9707465/},
   year = {2021},
}
@article{Zhou2021,
   author = {Yuxiang Zhou and Lejian Liao and Yang Gao and Heyan Huang},
   doi = {10.1016/j.ins.2020.12.084},
   issn = {00200255},
   journal = {Information Sciences},
   month = {5},
   pages = {265-279},
   publisher = {Elsevier BV},
   title = {Extracting salient features from convolutional discriminative filters},
   volume = {558},
   url = {https://linkinghub.elsevier.com/retrieve/pii/S0020025521000165},
   year = {2021},
}
@article{Dai2022,
   author = {Yong Dai and Linjun Shou and Ming Gong and Xiaolin Xia and Zhao Kang and Zenglin Xu and Daxin Jiang},
   doi = {10.1016/j.knosys.2021.107659},
   issn = {09507051},
   journal = {Knowledge-Based Systems},
   month = {1},
   pages = {107659},
   publisher = {Elsevier BV},
   title = {Graph Fusion Network for Text Classification},
   volume = {236},
   url = {https://linkinghub.elsevier.com/retrieve/pii/S0950705121009217},
   year = {2022},
}
@inproceedings{Xie2021,
   author = {Qianqian Xie and Jimin Huang and Pan Du and Min Peng and Jian-Yun Nie},
   city = {New York, NY, USA},
   doi = {10.1145/3442381.3450045},
   isbn = {9781450383127},
   journal = {Proceedings of the Web Conference 2021},
   month = {4},
   pages = {3055-3065},
   publisher = {ACM},
   title = {Graph Topic Neural Network for Document Representation},
   url = {https://dl.acm.org/doi/10.1145/3442381.3450045},
   year = {2021},
}
@inproceedings{Ragesh2021,
   author = {Rahul Ragesh and Sundararajan Sellamanickam and Arun Iyer and Ramakrishna Bairi and Vijay Lingam},
   city = {New York, NY, USA},
   doi = {10.1145/3437963.3441746},
   isbn = {9781450382977},
   journal = {Proceedings of the 14th ACM International Conference on Web Search and Data Mining},
   month = {3},
   pages = {860-868},
   publisher = {ACM},
   title = {HeteGCN: Heterogeneous Graph Convolutional Networks for Text Classification},
   url = {https://dl.acm.org/doi/10.1145/3437963.3441746},
   year = {2021},
}
@inproceedings{Zhang2021,
   author = {Cheng Zhang and Hayato Yamana},
   doi = {10.1109/ICBDA51983.2021.9403092},
   isbn = {978-1-6654-1513-2},
   journal = {2021 IEEE 6th International Conference on Big Data Analytics (ICBDA)},
   month = {3},
   pages = {193-197},
   publisher = {IEEE},
   title = {Improving Text Classification Using Knowledge in Labels},
   url = {https://ieeexplore.ieee.org/document/9403092/},
   year = {2021},
}
@article{Nagumothu2021,
   abstract = {<p>Standardized approaches to relevance classification in information retrieval use generative statistical models to identify the presence or absence of certain topics that might make a document relevant to the searcher. These approaches have been used to better predict relevance on the basis of what the document is “about”, rather than a simple-minded analysis of the bag of words contained within the document. In more recent times, this idea has been extended by using pre-trained deep learning models and text representations, such as GloVe or BERT. These use an external corpus as a knowledge-base that conditions the model to help predict what a document is about. This paper adopts a hybrid approach that leverages the structure of knowledge embedded in a corpus. In particular, the paper reports on experiments where linked data triples (subject-predicate-object), constructed from natural language elements are derived from deep learning. These are evaluated as additional latent semantic features for a relevant document classifier in a customized news-feed website. The research is a synthesis of current thinking in deep learning models in NLP and information retrieval and the predicate structure used in semantic web research. Our experiments indicate that linked data triples increased the F-score of the baseline GloVe representations by 6% and show significant improvement over state-of-the art models, like BERT. The findings are tested and empirically validated on an experimental dataset and on two standardized pre-classified news sources, namely the Reuters and 20 News groups datasets.</p>},
   author = {Dinesh Nagumothu and Peter W. Eklund and Bahadorreza Ofoghi and Mohamed Reda Bouadjenek},
   doi = {10.3390/app11146636},
   issn = {2076-3417},
   issue = {14},
   journal = {Applied Sciences},
   month = {7},
   pages = {6636},
   publisher = {MDPI AG},
   title = {Linked Data Triples Enhance Document Relevance Classification},
   volume = {11},
   url = {https://www.mdpi.com/2076-3417/11/14/6636},
   year = {2021},
}
@article{Wang2021,
   author = {Tao Wang and Yi Cai and Ho-fung Leung and Raymond Y. K. Lau and Haoran Xie and Qing Li},
   doi = {10.1007/s10115-021-01581-5},
   issn = {0219-1377},
   issue = {9},
   journal = {Knowledge and Information Systems},
   month = {9},
   pages = {2313-2346},
   publisher = {Springer Science and Business Media LLC},
   title = {On entropy-based term weighting schemes for text categorization},
   volume = {63},
   url = {https://link.springer.com/10.1007/s10115-021-01581-5},
   year = {2021},
}
@article{Yan2021,
   author = {Peng Yan and Linjing Li and Miaotianzi Jin and Daniel Zeng},
   doi = {10.1016/j.neucom.2021.02.060},
   issn = {09252312},
   journal = {Neurocomputing},
   month = {7},
   pages = {276-286},
   publisher = {Elsevier BV},
   title = {Quantum probability-inspired graph neural network for document representation and classification},
   volume = {445},
   url = {https://linkinghub.elsevier.com/retrieve/pii/S0925231221003167},
   year = {2021},
}
@article{Wang2021,
   author = {Shuaihui Wang and Yu Pan and Jin Zhang and Xingyu Zhou and Zhen Cui and Guyu Hu and Zhisong Pan},
   doi = {10.1016/j.knosys.2021.106891},
   issn = {09507051},
   journal = {Knowledge-Based Systems},
   month = {7},
   pages = {106891},
   publisher = {Elsevier BV},
   title = {Robust and label efficient bi-filtering graph convolutional networks for node classification},
   volume = {224},
   url = {https://linkinghub.elsevier.com/retrieve/pii/S0950705121001544},
   year = {2021},
}
@article{Prabhakar2022,
   abstract = {<p>To classify the texts accurately, many machine learning techniques have been utilized in the field of Natural Language Processing (NLP). For many pattern classification applications, great success has been obtained when implemented with deep learning models rather than using ordinary machine learning techniques. Understanding the complex models and their respective relationships within the data determines the success of such deep learning techniques. But analyzing the suitable deep learning methods, techniques, and architectures for text classification is a huge challenge for researchers. In this work, a Contiguous Convolutional Neural Network (CCNN) based on Differential Evolution (DE) is initially proposed and named as Evolutionary Contiguous Convolutional Neural Network (ECCNN) where the data instances of the input point are considered along with the contiguous data points in the dataset so that a deeper understanding is provided for the classification of the respective input, thereby boosting the performance of the deep learning model. Secondly, a swarm-based Deep Neural Network (DNN) utilizing Particle Swarm Optimization (PSO) with DNN is proposed for the classification of text, and it is named Swarm DNN. This model is validated on two datasets and the best results are obtained when implemented with the Swarm DNN model as it produced a high classification accuracy of 97.32% when tested on the BBC newsgroup text dataset and 87.99% when tested on 20 newsgroup text datasets. Similarly, when implemented with the ECCNN model, it produced a high classification accuracy of 97.11% when tested on the BBC newsgroup text dataset and 88.76% when tested on 20 newsgroup text datasets.</p>},
   author = {Sunil Kumar Prabhakar and Harikumar Rajaguru and Kwangsub So and Dong-Ok Won},
   doi = {10.3389/fncom.2022.900885},
   issn = {1662-5188},
   journal = {Frontiers in Computational Neuroscience},
   month = {6},
   publisher = {Frontiers Media SA},
   title = {A Framework for Text Classification Using Evolutionary Contiguous Convolutional Neural Network and Swarm Based Deep Neural Network},
   volume = {16},
   url = {https://www.frontiersin.org/articles/10.3389/fncom.2022.900885/full},
   year = {2022},
}
@article{Jia2022,
   abstract = {<p lang="fr"><![CDATA[<abstract> <p>There are two main factors involved in documents classification, document representation method and classification algorithm. In this study, we focus on document representation method and demonstrate that the choice of representation methods has impacts on quality of classification results. We propose a document representation strategy for supervised text classification named document representation based on global policy (<italic>DRGP</italic>), which can obtain an appropriate document representation according to the distribution of terms. The main idea of <italic>DRGP</italic> is to construct the optimization function through the importance of terms to different categories. In the experiments, we investigate the effects of <italic>DRGP</italic> on the 20 Newsgroups, Reuters21578 datasets, and using the <italic>SVM</italic> as classifier. The results show that the <italic>DRGP</italic> outperforms other text representation strategy schemes, such as Document Max, Document Two Max and global policy.</p> </abstract>]]></p>},
   author = {Longjia Jia and Bangzuo Zhang},
   doi = {10.3934/mbe.2022245},
   issn = {1551-0018},
   issue = {5},
   journal = {Mathematical Biosciences and Engineering},
   pages = {5223-5240},
   publisher = {American Institute of Mathematical Sciences (AIMS)},
   title = {A new document representation based on global policy for supervised term weighting schemes in text categorization},
   volume = {19},
   url = {http://www.aimspress.com/article/doi/10.3934/mbe.2022245},
   year = {2022},
}
@article{Tang2022,
   author = {Zhong Tang and Wenqiang Li and Yan Li},
   doi = {10.1016/j.eswa.2021.115985},
   issn = {09574174},
   journal = {Expert Systems with Applications},
   month = {3},
   pages = {115985},
   publisher = {Elsevier BV},
   title = {An improved supervised term weighting scheme for text representation and classification},
   volume = {189},
   url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417421013348},
   year = {2022},
}
@inproceedings{Lin2021,
   author = {Yuxiao Lin and Yuxian Meng and Xiaofei Sun and Qinghong Han and Kun Kuang and Jiwei Li and Fei Wu},
   city = {Stroudsburg, PA, USA},
   doi = {10.18653/v1/2021.findings-acl.126},
   journal = {Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021},
   pages = {1456-1462},
   publisher = {Association for Computational Linguistics},
   title = {BertGCN: Transductive Text Classification by Combining GNN and BERT},
   url = {https://aclanthology.org/2021.findings-acl.126},
   year = {2021},
}
@article{Shehzad2022,
   abstract = {<p>In text categorization, a well-known problem related to document length is that larger term counts in longer documents cause classification algorithms to become biased. The effect of document length can be eliminated by normalizing term counts, thus reducing the bias towards longer documents. This gives us term frequency (TF), which in conjunction with inverse document frequency (IDF) became the most commonly used term weighting scheme to capture the importance of a term in a document and corpus. However, normalization may cause term frequency of a term in a related document to become equal or smaller than its term frequency in an unrelated document, thus perturbing a term’s strength from its true worth. In this paper, we solve this problem by introducing a non-linear mapping of term frequency. This alternative to TF is called binned term count (BTC). The newly proposed term frequency factor trims large term counts before normalization, thus moderating the normalization effect on large documents. To investigate the effectiveness of BTC, we compare it against the original TF and its more recently proposed alternative named modified term frequency (MTF). In our experiments, each of these term frequency factors (BTC, TF, and MTF) is combined with four well-known collection frequency factors (IDF), RF, IGM, and MONO and the performance of each of the resulting term weighting schemes is evaluated on three standard datasets (Reuters (R8-21578), 20-Newsgroups, and WebKB) using support vector machines and K-nearest neighbor classifiers. To determine whether BTC is statistically better than TF and MTF, we have applied the paired two-sided t-test on the macro F1 results. Overall, BTC is found to be 52% statistically significant than TF and MTF. Furthermore, the highest macro F1 value on the three datasets was achieved by BTC-based term weighting schemes.</p>},
   author = {Farhan Shehzad and Abdur Rehman and Kashif Javed and Khalid A. Alnowibet and Haroon A. Babri and Hafiz Tayyab Rauf},
   doi = {10.3390/math10214124},
   issn = {2227-7390},
   issue = {21},
   journal = {Mathematics},
   month = {11},
   pages = {4124},
   publisher = {MDPI AG},
   title = {Binned Term Count: An Alternative to Term Frequency for Text Categorization},
   volume = {10},
   url = {https://www.mdpi.com/2227-7390/10/21/4124},
   year = {2022},
}
@article{Yang2022,
   author = {Fei Yang and Huyin Zhang and Shiming Tao and Sheng Hao},
   doi = {10.1007/s10489-021-02889-z},
   issn = {0924-669X},
   issue = {10},
   journal = {Applied Intelligence},
   month = {8},
   pages = {11324-11342},
   publisher = {Springer Science and Business Media LLC},
   title = {Graph representation learning via simple jumping knowledge networks},
   volume = {52},
   url = {https://link.springer.com/10.1007/s10489-021-02889-z},
   year = {2022},
}
@article{Attieh2023,
   author = {Joseph Attieh and Joe Tekli},
   doi = {10.1016/j.knosys.2022.110215},
   issn = {09507051},
   journal = {Knowledge-Based Systems},
   month = {2},
   pages = {110215},
   publisher = {Elsevier BV},
   title = {Supervised term-category feature weighting for improved text classification},
   volume = {261},
   url = {https://linkinghub.elsevier.com/retrieve/pii/S0950705122013119},
   year = {2023},
}
@article{Wang2023,
   author = {Yizhao Wang and Chenxi Wang and Jieyu Zhan and Wenjun Ma and Yuncheng Jiang},
   doi = {10.1016/j.eswa.2023.119658},
   issn = {09574174},
   journal = {Expert Systems with Applications},
   month = {6},
   pages = {119658},
   publisher = {Elsevier BV},
   title = {Text FCG: Fusing Contextual Information via Graph Learning for text classification},
   volume = {219},
   url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417423001598},
   year = {2023},
}
@inproceedings{Zhu2021,
   author = {Hao Zhu and Piotr Koniusz},
   journal = {International Conference on Learning Representations},
   title = {Simple Spectral Graph Convolution},
   url = {https://openreview.net/forum?id=CYO5T-YjWZV},
   year = {2021},
}
@inproceedings{Xie2021,
   author = {Qianqian Xie and Jimin Huang and Pan Du and Min Peng and Jian-Yun Nie},
   city = {Stroudsburg, PA, USA},
   doi = {10.18653/v1/2021.naacl-main.333},
   journal = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
   pages = {4218-4227},
   publisher = {Association for Computational Linguistics},
   title = {Inductive Topic Variational Graph Auto-Encoder for Text Classification},
   url = {https://aclanthology.org/2021.naacl-main.333},
   year = {2021},
}
@article{Liu2020,
   abstract = {<p>Compared to sequential learning models, graph-based neural networks exhibit some excellent properties, such as ability capturing global information. In this paper, we investigate graph-based neural networks for text classification problem. A new framework TensorGCN (tensor graph convolutional networks), is presented for this task. A text graph tensor is firstly constructed to describe semantic, syntactic, and sequential contextual information. Then, two kinds of propagation learning perform on the text graph tensor. The first is intra-graph propagation used for aggregating information from neighborhood nodes in a single graph. The second is inter-graph propagation used for harmonizing heterogeneous information between graphs. Extensive experiments are conducted on benchmark datasets, and the results illustrate the effectiveness of our proposed framework. Our proposed TensorGCN presents an effective way to harmonize and integrate heterogeneous information from different kinds of graphs.</p>},
   author = {Xien Liu and Xinxin You and Xiao Zhang and Ji Wu and Ping Lv},
   doi = {10.1609/aaai.v34i05.6359},
   issn = {2374-3468},
   issue = {05},
   journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
   month = {4},
   pages = {8409-8416},
   publisher = {Association for the Advancement of Artificial Intelligence (AAAI)},
   title = {Tensor Graph Convolutional Networks for Text Classification},
   volume = {34},
   url = {https://ojs.aaai.org/index.php/AAAI/article/view/6359},
   year = {2020},
}
@inproceedings{Gupta2019,
   author = {Vivek Gupta and Ankit Kumar Saw and Pegah Nokhiz and Harshit Gupta and Partha Pratim Talukdar},
   journal = {European Conference on Artificial Intelligence},
   title = {Improving Document Classification with Multi-Sense Embeddings},
   year = {2019},
}
@inproceedings{Wang2020,
   abstract = {Constructing a graph with graph convolutional network (GCN)  to explore the relational structure of the data has attracted lots of interests in various tasks. However, for document classification, existing graph based methods often focus on the straightforward word-word and word-document relations, ignoring the hierarchical semantics. Besides, the graph construction is often independent from the task-specific GCN learning. To address these constrains, we integrate a probabilistic deep topic model into graph construction, and propose a novel trainable hierarchical topic graph (HTG), including word-level, hierarchical topic-level and document-level nodes, exhibiting semantic variation from fine-grained to coarse. Regarding the document classification as a document-node label generation task, HTG can be dynamically evolved with GCN by performing variational inference, which leads to an end-to-end document classification method, named dynamic HTG (DHTG). Besides achieving state-of-the-art classification results, our model learns an interpretable document graph with meaningful node embeddings and semantic edges.},
   author = {Zhengjue Wang and Chaojie Wang and Hao Zhang and Zhibin Duan and Mingyuan Zhou and Bo Chen},
   editor = {Silvia Chiappa and Roberto Calandra},
   journal = {Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics},
   month = {2},
   pages = {3959-3969},
   publisher = {PMLR},
   title = {Learning Dynamic Hierarchical Topic Graph with Graph Convolutional Network for Document Classification},
   volume = {108},
   url = {https://proceedings.mlr.press/v108/wang20l.html},
   year = {2020},
}
@inproceedings{Johnson2016,
   abstract = {One-hot CNN (convolutional neural network) has been shown to be effective for text categorization (Johnson &amp; Zhang, 2015a;b). We view it as a special case of a general framework which jointly trains a linear model with a non-linear feature generator consisting of 'text region embedding + pooling'. Under this framework, we explore a more sophisticated region embedding method using Long Short-Term Memory (LSTM). LSTM can embed text regions of variable (and possibly large) sizes, whereas the region size needs to be fixed in a CNN. We seek effective and efficient use of LSTM for this purpose in the supervised and semi-supervised settings. The best results were obtained by combining region embeddings in the form of LSTM and convolution layers trained on unlabeled data. The results indicate that on this task, embeddings of text regions, which can convey complex concepts, are more useful than embeddings of single words in isolation. We report performances exceeding the previous best results on four benchmark datasets.},
   author = {Rie Johnson and Tong Zhang},
   journal = {Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48},
   pages = {526-534},
   publisher = {JMLR.org},
   title = {Supervised and Semi-Supervised Text Categorization Using LSTM for Region Embeddings},
   year = {2016},
}
@inproceedings{Ding2020,
   author = {Kaize Ding and Jianling Wang and Jundong Li and Dingcheng Li and Huan Liu},
   city = {Stroudsburg, PA, USA},
   doi = {10.18653/v1/2020.emnlp-main.399},
   journal = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
   pages = {4927-4936},
   publisher = {Association for Computational Linguistics},
   title = {Be More with Less: Hypergraph Attention Networks for Inductive Text Classification},
   url = {https://www.aclweb.org/anthology/2020.emnlp-main.399},
   year = {2020},
}
@inproceedings{Guidotti2022,
   author = {Emanuele Guidotti and Alfio Ferrara},
   editor = {Alice H Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
   journal = {Advances in Neural Information Processing Systems},
   title = {Text Classification with Born's Rule},
   url = {https://openreview.net/forum?id=sNcn-E3uPHA},
   year = {2022},
}
@inproceedings{Khandve2022,
   author = {Snehal Ishwar Khandve and Vedangi Kishor Wagh and Apurva Dinesh Wani and Isha Mandar Joshi and Raviraj Bhuminand Joshi},
   city = {New York, NY, USA},
   doi = {10.1145/3529836.3529935},
   isbn = {9781450395700},
   journal = {2022 14th International Conference on Machine Learning and Computing (ICMLC)},
   month = {2},
   pages = {115-119},
   publisher = {ACM},
   title = {Hierarchical Neural Network Approaches for Long Document Classification},
   url = {https://dl.acm.org/doi/10.1145/3529836.3529935},
   year = {2022},
}
@article{Yao2019,
   abstract = {<p>Text classification is an important and classical problem in natural language processing. There have been a number of studies that applied convolutional neural networks (convolution on regular grid, e.g., sequence) to classification. However, only a limited number of studies have explored the more flexible graph convolutional neural networks (convolution on non-grid, e.g., arbitrary graph) for the task. In this work, we propose to use graph convolutional networks for text classification. We build a single text graph for a corpus based on word co-occurrence and document word relations, then learn a Text Graph Convolutional Network (Text GCN) for the corpus. Our Text GCN is initialized with one-hot representation for word and document, it then jointly learns the embeddings for both words and documents, as supervised by the known class labels for documents. Our experimental results on multiple benchmark datasets demonstrate that a vanilla Text GCN without any external word embeddings or knowledge outperforms state-of-the-art methods for text classification. On the other hand, Text GCN also learns predictive word and document embeddings. In addition, experimental results show that the improvement of Text GCN over state-of-the-art comparison methods become more prominent as we lower the percentage of training data, suggesting the robustness of Text GCN to less training data in text classification.</p>},
   author = {Liang Yao and Chengsheng Mao and Yuan Luo},
   doi = {10.1609/aaai.v33i01.33017370},
   issn = {2374-3468},
   issue = {01},
   journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
   month = {7},
   pages = {7370-7377},
   publisher = {Association for the Advancement of Artificial Intelligence (AAAI)},
   title = {Graph Convolutional Networks for Text Classification},
   volume = {33},
   url = {https://ojs.aaai.org/index.php/AAAI/article/view/4725},
   year = {2019},
}
