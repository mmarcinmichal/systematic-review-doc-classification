% Encoding: UTF-8

@inproceedings{Kusner2015,
abstract = {We present the Word Mover's Distance (WMD), a novel distance function between text documents. Our work is based on recent results in word embeddings that learn semantically meaningful representations for words from local cooccurrences in sentences. The WMD distance measures the dissimilarity between two text documents as the minimum amount of distance that the embedded words of one document need to "travel" to reach the embedded words of another document. We show that this distance metric can be cast as an instance of the Earth Mover's Distance, a well studied transportation problem for which several highly efficient solvers have been developed. Our metric has no hyperparameters and is straight-forward to implement. Further, we demonstrate on eight real world document classification data sets, in comparison with seven state-of-the-art baselines, that the WMD metric leads to unprecedented low k-nearest neighbor document classification error rates.},
author = {Kusner, Matt J. and Sun, Yu and Kolkin, Nicholas I. and Weinberger, Kilian Q.},
booktitle = {32nd International Conference on Machine Learning, ICML 2015},
editor = {Bach, Francis R and Blei, David M},
isbn = {9781510810587},
pages = {957--966},
publisher = {JMLR.org},
series = {{\{}JMLR{\}} Workshop and Conference Proceedings},
title = {{From word embeddings to document distances}},
url = {http://proceedings.mlr.press/v37/kusnerb15.html},
volume = {2},
year = {2015}
}

@Article{Kim2019,
  author   = {Kim, Donghwa and Seo, Deokseong and Cho, Suhyoun and Kang, Pilsung},
  journal  = {Information Sciences},
  title    = {{Multi-co-training for document classification using various document representations: TF–IDF, LDA, and Doc2Vec}},
  year     = {2019},
  issn     = {00200255},
  pages    = {15--29},
  volume   = {477},
  abstract = {The purpose of document classification is to assign the most appropriate label to a specified document. The main challenges in document classification are insufficient label information and unstructured sparse format. A semi-supervised learning (SSL) approach could be an effective solution to the former problem, whereas the consideration of multiple document representation schemes can resolve the latter problem. Co-training is a popular SSL method that attempts to exploit various perspectives in terms of feature subsets for the same example. In this paper, we propose multi-co-training (MCT) for improving the performance of document classification. In order to increase the variety of feature sets for classification, we transform a document using three document representation methods: term frequency–inverse document frequency (TF–IDF) based on the bag-of-words scheme, topic distribution based on latent Dirichlet allocation (LDA), and neural-network-based document embedding known as document to vector (Doc2Vec). The experimental results demonstrate that the proposed MCT is robust to parameter changes and outperforms benchmark methods under various conditions.},
  doi      = {10.1016/j.ins.2018.10.006},
  keywords = {Co-training, Doc2vec, Document classification, LDA, Semi-supervised learning, TF–IDF},
}

@Article{Pavlinek2017,
  author   = {Pavlinek, Miha and Podgorelec, Vili},
  journal  = {Expert Systems with Applications},
  title    = {{Text classification method based on self-training and LDA topic models}},
  year     = {2017},
  issn     = {09574174},
  pages    = {83--93},
  volume   = {80},
  abstract = {Supervised text classification methods are efficient when they can learn with reasonably sized labeled sets. On the other hand, when only a small set of labeled documents is available, semi-supervised methods become more appropriate. These methods are based on comparing distributions between labeled and unlabeled instances, therefore it is important to focus on the representation and its discrimination abilities. In this paper we present the ST LDA method for text classification in a semi-supervised manner with representations based on topic models. The proposed method comprises a semi-supervised text classification algorithm based on self-training and a model, which determines parameter settings for any new document collection. Self-training is used to enlarge the small initial labeled set with the help of information from unlabeled data. We investigate how topic-based representation affects prediction accuracy by performing NBMN and SVM classification algorithms on an enlarged labeled set and then compare the results with the same method on a typical TF-IDF representation. We also compare ST LDA with supervised classification methods and other well-known semi-supervised methods. Experiments were conducted on 11 very small initial labeled sets sampled from six publicly available document collections. The results show that our ST LDA method, when used in combination with NBMN, performed significantly better in terms of classification accuracy than other comparable methods and variations. In this manner, the ST LDA method proved to be a competitive classification method for different text collections when only a small set of labeled instances is available. As such, the proposed ST LDA method may well help to improve text classification tasks, which are essential in many advanced expert and intelligent systems, especially in the case of a scarcity of labeled texts.},
  doi      = {10.1016/j.eswa.2017.03.020},
  keywords = {Classification, LDA, Self-training, Semi-supervised learning, Topic modeling},
}

@Article{Cai2012,
  author   = {Cai, Deng and He, Xiaofei},
  journal  = {IEEE Transactions on Knowledge and Data Engineering},
  title    = {{Manifold adaptive experimental design for text categorization}},
  year     = {2012},
  issn     = {10414347},
  number   = {4},
  pages    = {707--719},
  volume   = {24},
  abstract = {In many information processing tasks, labels are usually expensive and the unlabeled data points are abundant. To reduce the cost on collecting labels, it is crucial to predict which unlabeled examples are the most informative, i.e., improve the classifier the most if they were labeled. Many active learning techniques have been proposed for text categorization, such as {\{}$\backslash$rm SVM{\}}-{\{}Active{\}} and Transductive Experimental Design. However, most of previous approaches try to discover the discriminant structure of the data space, whereas the geometrical structure is not well respected. In this paper, we propose a novel active learning algorithm which is performed in the data manifold adaptive kernel space. The manifold structure is incorporated into the kernel space by using graph Laplacian. This way, the manifold adaptive kernel space reflects the underlying geometry of the data. By minimizing the expected error with respect to the optimal classifier, we can select the most representative and discriminative data points for labeling. Experimental results on text categorization have demonstrated the effectiveness of our proposed approach. {\textcopyright} 2012 IEEE.},
  doi      = {10.1109/TKDE.2011.104},
  keywords = {Text categorization, active learning, experimental design, kernel method, manifold learning},
}

@InProceedings{Zhang2005,
  author    = {Zhang, Dell and Chen, Xi and Lee, Wee Sun},
  booktitle = {SIGIR 2005 - Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
  title     = {{Text classification with kernels on the multinomial manifold}},
  year      = {2005},
  editor    = {Baeza-Yates, Ricardo A and Ziviani, Nivio and Marchionini, Gary and Moffat, Alistair and Tait, John},
  pages     = {266--273},
  publisher = {ACM},
  abstract  = {Support Vector Machines (SVMs) have been very successful in text classification. However, the intrinsic geometric structure of text data has been ignored by standard kernels commonly used in SVMs. It is natural to assume that the documents are on the multinomial manifold, which is the simplex of multinomial models furnished with the Riemannian structure induced by the Fisher information metric. We prove that the Negative Geodesic Distance (NGD) on the multinomial manifold is conditionally positive definite (cpd), thus can be used as a kernel in SVMs. Experiments show the NGD kernel on the multinomial manifold to be effective for text classification, significantly outperforming standard kernels on the ambient Euclidean space. {\textcopyright} 2005 ACM.},
  doi       = {10.1145/1076034.1076081},
  isbn      = {1595930345},
  keywords  = {differential geometry, kernels, machine learning, manifolds, support vector machine, text classification},
}

@Article{Brockmeier2018,
  author   = {Brockmeier, Austin J. and Mu, Tingting and Ananiadou, Sophia and Goulermas, John Y.},
  journal  = {IEEE Transactions on Knowledge and Data Engineering},
  title    = {{Self-Tuned Descriptive Document Clustering Using a Predictive Network}},
  year     = {2018},
  issn     = {15582191},
  number   = {10},
  pages    = {1929--1942},
  volume   = {30},
  abstract = {Descriptive clustering consists of automatically organizing data instances into clusters and generating a descriptive summary for each cluster. The description should inform a user about the contents of each cluster without further examination of the specific instances, enabling a user to rapidly scan for relevant clusters. Selection of descriptions often relies on heuristic criteria. We model descriptive clustering as an auto-encoder network that predicts features from cluster assignments and predicts cluster assignments from a subset of features. The subset of features used for predicting a cluster serves as its description. For text documents, the occurrence or count of words, phrases, or other attributes provides a sparse feature representation with interpretable feature labels. In the proposed network, cluster predictions are made using logistic regression models, and feature predictions rely on logistic or multinomial regression models. Optimizing these models leads to a completely self-tuned descriptive clustering approach that automatically selects the number of clusters and the number of features for each cluster. We applied the methodology to a variety of short text documents and showed that the selected clustering, as evidenced by the selected feature subsets, are associated with a meaningful topical organization.},
  doi      = {10.1109/TKDE.2017.2781721},
  keywords = {Descriptive clustering, feature selection, logistic regression, model selection, sparse models},
}

@Article{Qian2007,
  author   = {Qian, Tieyun and Xiong, Hui and Wang, Yuanzhen and Chen, Enhong},
  journal  = {Information Sciences},
  title    = {{On the strength of hyperclique patterns for text categorization}},
  year     = {2007},
  issn     = {00200255},
  number   = {19},
  pages    = {4040--4058},
  volume   = {177},
  abstract = {The use of association patterns for text categorization has attracted great interest and a variety of useful methods have been developed. However, the key characteristics of pattern-based text categorization remain unclear. Indeed, there are still no concrete answers for the following two questions: Firstly, what kind of association pattern is the best candidate for pattern-based text categorization? Secondly, what is the most desirable way to use patterns for text categorization? In this paper, we focus on answering the above two questions. More specifically, we show that hyperclique patterns are more desirable than frequent patterns for text categorization. Along this line, we develop an algorithm for text categorization using hyperclique patterns. As demonstrated by our experimental results on various real-world text documents, our method provides much better computational performance than state-of-the-art methods while retaining classification accuracy. {\textcopyright} 2007 Elsevier Inc. All rights reserved.},
  doi      = {10.1016/j.ins.2007.04.005},
  keywords = {Association rules, Hyperclique patterns, Text categorization},
}

@Article{Rehman2015,
  author   = {Rehman, Abdur and Javed, Kashif and Babri, Haroon A. and Saeed, Mehreen},
  journal  = {Expert Systems with Applications},
  title    = {{Relative discrimination criterion - A novel feature ranking method for text data}},
  year     = {2015},
  issn     = {09574174},
  number   = {7},
  pages    = {3670--3681},
  volume   = {42},
  abstract = {High dimensionality of text data hinders the performance of classifiers making it necessary to apply feature selection for dimensionality reduction. Most of the feature ranking metrics for text classification are based on document frequencies (df) of a term in positive and negative classes. Considering only document frequencies to rank features favors terms frequently occurring in larger classes in unbalanced datasets. In this paper we introduce a new feature ranking metric termed as relative discrimination criterion (RDC), which takes document frequencies for each term count of a term into account while estimating the usefulness of a term. The performance of RDC is compared with four well known feature ranking metrics, information gain (IG), CHI squared (CHI), odds ratio (OR) and distinguishing feature selector (DFS) using support vector machines (SVM) and multinomial naive Bayes (MNB) classifiers on four benchmark datasets, namely Reuters, 20 Newsgroups and two subsets of Ohsumed dataset. Our results based on macro and micro F1 measures show that the performance of RDC is superior than the other four metrics in 65{\%} of our experimental trials. Also, RDC attains highest macro and micro F1 values in 69{\%} of the cases.},
  doi      = {10.1016/j.eswa.2014.12.013},
  keywords = {Document frequency, False positive rate, Feature selection, Term count, Text classification, True positive rate},
}

@Article{Tiwari2019,
  author   = {Tiwari, Prayag and Melucci, Massimo},
  journal  = {IEEE Access},
  title    = {{Towards a Quantum-Inspired Binary Classifier}},
  year     = {2019},
  issn     = {21693536},
  pages    = {42354--42372},
  volume   = {7},
  abstract = {Machine Learning classification models learn the relation between input as features and output as a class in order to predict the class for the new given input. Several research works have demonstrated the effectiveness of machine learning algorithms but the state-of-the-art algorithms are based on the classical theories of probability and logic. Quantum Mechanics (QM) has already shown its effectiveness in many fields and researchers have proposed several interesting results which cannot be obtained through classical theory. In recent years, researchers have been trying to investigate whether the QM can help to improve the classical machine learning algorithms. It is believed that the theory of QM may also inspire an effective algorithm if it is implemented properly. From this inspiration, we propose the quantum-inspired binary classifier, which is based on quantum detection theory. We used text corpora and image corpora to explore the effect of our proposed model. Our proposed model outperforms the state-of-the-art models in terms of precision, recall, and F-measure for several topics (categories) in the 20 newsgroup text corpora. Our proposed model outperformed all the baselines in terms of recall when the MNIST handwritten image dataset was used; F-measure is also higher for most of the categories and precision is also higher for some categories. Our proposed model suggests that binary classification effectiveness can be achieved by using quantum detection theory. In particular, we found that our Quantum-Inspired Binary Classifier can increase the precision, recall, and F-measure of classification where the state-of-the-art methods cannot.},
  doi      = {10.1109/ACCESS.2019.2904624},
  keywords = {Binary classification, quantum mechanics, signal detection},
}

@Article{Genkin2007,
  author   = {Genkin, Alexander and Lewis, David D. and Madigan, David},
  journal  = {Technometrics},
  title    = {{Large-scale bayesian logistic regression for text categorization}},
  year     = {2007},
  issn     = {00401706},
  number   = {3},
  pages    = {291--304},
  volume   = {49},
  abstract = {Logistic regression analysis of high-dimensional data, such as natural language text, poses computational and statistical challenges. Maximum likelihood estimation often fails in these applications. We present a simple Bayesian logistic regression approach that uses a Laplace prior to avoid overfitting and produces sparse predictive models for text data. We apply this approach to a range of document classification problems and show that it produces compact predictive models at least as effective as those produced by support vector machine classifiers or ridge logistic regression combined with feature selection. We describe our model fitting algorithm, our open source implementations (BBR and BMR), and experimental results. {\textcopyright} 2007 American Statistical Association and the American Society for Quality.},
  doi      = {10.1198/004017007000000245},
  keywords = {Information retrieval, Lasso, Penalization, Ridge regression, Support vector classifier, Variable selection},
}

@Article{Rehman2017,
  author   = {Javed, Kashif and Babri, Haroon A.},
  journal  = {Information Processing and Management},
  title    = {{Feature selection based on a normalized difference measure for text classification}},
  year     = {2017},
  issn     = {03064573},
  number   = {2},
  pages    = {473--489},
  volume   = {53},
  abstract = {The goal of feature selection in text classification is to choose highly distinguishing features for improving the performance of a classifier. The well-known text classification feature selection metric named balanced accuracy measure (ACC2) (Forman, 2003) evaluates a term by taking the difference of its document frequency in the positive class (also known as true positives) and its document frequency in the negative class (also known as false positives). This however results in assigning equal ranks to terms having equal difference, ignoring their relative document frequencies in the classes. In this paper we propose a new feature ranking (FR) metric, called normalized difference measure (NDM), which takes into account the relative document frequencies. The performance of NDM is investigated against seven well known feature ranking metrics including odds ratio (OR), chi squared (CHI), information gain (IG), distinguishing feature selector (DFS), gini index (GINI), balanced accuracy measure (ACC2) and Poisson ratio (POIS) on seven datasets namely WebACE(WAP,K1a,K1b), Reuters (RE0, RE1),spam email dataset and 20 newsgroups using the multinomial naive Bayes (MNB) and supports vector machines (SVM) classifiers. Our results show that the NDM metric outperforms the seven metrics in 66{\%} cases in terms of macro-F1 measure and in 51{\%} cases in terms of micro F1 measure in our experimental trials on these datasets.},
  doi      = {10.1016/j.ipm.2016.12.004},
  keywords = {Accuracy measure, Document frequency, Feature selection, Text classification},
}

@InProceedings{Wang2016,
  author    = {Wang, Fen and Li, Xiaoxuan and Huang, Xiaotao and Kang, Ling},
  booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  title     = {{Improved document feature selection with categorical parameter for text classification}},
  year      = {2016},
  editor    = {Boumerdassi, Selma and Renault, {\'{E}}ric and Bouzefrane, Samia},
  pages     = {86--98},
  publisher = {Springer},
  series    = {Lecture Notes in Computer Science},
  volume    = {10026 LNCS},
  abstract  = {Social network develops rapidly and thousands of new data appears on the Internet every day. Classification technology is the key to organize big data. Feature Selection (FS) is a direct way to improve classification efficiency. FS can reduce the size of the feature subset and ensure classification accuracy based on features' score, which is calculated by FS methods. Most previous studies of FS emphasized on precision while timeefficiency was commonly ignored. In our study, we proposed a method named CDFDC at first. It combines both CDF and Category-Frequency. Secondly, we compared DF, CDF, CHI, IG, CDFP VM and CDFDC to figure out the relationships among algorithm complexity, time efficiency and classification accuracy. The experiment is implemented with 20-newsgroup data set and NB classifier. The performance of the FSmethods evaluated by seven aspects: precision, Micro F1, Macro F1, feature-selectiontime, documents-conversion-time, training-time and classification-time. The result shows that the proposed method performs well on efficiency and accuracy when the size of feature subset is greater than 3,000. And it is also discovered that FS algorithm's complexity is unrelated to accuracy but complexity can ensure time stability and predictability.},
  doi       = {10.1007/978-3-319-50463-6_8},
  isbn      = {9783319504629},
  issn      = {16113349},
  keywords  = {Comparison, Experimentation, Feature selection, Measurement, Time efficiency},
  url       = {https://doi.org/10.1007/978-3-319-50463-6{\_}8},
}

@Article{Li2004,
  author   = {Baoli, Li and Qin, Lu and Shiwen, Yu},
  journal  = {ACM Transactions on Asian Language Information Processing},
  title    = {{An adaptive k-nearest neighbor text categorization strategy}},
  year     = {2004},
  issn     = {15300226},
  number   = {4},
  pages    = {215--226},
  volume   = {3},
  abstract = {k is the most important parameter in a text categorization system based on the k-nearest neighbor algorithm (kNN). To classify a new document, the k-nearest documents in the training set are determined first. The prediction of categories for this document can then be made according to the category distribution among the k nearest neighbors. Generally speaking, the class distribution in a training set is not even; some classes may have more samples than others. The system's performance is very sensitive to the choice of the parameter k. And it is very likely that a fixed k value will result in a bias for large categories, and will not make full use of the information in the training set. To deal with these problems, an improved kNN strategy, in which different numbers of nearest neighbors for different categories are used instead of a fixed number across all categories, is proposed in this article. More samples (nearest neighbors) will be used to decide whether a test document should be classified in a category that has more samples in the training set. The numbers of nearest neighbors selected for different categories are adaptive to their sample size in the training set. Experiments on two different datasets show that our methods are less sensitive to the parameter k than the traditional ones, and can properly classify documents belonging to smaller classes with a large k. The strategy is especially applicable and promising for cases where estimating the parameter k via cross-validation is not possible and the class distribution of a training set is skewed.},
  doi      = {10.1145/1039621.1039623},
  keywords = {K-nearest neighbor algorithm, Machine learning, Text categorization, Text classification},
}

@Article{Feng2017,
  author   = {Feng, Xiaoyue and Liang, Yanchun and Shi, Xiaohu and Xu, Dong and Wang, Xu and Guan, Renchu},
  journal  = {Entropy},
  title    = {{Overfitting reduction of text classification based on AdaBELM}},
  year     = {2017},
  issn     = {10994300},
  number   = {7},
  pages    = {330},
  volume   = {19},
  abstract = {Overfitting is an important problem in machine learning. Several algorithms, such as the extreme learning machine (ELM), suffer from this issue when facing high-dimensional sparse data, e.g., in text classification. One common issue is that the extent of overfitting is not well quantified. In this paper, we propose a quantitative measure of overfitting referred to as the rate of overfitting (RO) and a novel model, named AdaBELM, to reduce the overfitting. With RO, the overfitting problem can be quantitatively measured and identified. The newly proposed model can achieve high performance on multi-class text classification. To evaluate the generalizability of the new model, we designed experiments based on three datasets, i.e., the 20 Newsgroups, Reuters-21578, and BioMed corpora, which represent balanced, unbalanced, and real application data, respectively. Experiment results demonstrate that AdaBELM can reduce overfitting and outperform classical ELM, decision tree, random forests, and AdaBoost on all three text-classification datasets; for example, it can achieve 62.2{\%} higher accuracy than ELM. Therefore, the proposed model has a good generalizability.},
  doi      = {10.3390/e19070330},
  keywords = {AdaBoost, Extreme learning machine, Feedforward neural network, Machine learning, Overfitting},
}

@Article{Sun2009,
  author   = {Sun, Aixin and Lim, Ee Peng and Liu, Ying},
  journal  = {Decision Support Systems},
  title    = {{On strategies for imbalanced text classification using SVM: A comparative study}},
  year     = {2009},
  issn     = {01679236},
  number   = {1},
  pages    = {191--201},
  volume   = {48},
  abstract = {Many real-world text classification tasks involve imbalanced training examples. The strategies proposed to address the imbalanced classification (e.g., resampling, instance weighting), however, have not been systematically evaluated in the text domain. In this paper, we conduct a comparative study on the effectiveness of these strategies in the context of imbalanced text classification using Support Vector Machines (SVM) classifier. SVM is the interest in this study for its good classification accuracy reported in many text classification tasks. We propose a taxonomy to organize all proposed strategies following the training and the test phases in text classification tasks. Based on the taxonomy, we survey the methods proposed to address the imbalanced classification. Among them, 10 commonly-used methods were evaluated in our experiments on three benchmark datasets, i.e., Reuters-21578, 20-Newsgroups, and WebKB. Using the area under the Precision-Recall Curve as the performance measure, our experimental results showed that the best decision surface was often learned by the standard SVM, not coupled with any of the proposed strategies. We believe such a negative finding will benefit both researchers and application developers in the area by focusing more on thresholding strategies. {\textcopyright} 2009 Elsevier B.V. All rights reserved.},
  doi      = {10.1016/j.dss.2009.07.011},
  keywords = {Imbalanced text classification, Instance weighting, Resampling, SVM, Support Vector Machines},
}

@Article{Li2020,
  author   = {Li, Pengfei and Mao, Kezhi and Xu, Yuecong and Li, Qi and Zhang, Jiaheng},
  journal  = {Knowledge-Based Systems},
  title    = {{Bag-of-Concepts representation for document classification based on automatic knowledge acquisition from probabilistic knowledge base}},
  year     = {2020},
  issn     = {09507051},
  pages    = {105436},
  volume   = {193},
  abstract = {Text representation, a crucial step for text mining and natural language processing, concerns about transforming unstructured textual data into structured numerical vectors to support various machine learning and data mining algorithms. For document classification, one classical and commonly adopted text representation method is Bag-of-Words (BoW) model. BoW represents document as a fixed-length vector of terms, where each term dimension is a numerical value such as term frequency or tf-idf weight. However, BoW simply looks at surface form of words. It ignores the semantic, conceptual and contextual information of texts, and also suffers from high dimensionality and sparsity issues. To address the aforementioned issues, we propose a novel document representation scheme called Bag-of-Concepts (BoC), which automatically acquires useful conceptual knowledge from external knowledge base, then conceptualizes words and phrases in the document into higher level semantics (i.e. concepts) in a probabilistic manner, and eventually represents a document as a distributed vector in the learned concept space. By utilizing background knowledge from knowledge base, BoC representation is able to provide more semantic and conceptual information of texts, as well as better interpretability for human understanding. We also propose Bag-of-Concept-Clusters (BoCCl) model which clusters semantically similar concepts together and performs entity sense disambiguation to further improve BoC representation. In addition, we combine BoCCl and BoW representations using an attention mechanism to effectively utilize both concept-level and word-level information and achieve optimal performance for document classification.},
  doi      = {10.1016/j.knosys.2019.105436},
  keywords = {Document classification, Interpretability, Knowledge base, Natural language processing, Text representation},
}

@InProceedings{Li2010,
  author    = {Li, Baoli and Vogel, Carl},
  booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  title     = {{Improving multiclass text classification with error-correcting output coding and sub-class partitions}},
  year      = {2010},
  editor    = {Farzindar, Atefeh and Keselj, Vlado},
  pages     = {4--15},
  publisher = {Springer},
  series    = {Lecture Notes in Computer Science},
  volume    = {6085 LNAI},
  abstract  = {Error-Correcting Output Coding (ECOC) is a general framework for multiclass text classification with a set of binary classifiers. It can not only help a binary classifier solve multi-class classification problems, but also boost the performance of a multi-class classifier. When building each individual binary classifier in ECOC, multiple classes are randomly grouped into two disjoint groups: positive and negative. However, when training such a binary classifier, sub-class distribution within positive and negative classes is neglected. Utilizing this information is expected to improve a binary classifier. We thus design a simple binary classification strategy via multi-class categorization (2vM) to make use of sub-class partition information, which can lead to better performance over the traditional binary classification. The proposed binary classification strategy is then applied to enhance ECOC. Experiments on document categorization and question classification show its effectiveness. {\textcopyright} 2010 Springer-Verlag Berlin Heidelberg.},
  doi       = {10.1007/978-3-642-13059-5_4},
  isbn      = {3642130585},
  issn      = {03029743},
  keywords  = {Binary Classification, Error Correcting Output Coding, Text Classification},
  url       = {https://doi.org/10.1007/978-3-642-13059-5{\_}4},
}

@Article{Li2016a,
  author   = {Li, Jianqiang and Li, Jing and Fu, Xianghua and Masud, M. A. and Huang, Joshua Zhexue},
  journal  = {Knowledge-Based Systems},
  title    = {{Learning distributed word representation with multi-contextual mixed embedding}},
  year     = {2016},
  issn     = {09507051},
  pages    = {220--230},
  volume   = {106},
  abstract = {Learning distributed word representations has been a popular method for various natural language processing applications such as word analogy and similarity, document classification and sentiment analysis. However, most existing word embedding models only exploit a shallow slide window as the context to predict the target word. Because the semantic of each word is also influenced by its global context, as the distributional models usually induced the word representations from the global co-occurrence matrix, the window-based models are insufficient to capture semantic knowledge. In this paper, we propose a novel hybrid model called mixed word embedding (MWE) based on the well-known word2vec toolbox. Specifically, the proposed MWE model combines the two variants of word2vec, i.e.; SKIP-GRAM and CBOW, in a seamless way via sharing a common encoding structure, which is able to capture the syntax information of words more accurately. Furthermore, it incorporates a global text vector into the CBOW variant so as to capture more semantic information. Our MWE preserves the same time complexity as the SKIP-GRAM. To evaluate our MWE model efficiently and adaptively, we study our model on linguistic and application perspectives with both English and Chinese dataset. For linguistics, we conduct empirical studies on word analogies and similarities. The learned latent representations on both document classification and sentiment analysis are considered for application point of view of this work. The experimental results show that our MWE model is very competitive in all tasks as compared with the state-of-the-art word embedding models such as CBOW, SKIP-GRAM, and GloVe.},
  doi      = {10.1016/j.knosys.2016.05.045},
  keywords = {Distributed word representation, Natural language processing, Word embedding, Word2vec},
}

@Article{Tang2016a,
  author        = {Tang, Bo and Kay, Steven and He, Haibo},
  journal       = {IEEE Transactions on Knowledge and Data Engineering},
  title         = {{Toward Optimal Feature Selection in Naive Bayes for Text Categorization}},
  year          = {2016},
  issn          = {10414347},
  number        = {9},
  pages         = {2508--2521},
  volume        = {28},
  abstract      = {Automated feature selection is important for text categorization to reduce feature size and to speed up learning process of classifiers. In this paper, we present a novel and efficient feature selection framework based on the Information Theory, which aims to rank the features with their discriminative capacity for classification. We first revisit two information measures: Kullback-Leibler divergence and Jeffreys divergence for binary hypothesis testing, and analyze their asymptotic properties relating to type I and type II errors of a Bayesian classifier. We then introduce a new divergence measure, called Jeffreys-Multi-Hypothesis (JMH) divergence, to measure multi-distribution divergence for multi-class classification. Based on the JMH-divergence, we develop two efficient feature selection methods, termed maximum discrimination (MD) and MD-$\chi$2 methods, for text categorization. The promising results of extensive experiments demonstrate the effectiveness of the proposed approaches.},
  archiveprefix = {arXiv},
  arxivid       = {1602.02850},
  doi           = {10.1109/TKDE.2016.2563436},
  eprint        = {1602.02850},
  keywords      = {Feature selection, Jeffreys divergence, Kullback-Leibler divergence, feature reduction, information gain, text categorization},
}

@Article{Pang2015,
  author   = {Pang, Guansong and Jin, Huidong and Jiang, Shengyi},
  journal  = {Data Mining and Knowledge Discovery},
  title    = {{CenKNN: a scalable and effective text classifier}},
  year     = {2015},
  issn     = {13845810},
  number   = {3},
  pages    = {593--625},
  volume   = {29},
  abstract = {A big challenge in text classification is to perform classification on a large-scale and high-dimensional text corpus in the presence of imbalanced class distributions and a large number of irrelevant or noisy term features. A number of techniques have been proposed to handle this challenge with varying degrees of success. In this paper, by combining the strengths of two widely used text classification techniques, K-Nearest-Neighbor (KNN) and centroid based (Centroid) classifiers, we propose a scalable and effective flat classifier, called CenKNN, to cope with this challenge. CenKNN projects high-dimensional (often hundreds of thousands) documents into a low-dimensional (normally a few dozen) space spanned by class centroids, and then uses the {\$}{\$}k{\$}{\$}k-d tree structure to find {\$}{\$}K{\$}{\$}K nearest neighbors efficiently. Due to the strong representation power of class centroids, CenKNN overcomes two issues related to existing KNN text classifiers, i.e., sensitivity to imbalanced class distributions and irrelevant or noisy term features. By working on projected low-dimensional data, CenKNN substantially reduces the expensive computation time in KNN. CenKNN also works better than Centroid since it uses all the class centroids to define similarity and works well on complex data, i.e., non-linearly separable data and data with local patterns within each class. A series of experiments on both English and Chinese, benchmark and synthetic corpora demonstrates that although CenKNN works on a significantly lower-dimensional space, it performs substantially better than KNN and its five variants, and existing scalable classifiers, including Centroid and Rocchio. CenKNN is also empirically preferable to another well-known classifier, support vector machines, on highly imbalanced corpora with a small number of classes.},
  doi      = {10.1007/s10618-014-0358-x},
  keywords = {Centroid, Dimension reduction, Imbalanced classification, KNN, Text classification},
}

@Article{Arras2017,
  author        = {Arras, Leila and Horn, Franziska and Montavon, Gr{\'{e}}goire and M{\"{u}}ller, Klaus Robert and Samek, Wojciech},
  journal       = {PLoS ONE},
  title         = {{"What is relevant in a text document?": An interpretable machine learning approach}},
  year          = {2017},
  issn          = {19326203},
  number        = {8},
  pages         = {1--23},
  volume        = {12},
  abstract      = {Text documents can be described by a number of abstract concepts such as semantic category, writing style, or sentiment. Machine learning (ML) models have been trained to automatically map documents to these abstract concepts, allowing to annotate very large text collections, more than could be processed by a human in a lifetime. Besides predicting the text's category very accurately, it is also highly desirable to understand how and why the categorization process takes place. In this paper, we demonstrate that such understanding can be achieved by tracing the classification decision back to individual words using layer-wise relevance propagation (LRP), a recently developed technique for explaining predictions of complex non-linear classifiers. We train two word-based ML models, a convolutional neural network (CNN) and a bag-of-words SVM classifier, on a topic categorization task and adapt the LRP method to decompose the predictions of these models onto words. Resulting scores indicate how much individual words contribute to the overall classification decision. This enables one to distill relevant information from text documents without an explicit semantic information extraction step. We further use the word-wise relevance scores for generating novel vector-based document representations which capture semantic information. Based on these document vectors, we introduce a measure of model explanatory power and show that, although the SVM and CNN models perform similarly in terms of classification accuracy, the latter exhibits a higher level of explainability which makes it more comprehensible for humans and potentially more useful for other applications.},
  archiveprefix = {arXiv},
  arxivid       = {1612.07843},
  doi           = {10.1371/journal.pone.0181142},
  eprint        = {1612.07843},
  pmid          = {28800619},
  publisher     = {Public Library of Science},
  url           = {https://doi.org/10.1371/journal.pone.0181142},
}

@article{Berge2019,
abstract = {Medical applications challenge today's text categorization techniques by demanding both high accuracy and ease-of-interpretation. Although deep learning has provided a leap ahead in accuracy, this leap comes at the sacrifice of interpretability. To address this accuracy-interpretability challenge, we here introduce, for the first time, a text categorization approach that leverages the recently introduced Tsetlin Machine. In all brevity, we represent the terms of a text as propositional variables. From these, we capture categories using simple propositional formulae, such as: if "rash" and "reaction" and "penicillin" then Allergy. The Tsetlin Machine learns these formulae from a labelled text, utilizing conjunctive clauses to represent the particular facets of each category. Indeed, even the absence of terms (negated features) can be used for categorization purposes. Our empirical results are quite conclusive. The Tsetlin Machine either performs on par with or outperforms all of the evaluated methods on both the 20 Newsgroups and IMDb datasets, as well as on a non-public clinical dataset. On average, the Tsetlin Machine delivers the best recall and precision scores across the datasets. The GPU implementation of the Tsetlin Machine is further 8 times faster than the GPU implementation of the neural network. We thus believe that our novel approach can have a significant impact on a wide range of text analysis applications, forming a promising starting point for deeper natural language understanding with the Tsetlin Machine.},
archivePrefix = {arXiv},
arxivId = {1809.04547},
author = {Berge, Geir Thore and Granmo, Ole-Christoffer and Tveit, Tor Oddbjorn and Goodwin, Morten and Jiao, Lei and Matheussen, Bernt Viggo},
doi = {10.1109/access.2019.2935416},
eprint = {1809.04547},
journal = {IEEE Access},
pages = {115134--115146},
title = {{Using the Tsetlin Machine to Learn Human-Interpretable Rules for High-Accuracy Text Categorization With Medical Applications}},
volume = {7},
year = {2019}
}

@Article{Tang2020,
  author   = {Tang, Zhong and Li, Wenqiang and Li, Yan},
  journal  = {Concurrency Computation},
  title    = {{An improved term weighting scheme for text classification}},
  year     = {2020},
  issn     = {15320634},
  number   = {9},
  volume   = {32},
  abstract = {Text representation is a necessary and primary procedure in performing text classification (TC), which first needs to be obtained through an information-rich term weighting scheme to achieve higher TC performance. So far, term frequency-inverse document frequency (TF-IDF) is the most widely used term weighting scheme, but it suffers from two deficiencies. First, the global weighting factors IDF in TF-IDF approaches infinity if a certain term does not occur in a text. Second, the IDF is equal to zero if a certain term appears in any text. To offset these drawbacks, we first conduct an in-depth analysis of the current term weighting schemes, and subsequently, an improved term weighting scheme called term frequency-inverse exponential frequency (TF-IEF) and its various variants are proposed. The proposed method replaces IDF with the new global weighting factor IEF to characterize the global weighting factor log-like IDF in the corpus, which can greatly reduce the effect of feature (term) with high local weighting factor TF in term weighting. As a result, a more representative feature can be generated. We carried out a series of experiments on two commonly used data sets (corpora) utilizing Na{\"{i}}ve Bayes and support vector machine classifiers to validate the performance of our proposed schemes. Experimental results explicitly reveal that the proposed term weighting schemes come with better performance than the compared schemes.},
  doi      = {10.1002/cpe.5604},
  keywords = {TF-IEF, feature selection, term weighting, text classification, text representation},
}

@Article{Gao2018,
  author   = {Yang, Gao and Wenbo, Wang and Qian, Liu and Heyan, Huang and Li, Yuefeng},
  journal  = {IEEE Access},
  title    = {{Extending Embedding Representation by Incorporating Latent Relations}},
  year     = {2018},
  issn     = {21693536},
  pages    = {52682--52690},
  volume   = {6},
  abstract = {The semantic representation of words is a fundamental task in natural language processing and text mining. Learning word embedding has shown its power on various tasks. Most studies are aimed at generating embedding representation of a word based on encoding its context information. However, many latent relations, such as co-occurring associative patterns and semantic conceptual relations, are not well considered. In this paper, we propose an extensible model to incorporate these kinds of valuable latent relations to increase the semantic relatedness of word pairs by learning word embeddings. To assess the effectiveness of our model, we conduct experiments on both information retrieval and text classification tasks. The results indicate the effectiveness of our model as well as its flexibility on different tasks.},
  doi      = {10.1109/ACCESS.2018.2866531},
  keywords = {Word embedding, natural language processing, text mining},
}

@InProceedings{Hassaine2017,
  author    = {Hassaine, Abdelaali and Safi, Zeineb and Otaibi, Jameela and Jaoua, Ali},
  booktitle = {Proceedings of IEEE/ACS International Conference on Computer Systems and Applications, AICCSA},
  title     = {{Text categorization using weighted hyper rectangular keyword extraction}},
  year      = {2017},
  pages     = {959--965},
  publisher = {{\{}IEEE{\}} Computer Society},
  volume    = {2017-Octob},
  abstract  = {Text categorization is an important research field that finds many applications nowadays. It is usually performed in two steps: feature extraction and classification. In the feature extraction step, discriminating keywords are extracted in order to distinguish between different categories of documents. In the classification step, the extracted keywords are fed to a classifier in order to detect the category of each document. In this paper, we use the hyper rectangle method which represents the corpus of documents using a binary relation in which the documents correspond to objects and words to attributes. The hyper rectangle method extracts a tree of keywords such that most discriminative keywords are at the top levels and less discriminative keywords are in the deep levels. We are particularly interested to study different proposed weighting metrics that yield different orderings of keywords. We study how these weighting metrics impact the categorization performance. For the classification step we used both a logistic regression and random forests classifiers. We tested our method on both the 20 newsgroups dataset as well as the Reuters R8 dataset. Our method achieves high performance on both datasets which compete very well with state-of-the-art methods.},
  doi       = {10.1109/AICCSA.2017.102},
  isbn      = {9781538635810},
  issn      = {21615330},
}

@InProceedings{Li2016,
  author    = {Li, Baoli},
  booktitle = {Proceedings of the 2016 International Conference on Asian Language Processing, IALP 2016},
  title     = {{Importance weighted feature selection strategy for text classification}},
  year      = {2016},
  editor    = {Dong, Minghui and Tseng, Yuen-Hsien and Lu, Yanfeng and Yu, Liang-Chih and Lee, Lung-Hao and Wu, Chung-Hsien and Li, Haizhou},
  pages     = {344--347},
  publisher = {IEEE},
  abstract  = {Feature selection, which aims at obtaining a compact and effective feature subset for better performance and higher efficiency, has been studied for decades. The traditional feature selection metrics, such as Chi-square and information gain, fail to consider how important a feature is in a document. Features, no matter how much effective semantic information they hold, are treated equally. Intuitively, thus calculated feature selection metrics are very likely to introduce much noise. We, therefore, in this study, extend the work of Li et al. [1] on document frequency metric, propose a general importance weighted feature selection strategy for text classification, in which the importance value of a feature in a document is derived from its relative frequency in that document. Extensive experiments with two state-of-The-Art feature selection metrics (Chi-square and information gain) on three text classification datasets demonstrate the effectiveness of the proposed strategy.},
  doi       = {10.1109/IALP.2016.7876002},
  isbn      = {9781509009213},
  keywords  = {Chi-square, Feature Selection, Importance Weighted Feature Selection, Information Gain, Text Classification},
}

@InProceedings{Li2015,
  author    = {Li, Baoli and Yan, Qiuling and Xu, Zhenqiang and Wang, Guicai},
  booktitle = {Proceedings of 2015 International Conference on Asian Language Processing, IALP 2015},
  title     = {{Weighted Document Frequency for feature selection in text classification}},
  year      = {2015},
  pages     = {132--135},
  publisher = {IEEE},
  abstract  = {In the past research, Document Frequency (DF) has been validated to be a simple yet quite effective measure for feature selection in text classification. The calculation is based on how many documents in a collection contain a feature, which can be a word, a phrase, a n-gram, or a specially derived attribute. The counting process takes a binary strategy: if a feature appears in a document, its DF will be increased by one. This traditional DF metric concerns only about whether a feature appears in a document, but does not consider how important the feature is in that document. Obviously, thus counted document frequency is very likely to introduce much noise. Therefore, a weighted document frequency (WDF) is proposed and expected to reduce such noise to some extent. Extensive experiments on two text classification datasets demonstrate the effectiveness of the proposed measure.},
  doi       = {10.1109/IALP.2015.7451549},
  isbn      = {9781467395953},
  keywords  = {Document Frequency, Feature Selection, Text Categorization, Text Classification, Weighted Document Frequency},
}

@Article{AlSalemi2016,
  author   = {Al-Salemi, Bassam and {Mohd Noah}, Shahrul Azman and {Ab Aziz}, Mohd Juzaiddin},
  journal  = {Knowledge-Based Systems},
  title    = {{RFBoost: An improved multi-label boosting algorithm and its application to text categorisation}},
  year     = {2016},
  issn     = {09507051},
  pages    = {104--117},
  volume   = {103},
  abstract = {The AdaBoost.MH boosting algorithm is considered to be one of the most accurate algorithms for multi-label classification. AdaBoost.MH works by iteratively building a committee of weak hypotheses of decision stumps. In each round of AdaBoost.MH learning, all features are examined, but only one feature is used to build a new weak hypothesis. This learning mechanism may entail a high degree of computational time complexity, particularly in the case of a large-scale dataset. This paper describes a way to manage the learning complexity and improve the classification performance of AdaBoost.MH. We propose an improved version of AdaBoost.MH, called RFBoost. The weak learning in RFBoost is based on filtering a small fixed number of ranked features in each boosting round rather than using all features, as AdaBoost.MH does. We propose two methods for ranking the features: One Boosting Round and Labeled Latent Dirichlet Allocation (LLDA), a supervised topic model based on Gibbs sampling. Additionally, we investigate the use of LLDA as a feature selection method for reducing the feature space based on the maximal conditional probabilities of words across labels. Our experimental results on eight well-known benchmarks for multi-label text categorisation show that RFBoost is significantly more efficient and effective than the baseline algorithms. Moreover, the LLDA-based feature ranking yields the best performance for RFBoost.},
  doi      = {10.1016/j.knosys.2016.03.029},
  keywords = {AdaBoost.MH, Boosting, Labeled Latent Dirichlet Allocation, Multi-label classification, RFBoost, Text categorisation},
}

@Article{AlSalemi2018,
  author   = {Al-Salemi, Bassam and Ayob, Masri and Noah, Shahrul Azman Mohd},
  journal  = {Expert Systems with Applications},
  title    = {{Feature ranking for enhancing boosting-based multi-label text categorization}},
  year     = {2018},
  issn     = {09574174},
  pages    = {531--543},
  volume   = {113},
  abstract = {Boosting algorithms have been proved effective for multi-label learning. As ensemble learning algorithms, boosting algorithms build classifiers by composing a set of weak hypotheses. The high computational cost of boosting algorithms in learning from large volumes of data such as text categorization datasets is a real challenge. Most boosting algorithms, such as AdaBoost.MH, iteratively examine all training features to generate the weak hypotheses, which increases the learning time. RFBoost was introduced to manage this problem based on a rank-and-filter strategy in which it first ranks the training features and then, in each learning iteration, filters and uses only a subset of the highest-ranked features to construct the weak hypotheses. This step ensures accelerated learning time for RFBoost compared to AdaBoost.MH, as the weak hypotheses produced in each iteration are reduced to a very small number. As feature ranking is the core idea of RFBoost, this paper presents and investigates seven feature ranking methods (information gain, chi-square, GSS-coefficient, mutual information, odds ratio, F1 score, and accuracy) in order to improve RFBoost's performance. Moreover, an accelerated version of RFBoost, called RFBoost1, is also introduced. Rather than filtering a subset of the highest-ranked features, FBoost1 selects only one feature, based on its weight, to build a new weak hypothesis. Experimental results on four benchmark datasets for multi-label text categorization) Reuters-21578, 20-Newsgroups, OHSUMED, and TMC2007(demonstrate that among the methods evaluated for feature ranking, mutual information yields the best performance for RFBoost. In addition, the results prove that RFBoost statistically outperforms both RFBoost1 and AdaBoost.MH on all datasets. Finally, RFBoost1 proved more efficient than AdaBoost.MH, making it a better alternative for addressing classification problems in real-life applications and expert systems.},
  doi      = {10.1016/j.eswa.2018.07.024},
  keywords = {Boosting, Feature ranking, Multi-label learning, RFBoost, Text categorization},
}

@Article{Jin2010,
  author   = {Jin, Xiao Bo and Liu, Cheng Lin and Hou, Xinwen},
  journal  = {Pattern Recognition},
  title    = {{Regularized margin-based conditional log-likelihood loss for prototype learning}},
  year     = {2010},
  issn     = {00313203},
  number   = {7},
  pages    = {2428--2438},
  volume   = {43},
  abstract = {The classification performance of nearest prototype classifiers largely relies on the prototype learning algorithm. The minimum classification error (MCE) method and the soft nearest prototype classifier (SNPC) method are two important algorithms using misclassification loss. This paper proposes a new prototype learning algorithm based on the conditional log-likelihood loss (CLL), which is based on the discriminative model called log-likelihood of margin (LOGM). A regularization term is added to avoid over-fitting in training as well as to maximize the hypothesis margin. The CLL in the LOGM algorithm is a convex function of margin, and so, shows better convergence than the MCE. In addition, we show the effects of distance metric learning with both prototype-dependent weighting and prototype-independent weighting. Our empirical study on the benchmark datasets demonstrates that the LOGM algorithm yields higher classification accuracies than the MCE, generalized learning vector quantization (GLVQ), soft nearest prototype classifier (SNPC) and the robust soft learning vector quantization (RSLVQ), and moreover, the LOGM with prototype-dependent weighting achieves comparable accuracies to the support vector machine (SVM) classifier. Crown Copyright {\textcopyright} 2010.},
  doi      = {10.1016/j.patcog.2010.01.013},
  keywords = {Conditional log-likelihood loss, Distance metric learning, Log-likelihood of margin (LOGM), Prototype learning, Regularization},
}

@Article{Hu2017,
  author   = {Hu, Junying and Zhang, Jiangshe and Ji, Nannan and Zhang, Chunxia},
  journal  = {Knowledge-Based Systems},
  title    = {{A new regularized restricted Boltzmann machine based on class preserving}},
  year     = {2017},
  issn     = {09507051},
  pages    = {1--12},
  volume   = {123},
  abstract = {It is known that an Restricted Boltzmann machine (RBM) can be used as a feature extractor to automatically extract data features in a completely unsupervised learning manner. In this paper, we develop a new regularized RBM by adding the class information, referred to as class preserving RBM (CPr-RBM). Specifically, we impose two constraints on RBM to make the class information clearly reflected in extracted features. One constraint can decrease the distance between the features of the same class and the other one can increase the distance between the features of different classes. The two constraints introduce class information to RBM and make the extracted features contain more category information which contributes to a better classification result. Experiments are conducted on MNIST dataset and 20-newgroup dataset, which show that CPr-RBM learns more discriminate representations and outperforms other related state-of-the-art models in dealing with classification problems.},
  doi      = {10.1016/j.knosys.2017.02.012},
  keywords = {Class preserving, Feature learning, Restricted Boltzmann machine},
}

@InProceedings{Tesar2006,
  author    = {Tesar, Roman and Poesio, Massimo and Strnad, Vaclav and Jezek, Karel},
  booktitle = {Proceedings of the 2006 ACM Symposium on Document Engineering, DocEng 2006},
  title     = {{Extending the single words-based document model: A comparison of bigrams and 2-itemsets}},
  year      = {2006},
  editor    = {Bulterman, Dick C A and Brailsford, David F},
  pages     = {138--146},
  publisher = {ACM},
  volume    = {2006},
  abstract  = {The basic approach in text categorization is to represent documents by single words. However, often other features are utilized to achieve better classification results. In this paper, our attention is focused on bigrams and 2-itemsets. We compare the performance improvement in terms of classification accuracy when these features are used to extend the single words-based document representation on two standard text corpora: Reuters-21578 and 20 Newsgroups. For this comparison we use the multinomial Naive Bayes classifier and five different feature selection approaches. Algorithms for bigrams and 2-itemsets discovery are presented as well. Our results show a statistically significant improvement when bigrams and also 2-itemsets are incorporated. However, in the case of 2-itemsets it is important to use an appropriate feature selection method. On the other hand, even when a simple feature selection approach is applied to discover bigrams the classification accuracy improves. The conclusion is that, in our case, it is not very effective to extend document representation with 2-itemsets because bigrams achieve better results and discovering them is less resource-consuming. Copyright 2006 ACM.},
  doi       = {10.1145/1166160.1166197},
  isbn      = {1595935150},
  keywords  = {Bigrams, Comparison, Document model, Feature selection, Itemsets, Machine learning, N-grams, Text categorization},
}

@inproceedings{Yan2008,
abstract = {Dimension reduction for large-scale text data is attracting much attention lately due to the rapid growth of World Wide Web. We can consider cdimension reduction algorithms in two categories: feature extraction and feature selection. An important problem remains: it has been difficult to integrate these two algorithm categories into a single framework, making it difficult to reap the benefit of both. In this paper, we formulate the two algorithm categories through a unified optimization framework. Under this framework, we develop a novel feature selection algorithm called Trace Oriented Feature Analysis (TOFA). The novel objective function of TOFA is a unified framework that integrates many prominent feature extraction algorithms such as unsupervised Principal Component Analysis and supervised Maximum Margin Criterion are special cases of it. Thus TOFA can process not only supervised problem but also unsupervised and semi-supervised problems. Experimental results on real text datasets demonstrate the ef ectiveness and efficiency of TOFA. {\textcopyright} 2008 IEEE.},
author = {Yan, Jun and Liu, Ning and Yang, Qiang and Fan, Weiguo and Chen, Zheng},
booktitle = {Proceedings - IEEE International Conference on Data Mining, ICDM},
doi = {10.1109/ICDM.2008.67},
isbn = {9780769535029},
issn = {15504786},
pages = {668--677},
publisher = {{\{}IEEE{\}} Computer Society},
title = {{TOFA: Trace oriented feature analysis in text categorization}},
year = {2008}
}

@Article{Benites2017,
  author   = {Benites, Fernando and Sapozhnikova, Elena},
  journal  = {Neurocomputing},
  title    = {{Improving scalability of ART neural networks}},
  year     = {2017},
  issn     = {18728286},
  pages    = {219--229},
  volume   = {230},
  abstract = {With the increasing amount of available data, the need for classification of large data volumes is permanently growing. In order to cope with this challenge, neural classifiers should be adapted to large-scale data. We present here a well scalable extension to the fuzzy Adaptive Resonance Associative Map (ARAM) neural network, which was specially developed for the quick classification of high-dimensional and large data. This extension aims at increasing the classification speed by adding an extra layer for clustering learned prototypes into large clusters. This enables the activation of only one or a few clusters i.e. a small fraction of all prototypes, reducing the classification time significantly. Further we introduce two methods to adapt this extension to a multi-label classification task.},
  doi      = {10.1016/j.neucom.2016.12.022},
  keywords = {Adaptive resonance theory, Classification, Clustering, Multi-label classification, Neural networks},
}

@InCollection{Bramesh2019,
  author    = {Bramesh, S. M. and {Anil Kumar}, K. M.},
  booktitle = {Lecture Notes in Electrical Engineering},
  publisher = {Springer},
  title     = {{Empirical Study to Evaluate the Performance of Classification Algorithms on Public Datasets}},
  year      = {2019},
  isbn      = {9789811358012},
  pages     = {447--455},
  volume    = {545},
  abstract  = {In today's world, a huge amount of data is stored in the form of electronic documents in the World Wide Web. Text classification algorithms have been widely used for classifying those text documents into a fixed number of predefined classes. The applicable scopes and their performances of these algorithms are different. Therefore, finding an appropriate algorithm for a dataset is becoming a significant emphasis for researchers to solve practical problems quickly. This paper puts forward an experimental evaluation of five significant text classification algorithms with each other and with TF and TF-IDF feature selection methods built using decision tree (C5.0), support vector machine, K-nearest neighbor, Na{\"{i}}ve Bayes, and neural network on four public datasets, namely 20news-bydate, ohsumed-first-20000-docs, Reuters 21578-Apte-90 Cat, and 20 Newsgroup. The experimental results are examined from multiple perspectives and summarized to provide usefulness of different algorithms on different datasets.},
  doi       = {10.1007/978-981-13-5802-9_41},
  issn      = {18761119},
  keywords  = {Decision tree, K-nearest neighbor, Na{\"{i}}ve Bayes, Neural network, RStudio, Support vector machine, Text classification},
}

@Article{Luo2011,
  author   = {Luo, Qiming and Chen, Enhong and Xiong, Hui},
  journal  = {Expert Systems with Applications},
  title    = {{A semantic term weighting scheme for text categorization}},
  year     = {2011},
  issn     = {09574174},
  number   = {10},
  pages    = {12708--12716},
  volume   = {38},
  abstract = {Traditional term weighting schemes in text categorization, such as TF-IDF, only exploit the statistical information of terms in documents. Instead, in this paper, we propose a novel term weighting scheme by exploiting the semantics of categories and indexing terms. Specifically, the semantics of categories are represented by senses of terms appearing in the category labels as well as the interpretation of them by WordNet. Also, the weight of a term is correlated to its semantic similarity with a category. Experimental results on three commonly used data sets show that the proposed approach outperforms TF-IDF in the cases that the amount of training data is small or the content of documents is focused on well-defined categories. In addition, the proposed approach compares favorably with two previous studies. {\textcopyright} 2010 Elsevier Ltd. All rights reserved.},
  doi      = {10.1016/j.eswa.2011.04.058},
  keywords = {Semantic term weighting, TF-IDF, Text categorization, WordNet},
}

@InProceedings{Mazyad2017,
  author    = {Mazyad, Ahmad and Teytaud, Fabien and Fonlupt, Cyril},
  booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  title     = {{A comparative study on term weighting schemes for text classification}},
  year      = {2017},
  editor    = {Nicosia, Giuseppe and Pardalos, Panos M and Giuffrida, Giovanni and Umeton, Renato},
  pages     = {100--108},
  publisher = {Springer},
  series    = {Lecture Notes in Computer Science},
  volume    = {10710 LNCS},
  abstract  = {Text Classification (or Text Categorization) is a popular machine learning task. It consists in assigning categories to documents. In this paper, we are interested in comparing state of the art classifiers and state of the art feature weights. Feature weight methods are classic tools that are used in text categorization. We extend previous studies by evaluating numerous term weighting schemes for state of the art classification methods. We aim at providing a complete survey on text classification for fair benchmark comparisons.},
  doi       = {10.1007/978-3-319-72926-8_9},
  isbn      = {9783319729251},
  issn      = {16113349},
  url       = {https://doi.org/10.1007/978-3-319-72926-8{\_}9},
}

@InProceedings{Xia2009,
  author    = {Xia, Feng and Jicun, Tian and Zhihui, Liu},
  booktitle = {6th International Conference on Fuzzy Systems and Knowledge Discovery, FSKD 2009},
  title     = {{A text categorization method based on local document frequency}},
  year      = {2009},
  pages     = {468--471},
  publisher = {IEEE Press},
  series    = {FSKD'09},
  volume    = {7},
  abstract  = {In this paper, a fast and effective text categorization method named TCBLDF is proposed. TCBLDF barely needs dimensionality reduction except a stop words removal and a document frequency based feature selection. It tries to capture the relationship between a term and a category label, thus eliminates the need to know the semantic contribution of a term makes to a document it occurs in. TCBLDF use a measure to evaluate the importance of each term for the categorization task, and then gives different weights to them according to the importance evaluations. By doing so, we can make important terms affect more when making classification decision. At last we compare the method to two conventional classification methods, a Naive Bayesian learning and a linear SVM learning method. Experimental results show that TCBLDF is faster than SVM with a comparable performance and more effective than Naive Bayes, thus can be a good alternative to these methods. {\textcopyright} 2009 IEEE.},
  doi       = {10.1109/FSKD.2009.291},
  isbn      = {9780769537351},
  keywords  = {Local document frequency, Text categorization},
}

@Article{Zong2015,
  author   = {Zong, Wei and Wu, Feng and Chu, Lap Keung and Sculli, Domenic},
  journal  = {International Journal of Production Economics},
  title    = {{A discriminative and semantic feature selection method for text categorization}},
  year     = {2015},
  issn     = {09255273},
  pages    = {215--222},
  volume   = {165},
  abstract = {Abstract Text categorization is an important and critical task in the current era of high volume data storage and handling. Feature selection is obviously one of the most important steps in text categorization. Traditional feature selection methods tend to only consider the correlation between features and categories, and have in the main ignored the semantic similarity between features and documents. To further explore this issue, this paper proposes a novel feature selection method that first selects features in documents with discriminative power and then computes the semantic similarity between features and documents. The proposed feature selection method is tested using a support vector machine (SVM) classifier upon two published datasets, viz. Reuters-21578 and 20-Newsgroups. The experimental results show that the proposed feature selection method generally outperforms the traditional feature selection methods for text categorization for both published datasets.},
  doi      = {10.1016/j.ijpe.2014.12.035},
  keywords = {Big data, Discriminative power, Feature selection, Semantic similarity, Support vector machine (SVM), Text categorization},
  url      = {http://www.sciencedirect.com/science/article/pii/S0925527314004290},
}

@InProceedings{Sharma2017a,
  author       = {Sharma, Neeraj and Dileep, A. D. and Thenkanidiyoor, Veena},
  booktitle    = {Proceedings - 16th IEEE International Conference on Machine Learning and Applications, ICMLA 2017},
  title        = {{Text classification using hierarchical sparse representation classifiers}},
  year         = {2017},
  organization = {IEEE},
  pages        = {1015--1019},
  volume       = {2017-Decem},
  abstract     = {In this paper, we propose to use sparse representation classifier (SRC) for text classification. The sparse representation of an example is obtained by using an overcomplete dictionary made up of term frequency (TF) vectors corresponding to all the training documents. We propose to seed the dictionary using principal components of TF vector representation corresponding to training text documents. In this work, we also propose 2-level hierarchical SRC (HSRC) by exploiting the similarity among the classes. We propose to use weighted decomposition principal component analysis (WDPCA) in the second level of HSRC to seed the dictionary to discriminate the similar classes. The effectiveness of the proposed approach to build HSRC for text classification is demonstrated on 20 Newsgroup Corpus.},
  doi          = {10.1109/ICMLA.2017.00-18},
  isbn         = {9781538614174},
  keywords     = {Hierarchical Sparse Representation Classifiers, Sparse Representation Classifiers, Text Classification, Weighted decomposition principal component analysi},
}

@inproceedings{Jin2016,
abstract = {Words are central to text classification. It has been shown that simple Naive Bayes models with word and bigram features can give highly competitive accuracies when compared to more sophisticated models with part-of-speech, syntax and semantic features. Embeddings offer distributional features about words. We study a conceptually simple classification model by exploiting multiprototype word embeddings based on text classes. The key assumption is that words exhibit different distributional characteristics under different text classes. Based on this assumption, we train multiprototype distributional word representations for different text classes. Given a new document, its text class is predicted by maximizing the probabilities of embedding vectors of its words under the class. In two standard classification benchmark datasets, one is balance and the other is imbalance, our model outperforms state-of-the-art systems, on both accuracy and macro-average F-1 score.},
author = {Jin, Peng and Yue, Zhang and Chen, Xingyuan and Xia, Yunqing},
booktitle = {IJCAI International Joint Conference on Artificial Intelligence},
editor = {Kambhampati, Subbarao},
issn = {10450823},
pages = {2824--2830},
publisher = {{\{}IJCAI/AAAI{\}} Press},
title = {{Bag-of-embeddings for text classification}},
url = {http://www.ijcai.org/Abstract/16/401},
volume = {2016-Janua},
year = {2016}
}

@Article{Tang2016,
  author   = {Tang, Bo and He, Haibo and Baggenstoss, Paul M. and Kay, Steven},
  journal  = {IEEE Transactions on Knowledge and Data Engineering},
  title    = {{A Bayesian Classification Approach Using Class-Specific Features for Text Categorization}},
  year     = {2016},
  issn     = {10414347},
  number   = {6},
  pages    = {1602--1606},
  volume   = {28},
  abstract = {In this paper, we present a Bayesian classification approach for automatic text categorization using class-specific features. Unlike conventional text categorization approaches, our proposed method selects a specific feature subset for each class. To apply these class-specific features for classification, we follow Baggenstoss's PDF Projection Theorem (PPT) to reconstruct the PDFs in raw data space from the class-specific PDFs in low-dimensional feature subspace, and build a Bayesian classification rule. One noticeable significance of our approach is that most feature selection criteria, such as Information Gain (IG) and Maximum Discrimination (MD), can be easily incorporated into our approach. We evaluate our method's classification performance on several real-world benchmarks, compared with the state-of-the-art feature selection approaches. The superior results demonstrate the effectiveness of the proposed approach and further indicate its wide potential applications in data mining.},
  doi      = {10.1109/TKDE.2016.2522427},
  keywords = {Feature selection, PDF projection and estimation, class-specific features, dimension reduction, naive Bayes, text categorization},
}

@inproceedings{Zheng2016,
abstract = {Taking advantage of the large scale corpus on the web to effectively and efficiently mine the topics within texts is an essential problem in the era of big data. We focus on the problem of learning text topic embedding in an unsupervised manner, which enjoys the properties of efficiency and scalability. Text topic embedding represents words and documents in a semantic topic space, in which the words and documents with similar topic will be embedded close to each other. When compared with conventional topic models, which implicitly capture the document-level word co-occurrence patterns, text topic embedding alleviates the data sparsity problem and captures the semantic relevance between different words and documents. To model text topic embedding, we propose a Bidirectional Hierarchical Skip-Gram model (BHSG) based on skip-gram model. BHSG includes two components: semantic generation module to learn semantic relevance between texts and topic enhance module to produce the text topic embedding based on text embedding learned in the former module. We evaluated our method on two kinds of topic-related tasks: text classification and information retrieval. The experimental results on four public datasets and one dataset we provide all demonstrate that our proposed method can achieve a better performance.},
author = {Zheng, Suncong and Bao, Hongyun and Xu, Jiaming and Hao, Yuexing and Qi, Zhenyu and Hao, Hongwei},
booktitle = {Proceedings of the International Joint Conference on Neural Networks},
doi = {10.1109/IJCNN.2016.7727289},
isbn = {9781509006199},
pages = {855--862},
publisher = {IEEE},
title = {{A Bidirectional Hierarchical Skip-Gram model for text topic embedding}},
volume = {2016-Octob},
year = {2016}
}

@Article{Unnikrishnan2019,
  author   = {Unnikrishnan, P. and Govindan, V. K. and {Madhu Kumar}, S. D.},
  journal  = {Expert Systems with Applications},
  title    = {{Enhanced sparse representation classifier for text classification}},
  year     = {2019},
  issn     = {09574174},
  pages    = {260--272},
  volume   = {129},
  abstract = {Classification of text based on its substance is an essential part of analysis to organize enormously large text data and to mine the salient information contained in it. It is gaining greater attention with the surge in the volume of on-line data available. Classical algorithms like k-NN (k-nearest neighbor), SVM (Support Vector Machine) and their variations have been observed to yield only reasonable results in addressing the problem, leaving enough room for further improvement. A class of algorithms commonly referred to as Sparse Methods has been emerged recently from compressive sensing and found numerous effective applications in many areas of data analysis and image processing. Sparse Methods as a tool for text analysis is an alley that is largely unexplored rigorously. This paper presents exploration of sparse representation-based methods for text classification. Based on the success of sparse representation based methods in different areas of data analysis, we intuitively hypothesized that it should work well on text classification problems as well. This paper empirically reinforces the hypothesis by testing the method on Reuters and WebKB data sets. The empirical results on Reuters and WebKB benchmark data show that it can outperform classical classification algorithms like SVM and k-NN. It has been observed that obtaining the basis of representation and sparse codes are computationally costly operations affecting the performance of the system. We also propose a class-wise dictionary refinement algorithm and dynamic dictionary selection algorithm to make sparse coding faster. The addition of dictionary refinement to the classification system not only reduces the time taken for sparse coding but also gives improved classification accuracy. The outcomes of the study are empirical verification of sparse representation classifier as a text classification tool and a computationally efficient solution for the bottleneck operation of sparse coding.},
  doi      = {10.1016/j.eswa.2019.04.003},
  keywords = {Dictionary, Orthogonal matching pursuit, Sparse representation, Text classification},
}

@InProceedings{Kesiraju2016,
  author    = {Kesiraju, Santosh and Burget, Luk{\'{a}}{\v{s}} and Szoke, Igor and {\v{C}}ernock{\'{y}}, Jan},
  booktitle = {Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
  title     = {{Learning document representations using subspace multinomial model}},
  year      = {2016},
  editor    = {Morgan, Nelson},
  pages     = {700--704},
  publisher = {ISCA},
  volume    = {08-12-Sept},
  abstract  = {Subspace multinomial model (SMM) is a log-linear model and can be used for learning low dimensional continuous representation for discrete data. SMMand its variants have been used for speaker verification based on prosodic features and phonotactic language recognition. In this paper, we propose a new variant of SMM that introduces sparsity and call the resulting model as ℓ1 SMM. We show that ℓ1 SMM can be used for learning document representations that are helpful in topic identification or classification and clustering tasks. Our experiments in document classification show that SMM achieves comparable results to models such as latent Dirichlet allocation and sparse topical coding, while having a useful property that the resulting document vectors are Gaussian distributed.},
  doi       = {10.21437/Interspeech.2016-1634},
  issn      = {19909772},
  keywords  = {Document representation, Latent topic discovery, Subspace modelling, Topic identification},
}

@InProceedings{Sharma2016,
  author       = {Sharma, Neeraj and Sharma, Anshu and Thenkanidiyoor, Veena and Dileep, A. D.},
  booktitle    = {2016 4th International Symposium on Computational and Business Intelligence, ISCBI 2016},
  title        = {{Text classification using combined sparse representation classifiers and support vector machines}},
  year         = {2016},
  organization = {IEEE},
  pages        = {181--185},
  abstract     = {Text classification is an important task in managing huge repository of textual content prevailing in various domains. In this paper, we propose to use sparse representation classifier (SRC) and support vector machines (SVMs) based classifiers using frequency-based kernels for text classification. We consider term-frequency (TF) representation for a text document. The sparse representation of an example is obtained by using an overcomplete dictionary made up of TF vectors corresponding to all the training documents [1]. We propose to seed the dictionary using principal components of TF vector representation corresponding to training text documents. SVM-based text classifiers use linear kernel or Gaussian kernel on the TF vector representation of documents. TF representation being a non-negative, histogram representation, we propose to build SVM-based text classifiers using frequency-based kernels such as histogram intersection kernel, Chi-square (X2) kernel and Hellinger's kernel. It is observed that the examples misclassified by one classifier is correctly classified in another classifier. To take advantage of the various classifiers, we introduce an approach to combine classifiers to improve the performance of text classification. The effectiveness of all the proposed techniques for text classification is demonstrated on 20 Newsgroup Corpus.},
  doi          = {10.1109/ISCBI.2016.7743280},
  isbn         = {9781509034888},
  keywords     = {Text classification, frequency-based kernels, sparse representation, sparse representation classifier, support vector machines},
}

@TechReport{Rennie2003,
  author      = {Rennie, Jason D M},
  institution = {MIT},
  title       = {{On the value of leave-one-out cross-validation bounds}},
  year        = {2003},
  url         = {https://pdfs.semanticscholar.org/4f8e/c2f5b9d6dc1bbccf6d8c03730c3cec4e35f7.pdf},
}

@inproceedings{Salakhutdinov2009,
abstract = {We introduce a two-layer undirected graphical model, called a "Replicated Soft-max", that can be used to model and automatically extract low-dimensional latent semantic representations from a large unstructured collection of documents. We present efficient learning and inference algorithms for this model, and show how a Monte-Carlo based method, Annealed Importance Sampling, can be used to produce an accurate estimate of the log-probability the model assigns to test data. This allows us to demonstrate that the proposed model is able to generalize much better compared to Latent Dirichlet Allocation in terms of both the log-probability of held-out documents and the retrieval accuracy.},
author = {Salakhutdinov, Ruslan and Hinton, Geoffrey},
booktitle = {Advances in Neural Information Processing Systems 22 - Proceedings of the 2009 Conference},
editor = {Bengio, Yoshua and Schuurmans, Dale and Lafferty, John D and Williams, Christopher K I and Culotta, Aron},
isbn = {9781615679119},
pages = {1607--1614},
publisher = {Curran Associates, Inc.},
title = {{Replicated softmax: An undirected topic model}},
url = {http://papers.nips.cc/paper/3856-replicated-softmax-an-undirected-topic-model},
year = {2009}
}

@InProceedings{Rodrigues2014,
  author    = {Rodrigues, Thiago Fredes and Engel, Paulo Martins},
  booktitle = {Proceedings - 2014 Brazilian Conference on Intelligent Systems, BRACIS 2014},
  title     = {{Probabilistic clustering and classification for textual data: An online and incremental approach}},
  year      = {2014},
  pages     = {288--293},
  publisher = {{\{}IEEE{\}} Computer Society},
  abstract  = {Given the amount of information stored in textual data and the fact that it is unstructured, algorithms able to process and transform it to a format useful to solve real world problems are desirable. Tasks like organization and exploration of large document collections can benefit from the design of such methods. This work proposes an incremental, online and probabilistic clustering algorithm for textual data, based on a mixture of Multinomial distributions. The main advantage of the model is that only a single step over the training data is necessary to learn from it. As more texts are processed, the model improves its structure to better represent the data stream.},
  doi       = {10.1109/BRACIS.2014.59},
  isbn      = {9781479956180},
  keywords  = {Document Classification, Document Clustering, Incremental Learning, Online Learning, Topic Modeling},
}

@inproceedings{Gliozzo2005,
abstract = {We propose a generalized bootstrapping algorithm in which categories are described by relevant seed features. Our method introduces two unsupervised steps that improve the initial categorization step of the bootstrapping scheme: (i) using Latent Semantic space to obtain a generalized similarity measure between instances and features, and (ii) the Gaussian Mixture algorithm, to obtain uniform classification probabilities for unlabeled examples. The algorithm was evaluated on two Text Categorization tasks and obtained state-of-theart performance using only the category names as initial seeds. {\textcopyright} 2005 Association for Computational Linguistics.},
author = {Gliozzo, Alfio and Strapparava, Carlo and Dagan, Ido},
booktitle = {HLT/EMNLP 2005 - Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, Proceedings of the Conference},
doi = {10.3115/1220575.1220592},
pages = {129--136},
publisher = {The Association for Computational Linguistics},
title = {{Investigating unsupervised learning for text categorization bootstrapping}},
url = {https://www.aclweb.org/anthology/H05-1017/},
year = {2005}
}

@InProceedings{Lo2012,
  author    = {Lo, Sio Long and Ding, Liya},
  booktitle = {Proceedings - International Conference on Machine Learning and Cybernetics},
  title     = {{Probabilistic reasoning on background net: An application to text categorization}},
  year      = {2012},
  pages     = {688--694},
  publisher = {IEEE},
  volume    = {2},
  abstract  = {Background net previously proposed is a novel approach for capturing and representing background information as a knowledge background accumulated through incremental learning on articles. As a continued study on background net, this article proposes a probabilistic reasoning on background nets by defining new acceptance measure based on conditional probabilities. Experiments on text categorization using representative data sets show that our approach, without requiring great effort in preprocessing, achieves competitive performance compared with Naive Bayes, kNN, and SVM methods. {\textcopyright} 2012 IEEE.},
  doi       = {10.1109/ICMLC.2012.6359008},
  isbn      = {9781467314855},
  issn      = {2160133X},
  keywords  = {Acceptance measure, Background net, Personalized articles selection, Probabilistic reasoning, Text categorization},
}

@Article{Chen2016,
  author   = {Chen, Kewen and Zhang, Zuping and Long, Jun and Zhang, Hao},
  journal  = {Expert Systems with Applications},
  title    = {{Turning from TF-IDF to TF-IGM for term weighting in text classification}},
  year     = {2016},
  issn     = {09574174},
  pages    = {1339--1351},
  volume   = {66},
  abstract = {Massive textual data management and mining usually rely on automatic text classification technology. Term weighting is a basic problem in text classification and directly affects the classification accuracy. Since the traditional TF-IDF (term frequency {\&} inverse document frequency) is not fully effective for text classification, various alternatives have been proposed by researchers. In this paper we make comparative studies on different term weighting schemes and propose a new term weighting scheme, TF-IGM (term frequency {\&} inverse gravity moment), as well as its variants. TF-IGM incorporates a new statistical model to precisely measure the class distinguishing power of a term. Particularly, it makes full use of the fine-grained term distribution across different classes of text. The effectiveness of TF-IGM is validated by extensive experiments of text classification using SVM (support vector machine) and kNN (k nearest neighbors) classifiers on three commonly used corpora. The experimental results show that TF-IGM outperforms the famous TF-IDF and the state-of-the-art supervised term weighting schemes. In addition, some new findings different from previous studies are obtained and analyzed in depth in the paper.},
  doi      = {10.1016/j.eswa.2016.09.009},
  keywords = {Class distinguishing power, Classifier, Inverse gravity moment (IGM), Term weighting, Text classification},
}

@inproceedings{Sainath2010,
abstract = {Sparse representations (SRs) are often used to characterize a test signal using few support training examples, and allow the number of supports to be adapted to the specific signal being categorized. Given the good performance of SRs compared to other classifiers for both image classification and phonetic classification, in this paper, we extended the use of SRs for text classification, a method which has thus far not been explored for this domain. Specifically, we demonstrate how sparse representations can be used for text classification and how their performance varies with the vocabulary size of the documents. In addition, we also show that this method offers promising results over the Naive Bayes (NB) classifier, a standard baseline classifier used for text categorization, thus introducing an alternative class of methods for text categorization. {\textcopyright} 2010 ISCA.},
author = {Sainath, Tara N. and Maskey, Sameer and Kanevsky, Dimitri and Ramabhadran, Bhuvana and Nahamoo, David and Hirschberg, Julia},
booktitle = {Proceedings of the 11th Annual Conference of the International Speech Communication Association, INTERSPEECH 2010},
doi = {10/i10},
editor = {Kobayashi, Takao and Hirose, Keikichi and Nakamura, Satoshi},
pages = {2266--2269},
publisher = {ISCA},
title = {{Sparse representations for text categorization}},
year = {2010}
}

@InProceedings{Chen2017,
  author        = {Chen, Yu and Zaki, Mohammed J.},
  booktitle     = {Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  title         = {{KATE: K-competitive autoencoder for text}},
  year          = {2017},
  pages         = {85--94},
  publisher     = {ACM},
  volume        = {Part F1296},
  abstract      = {Autoencoders have been successful in learning meaningful representations from image datasets. However, their performance on text datasets has not been widely studied. Traditional autoencoders tend to learn possibly trivial representations of text documents due to their confounding properties such as high-dimensionality, sparsity and power-law word distributions. In this paper, we propose a novel k-competitive autoencoder, called KATE, for text documents. Due to the competition between the neurons in the hidden layer, each neuron becomes specialized in recognizing specific data patterns, and overall the model can learn meaningful representations of textual data. A comprehensive set of experiments show that KATE can learn better representations than traditional autoencoders including denoising, contractive, variational, and k-sparse autoencoders. Our model also outperforms deep generative models, probabilistic topic models, and even word representation models (e.g., Word2Vec) in terms of several downstream tasks such as document classification, regression, and retrieval.},
  archiveprefix = {arXiv},
  arxivid       = {1705.02033},
  doi           = {10.1145/3097983.3098017},
  eprint        = {1705.02033},
  isbn          = {9781450348874},
  keywords      = {Autoencoders, Competitive learning, Representation learning, Text analytics},
}

@Article{Li2011,
  author   = {Li, Zhixing and Xiong, Zhongyang and Zhang, Yufang and Liu, Chunyong and Li, Kuan},
  journal  = {Pattern Recognition Letters},
  title    = {{Fast text categorization using concise semantic analysis}},
  year     = {2011},
  issn     = {01678655},
  number   = {3},
  pages    = {441--448},
  volume   = {32},
  abstract = {Text representation is a necessary procedure for text categorization tasks. Currently, bag of words (BOW) is the most widely used text representation method but it suffers from two drawbacks. First, the quantity of words is huge; second, it is not feasible to calculate the relationship between words. Semantic analysis (SA) techniques help BOW overcome these two drawbacks by interpreting words and documents in a space of concepts. However, existing SA techniques are not designed for text categorization and often incur huge computing cost. This paper proposes a concise semantic analysis (CSA) technique for text categorization tasks. CSA extracts a few concepts from category labels and then implements concise interpretation on words and documents. These concepts are small in quantity and great in generality and tightly related to the category labels. Therefore, CSA preserves necessary information for classifiers with very low computing cost. To evaluate CSA, experiments on three data sets (Reuters-21578, 20-NewsGroup and Tancorp) were conducted and the results show that CSA reaches a comparable micro- and macro-F1 performance with BOW, if not better one. Experiments also show that CSA helps dimension sensitive learning algorithms such as k-nearest neighbor (kNN) to eliminate the "Curse of Dimensionality" and as a result reaches a comparable performance with support vector machine (SVM) in text categorization applications. In addition, CSA is language independent and performs equally well both in Chinese and English. {\textcopyright} 2010 Elsevier B.V. All rights reserved.},
  doi      = {10.1016/j.patrec.2010.11.001},
  keywords = {Dimensionality reduction, Semantic analysis, Text categorization, Text representation},
}

@inproceedings{Larochelle2008,
abstract = {Recently, many applications for Restricted Boltzmann Machines (RBMs) have been developed for a large variety of learning problems. However, RBMs are usually used as feature extractors for another learning algorithm or to provide a good initialization for deep feed-forward neural network classifiers, and are not considered as a stand-alone solution to classification problems. In this paper, we argue that RBMs provide a self-contained framework for deriving competitive non-linear classifiers. We present an evaluation of different learning algorithms for RBMs which aim at introducing a discriminative component to RBM training and improve their performance as classifiers. This approach is simple in that RBMs are used directly to build a classifier, rather than as a stepping stone. Finally, we demonstrate how discriminative RBMs can also be successfully employed in a semi-supervised setting. Copyright 2008 by the author(s)/owner(s).},
author = {Larochelle, Hugo and Bengio, Yoshua},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
doi = {10.1145/1390156.1390224},
editor = {Cohen, William W and McCallum, Andrew and Roweis, Sam T},
isbn = {9781605582054},
pages = {536--543},
publisher = {ACM},
series = {{\{}ACM{\}} International Conference Proceeding Series},
title = {{Classification using discriminative restricted boltzmann machines}},
volume = {307},
year = {2008}
}

@InProceedings{Pappagari2018,
  author    = {Pappagari, Raghavendra and Villalba, Jesus and Dehak, Najim},
  booktitle = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
  title     = {{Joint Verification-Identification in end-to-end Multi-Scale CNN Framework for Topic Identification}},
  year      = {2018},
  pages     = {6199--6203},
  publisher = {IEEE},
  volume    = {2018-April},
  abstract  = {We present an end-to-end multi-scale Convolutional Neural Network (CNN) framework for topic identification (topic ID). In this work, we examined multi-scale CNN for classification using raw text input. Topical word embeddings are learnt at multiple scales using parallel convolutional layers. A technique to integrate verification and identification objectives is examined to improve topic ID performance. With this approach, we achieved significant improvement in identification task. We evaluated our framework on two contrasting datasets: 20 newsgroups and Fisher. We obtained 92.93{\%} accuracy on Fisher and 86.12{\%} on 20 newsgroups, which to our know ledge are the best published results on these datasets at the moment.},
  doi       = {10.1109/ICASSP.2018.8461673},
  isbn      = {9781538646588},
  issn      = {15206149},
  keywords  = {BOW, CNN, End-to-end, Identification, Raw text, Topic id, Verification},
}

@Article{Gomez2014,
  author   = {Gomez, Juan Carlos and Moens, Marie Francine},
  journal  = {Expert Systems with Applications},
  title    = {{Minimizer of the Reconstruction Error for multi-class document categorization}},
  year     = {2014},
  issn     = {09574174},
  number   = {3},
  pages    = {861--868},
  volume   = {41},
  abstract = {In the present article we introduce and validate an approach for single-label multi-class document categorization based on text content features. The introduced approach uses the statistical property of Principal Component Analysis, which minimizes the reconstruction error of the training documents used to compute a low-rank category transformation matrix. Such matrix transforms the original set of training documents from a given category to a new low-rank space and then optimally reconstructs them to the original space with a minimum reconstruction error. The proposed method, called Minimizer of the Reconstruction Error (mRE) classifier, uses this property, and extends and applies it to new unseen test documents. Several experiments on four multi-class datasets for text categorization are conducted in order to test the stable and generally better performance of the proposed approach in comparison with other popular classification methods. {\textcopyright} 2013 Elsevier Ltd. All rights reserved.},
  doi      = {10.1016/j.eswa.2013.08.016},
  keywords = {Dimensionality reduction, Document categorization, Principal Component Analysis, Text mining},
}

@Article{Feng2015,
  author   = {Feng, Guozhong and Guo, Jianhua and Jing, Bing Yi and Sun, Tieli},
  journal  = {Pattern Recognition Letters},
  title    = {{Feature subset selection using naive Bayes for text classification}},
  year     = {2015},
  issn     = {01678655},
  pages    = {109--115},
  volume   = {65},
  abstract = {Feature subset selection is known to improve text classification performance of various classifiers. The model using the selected features is often regarded as if it had generated the data. By taking its uncertainty into account, the discrimination capabilities can be measured by a global selection index (GSI), which can be used in the prediction function. In this paper, we propose a latent selection augmented naive (LSAN) Bayes classifier. By introducing a latent feature selection indicator, the GSI can be factorized into each local selection index (LSI). Using conjugate priors, the LSI for feature evaluation can be explicitly calculated. Then the feature subset selection models can be pruned by thresholding the LSIs, and the LSAN classifier can be achieved by the product of a small percentage of single feature model averages. The numerical results on some real datasets show that the proposed method outperforms the contrast feature weighting methods, and is very competitive if compared with some other commonly used classifiers such as SVM.},
  doi      = {10.1016/j.patrec.2015.07.028},
  keywords = {Bayesian model averaging, Global selection index, Latent selection augmented naive Bayes, Local selection index, Text classification},
}

@article{Unnam2020,
   author = {Narendra Babu Unnam and P. Krishna Reddy},
   doi = {10.1007/s41060-019-00200-5},
   issn = {2364-415X},
   issue = {1},
   journal = {International Journal of Data Science and Analytics},
   month = {6},
   pages = {49-64},
   publisher = {Springer Science and Business Media LLC},
   title = {A document representation framework with interpretable features using pre-trained word embeddings},
   volume = {10},
   url = {http://link.springer.com/10.1007/s41060-019-00200-5},
   year = {2020},
}

@article{Chen2020,
   author = {C. L. Philip Chen and Shuang Feng},
   doi = {10.1109/TCYB.2018.2869902},
   issn = {2168-2267},
   issue = {5},
   journal = {IEEE Transactions on Cybernetics},
   month = {5},
   pages = {2237-2248},
   publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
   title = {Generative and Discriminative Fuzzy Restricted Boltzmann Machine Learning for Text and Image Classification},
   volume = {50},
   url = {https://ieeexplore.ieee.org/document/8478774/},
   year = {2020},
}

@article{Guo2021,
   author = {Shun Guo and Nianmin Yao},
   doi = {10.1109/TKDE.2019.2961343},
   issn = {1041-4347},
   issue = {8},
   journal = {IEEE Transactions on Knowledge and Data Engineering},
   month = {8},
   pages = {3062-3074},
   publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
   title = {Document Vector Extension for Documents Classification},
   volume = {33},
   url = {https://ieeexplore.ieee.org/document/8938709/},
   year = {2021},
}

@article{Guo2020,
   author = {Shun Guo and Nianmin Yao},
   doi = {10.1007/s00521-019-04541-x},
   issn = {0941-0643},
   issue = {14},
   journal = {Neural Computing and Applications},
   month = {7},
   pages = {10087-10108},
   publisher = {Springer Science and Business Media LLC},
   title = {Generating word and document matrix representations for document classification},
   volume = {32},
   url = {http://link.springer.com/10.1007/s00521-019-04541-x},
   year = {2020},
}

@article{Zhou2020,
   abstract = {<p> Feature representation and feature extraction are two crucial procedures in text mining. Convolutional Neural Networks (CNN) have shown overwhelming success for text-mining tasks, since they are capable of efficiently extracting <italic>n</italic> -gram features from source data. However, vanilla CNN has its own weaknesses on feature representation and feature extraction. A certain amount of filters in CNN are inevitably duplicate and thus hinder to discriminatively represent a given text. In addition, most existing CNN models extract features in a fixed way (i.e., max pooling) that either limit the CNN to local optimum nor without considering the relation between all features, thereby unable to learn a contextual <italic>n</italic> -gram features adaptively. In this article, we propose a discriminative CNN with context-aware attention to solve the challenges of vanilla CNN. Specifically, our model mainly encourages discrimination across different filters via maximizing their earth mover distances and estimates the salience of feature candidates by considering the relation between context features. We validate carefully our findings against baselines on five benchmark datasets of classification and two datasets of summarization. The results of the experiments verify the competitive performance of our proposed model. </p>},
   author = {Yuxiang Zhou and Lejian Liao and Yang Gao and Heyan Huang and Xiaochi Wei},
   doi = {10.1145/3397464},
   issn = {2157-6904},
   issue = {5},
   journal = {ACM Transactions on Intelligent Systems and Technology},
   month = {10},
   pages = {1-21},
   publisher = {Association for Computing Machinery (ACM)},
   title = {A Discriminative Convolutional Neural Network with Context-aware Attention},
   volume = {11},
   url = {https://dl.acm.org/doi/10.1145/3397464},
   year = {2020},
}

@InProceedings{Chiu2020,
  author    = {Billy Chiu and Sunil Kumar Sahu and Neha Sengupta and Derek Thomas and Mohammady Mahdy},
  booktitle = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
  title     = {Attending to Inter-sentential Features in Neural Text Classification},
  year      = {2020},
  month     = {7},
  pages     = {1685-1688},
  publisher = {ACM},
  city      = {New York, NY, USA},
  doi       = {10.1145/3397271.3401203},
  isbn      = {9781450380164},
  journal   = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
  url       = {https://dl.acm.org/doi/10.1145/3397271.3401203},
}

@InProceedings{Bialas2020,
  author    = {Marcin Białas and Marcin Michał Mirończuk and Jacek Mańdziuk},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  title     = {Biologically Plausible Learning of Text Representation with Spiking Neural Networks},
  year      = {2020},
  pages     = {433-447},
  publisher = {Springer International Publishing},
  doi       = {10.1007/978-3-030-58112-1_30},
  journal   = {Parallel Problem Solving from Nature PPSN XVI},
  url       = {http://link.springer.com/10.1007/978-3-030-58112-1_30},
}

@InProceedings{Yang2020,
  author    = {Liang Yang and Fan Wu and Junhua Gu and Chuan Wang and Xiaochun Cao and Di Jin and Yuanfang Guo},
  booktitle = {Proceedings of The Web Conference 2020},
  title     = {Graph Attention Topic Modeling Network},
  year      = {2020},
  month     = {4},
  pages     = {144-154},
  publisher = {ACM},
  city      = {New York, NY, USA},
  doi       = {10.1145/3366423.3380102},
  isbn      = {9781450370233},
  journal   = {Proceedings of The Web Conference 2020},
  url       = {https://dl.acm.org/doi/10.1145/3366423.3380102},
}

@article{Kesiraju2020,
   author = {Santosh Kesiraju and Oldrich Plchot and Lukas Burget and Suryakanth V. Gangashetty},
   doi = {10.1109/TASLP.2020.3012062},
   issn = {2329-9290},
   journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
   pages = {2319-2332},
   publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
   title = {Learning Document Embeddings Along With Their Uncertainties},
   volume = {28},
   url = {https://ieeexplore.ieee.org/document/9149686/},
   year = {2020},
}

@article{Shen2020,
   author = {Yuan-Yuan Shen and Yan-Ming Zhang and Xu-Yao Zhang and Cheng-Lin Liu},
   doi = {10.1016/j.neucom.2020.03.025},
   issn = {09252312},
   journal = {Neurocomputing},
   month = {7},
   pages = {467-478},
   publisher = {Elsevier BV},
   title = {Online semi-supervised learning with learning vector quantization},
   volume = {399},
   url = {https://linkinghub.elsevier.com/retrieve/pii/S0925231220303672},
   year = {2020},
}

@inproceedings{Suneera2020,
   author = {C M Suneera and Jay Prakash},
   doi = {10.1109/INDICON49873.2020.9342208},
   isbn = {978-1-7281-6916-3},
   journal = {2020 IEEE 17th India Council International Conference (INDICON)},
   month = {12},
   pages = {1-6},
   publisher = {IEEE},
   title = {Performance Analysis of Machine Learning and Deep Learning Models for Text Classification},
   url = {https://ieeexplore.ieee.org/document/9342208/},
   year = {2020},
}

@InProceedings{Wei2020,
  author    = {Xinde Wei and Hai Huang and Longxuan Ma and Ze Yang and Liutong Xu},
  booktitle = {2020 IEEE 11th International Conference on Software Engineering and Service Science (ICSESS)},
  title     = {Recurrent Graph Neural Networks for Text Classification},
  year      = {2020},
  month     = {10},
  pages     = {91-97},
  publisher = {IEEE},
  doi       = {10.1109/ICSESS49938.2020.9237709},
  isbn      = {978-1-7281-6578-3},
  journal   = {2020 IEEE 11th International Conference on Software Engineering and Service Science (ICSESS)},
  url       = {https://ieeexplore.ieee.org/document/9237709/},
}

@article{Chen2020c,
   author = {Gang Chen and Sargur N. Srihari},
   doi = {10.1016/j.patrec.2020.10.006},
   issn = {01678655},
   journal = {Pattern Recognition Letters},
   month = {12},
   pages = {214-221},
   publisher = {Elsevier BV},
   title = {Revisiting hierarchy: Deep learning with orthogonally constrained prior for classification},
   volume = {140},
   url = {https://linkinghub.elsevier.com/retrieve/pii/S0167865520303810},
   year = {2020},
}

@article{Aler2020,
   author = {Ricardo Aler and José M. Valls and Henrik Boström},
   doi = {10.1016/j.eswa.2020.113264},
   issn = {09574174},
   journal = {Expert Systems with Applications},
   month = {7},
   pages = {113264},
   publisher = {Elsevier BV},
   title = {Study of Hellinger Distance as a splitting metric for Random Forests in balanced and imbalanced classification datasets},
   volume = {149},
   url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417420300890},
   year = {2020},
}

@article{Jiang2020,
   author = {Haiyun Jiang and Deqing Yang and Yanghua Xiao and Wei Wang},
   doi = {10.1007/s11280-020-00806-x},
   issn = {1386-145X},
   issue = {4},
   journal = {World Wide Web},
   month = {7},
   pages = {2429-2447},
   publisher = {Springer Science and Business Media LLC},
   title = {Understanding a bag of words by conceptual labeling with prior weights},
   volume = {23},
   url = {https://link.springer.com/10.1007/s11280-020-00806-x},
   year = {2020},
}

@Article{Wagh2021,
  author    = {Vedangi Wagh and Snehal Khandve and Isha Joshi and Apurva Wani and Geetanjali Kale and Raviraj Joshi},
  journal   = {TENCON 2021 - 2021 IEEE Region 10 Conference (TENCON)},
  title     = {Comparative Study of Long Document Classification},
  year      = {2021},
  month     = {12},
  pages     = {732-737},
  doi       = {10.1109/TENCON54134.2021.9707465},
  isbn      = {978-1-6654-9532-5},
  publisher = {IEEE},
  url       = {https://ieeexplore.ieee.org/document/9707465/},
}

@article{Zhou2021,
   author = {Yuxiang Zhou and Lejian Liao and Yang Gao and Heyan Huang},
   doi = {10.1016/j.ins.2020.12.084},
   issn = {00200255},
   journal = {Information Sciences},
   month = {5},
   pages = {265-279},
   publisher = {Elsevier BV},
   title = {Extracting salient features from convolutional discriminative filters},
   volume = {558},
   url = {https://linkinghub.elsevier.com/retrieve/pii/S0020025521000165},
   year = {2021},
}

@article{Dai2022,
   author = {Yong Dai and Linjun Shou and Ming Gong and Xiaolin Xia and Zhao Kang and Zenglin Xu and Daxin Jiang},
   doi = {10.1016/j.knosys.2021.107659},
   issn = {09507051},
   journal = {Knowledge-Based Systems},
   month = {1},
   pages = {107659},
   publisher = {Elsevier BV},
   title = {Graph Fusion Network for Text Classification},
   volume = {236},
   url = {https://linkinghub.elsevier.com/retrieve/pii/S0950705121009217},
   year = {2022},
}

@InProceedings{Xie2021,
  author    = {Qianqian Xie and Jimin Huang and Pan Du and Min Peng and Jian-Yun Nie},
  booktitle = {Proceedings of the Web Conference 2021},
  title     = {Graph Topic Neural Network for Document Representation},
  year      = {2021},
  month     = {4},
  pages     = {3055-3065},
  publisher = {ACM},
  city      = {New York, NY, USA},
  doi       = {10.1145/3442381.3450045},
  isbn      = {9781450383127},
  journal   = {Proceedings of the Web Conference 2021},
  url       = {https://dl.acm.org/doi/10.1145/3442381.3450045},
}

@InProceedings{Ragesh2021,
  author    = {Rahul Ragesh and Sundararajan Sellamanickam and Arun Iyer and Ramakrishna Bairi and Vijay Lingam},
  booktitle = {Proceedings of the 14th ACM International Conference on Web Search and Data Mining},
  title     = {HeteGCN: Heterogeneous Graph Convolutional Networks for Text Classification},
  year      = {2021},
  month     = {3},
  pages     = {860-868},
  publisher = {ACM},
  city      = {New York, NY, USA},
  doi       = {10.1145/3437963.3441746},
  isbn      = {9781450382977},
  journal   = {Proceedings of the 14th ACM International Conference on Web Search and Data Mining},
  url       = {https://dl.acm.org/doi/10.1145/3437963.3441746},
}

@inproceedings{Zhang2021,
   author = {Cheng Zhang and Hayato Yamana},
   doi = {10.1109/ICBDA51983.2021.9403092},
   isbn = {978-1-6654-1513-2},
   journal = {2021 IEEE 6th International Conference on Big Data Analytics (ICBDA)},
   month = {3},
   pages = {193-197},
   publisher = {IEEE},
   title = {Improving Text Classification Using Knowledge in Labels},
   url = {https://ieeexplore.ieee.org/document/9403092/},
   year = {2021},
}

@article{Nagumothu2021,
   abstract = {<p>Standardized approaches to relevance classification in information retrieval use generative statistical models to identify the presence or absence of certain topics that might make a document relevant to the searcher. These approaches have been used to better predict relevance on the basis of what the document is “about”, rather than a simple-minded analysis of the bag of words contained within the document. In more recent times, this idea has been extended by using pre-trained deep learning models and text representations, such as GloVe or BERT. These use an external corpus as a knowledge-base that conditions the model to help predict what a document is about. This paper adopts a hybrid approach that leverages the structure of knowledge embedded in a corpus. In particular, the paper reports on experiments where linked data triples (subject-predicate-object), constructed from natural language elements are derived from deep learning. These are evaluated as additional latent semantic features for a relevant document classifier in a customized news-feed website. The research is a synthesis of current thinking in deep learning models in NLP and information retrieval and the predicate structure used in semantic web research. Our experiments indicate that linked data triples increased the F-score of the baseline GloVe representations by 6% and show significant improvement over state-of-the art models, like BERT. The findings are tested and empirically validated on an experimental dataset and on two standardized pre-classified news sources, namely the Reuters and 20 News groups datasets.</p>},
   author = {Dinesh Nagumothu and Peter W. Eklund and Bahadorreza Ofoghi and Mohamed Reda Bouadjenek},
   doi = {10.3390/app11146636},
   issn = {2076-3417},
   issue = {14},
   journal = {Applied Sciences},
   month = {7},
   pages = {6636},
   publisher = {MDPI AG},
   title = {Linked Data Triples Enhance Document Relevance Classification},
   volume = {11},
   url = {https://www.mdpi.com/2076-3417/11/14/6636},
   year = {2021},
}

@article{Wang2021a,
   author = {Tao Wang and Yi Cai and Ho-fung Leung and Raymond Y. K. Lau and Haoran Xie and Qing Li},
   doi = {10.1007/s10115-021-01581-5},
   issn = {0219-1377},
   issue = {9},
   journal = {Knowledge and Information Systems},
   month = {9},
   pages = {2313-2346},
   publisher = {Springer Science and Business Media LLC},
   title = {On entropy-based term weighting schemes for text categorization},
   volume = {63},
   url = {https://link.springer.com/10.1007/s10115-021-01581-5},
   year = {2021},
}

@article{Yan2021a,
   author = {Peng Yan and Linjing Li and Miaotianzi Jin and Daniel Zeng},
   doi = {10.1016/j.neucom.2021.02.060},
   issn = {09252312},
   journal = {Neurocomputing},
   month = {7},
   pages = {276-286},
   publisher = {Elsevier BV},
   title = {Quantum probability-inspired graph neural network for document representation and classification},
   volume = {445},
   url = {https://linkinghub.elsevier.com/retrieve/pii/S0925231221003167},
   year = {2021},
}

@article{Wang2021b,
   author = {Shuaihui Wang and Yu Pan and Jin Zhang and Xingyu Zhou and Zhen Cui and Guyu Hu and Zhisong Pan},
   doi = {10.1016/j.knosys.2021.106891},
   issn = {09507051},
   journal = {Knowledge-Based Systems},
   month = {7},
   pages = {106891},
   publisher = {Elsevier BV},
   title = {Robust and label efficient bi-filtering graph convolutional networks for node classification},
   volume = {224},
   url = {https://linkinghub.elsevier.com/retrieve/pii/S0950705121001544},
   year = {2021},
}

@article{Prabhakar2022,
   abstract = {<p>To classify the texts accurately, many machine learning techniques have been utilized in the field of Natural Language Processing (NLP). For many pattern classification applications, great success has been obtained when implemented with deep learning models rather than using ordinary machine learning techniques. Understanding the complex models and their respective relationships within the data determines the success of such deep learning techniques. But analyzing the suitable deep learning methods, techniques, and architectures for text classification is a huge challenge for researchers. In this work, a Contiguous Convolutional Neural Network (CCNN) based on Differential Evolution (DE) is initially proposed and named as Evolutionary Contiguous Convolutional Neural Network (ECCNN) where the data instances of the input point are considered along with the contiguous data points in the dataset so that a deeper understanding is provided for the classification of the respective input, thereby boosting the performance of the deep learning model. Secondly, a swarm-based Deep Neural Network (DNN) utilizing Particle Swarm Optimization (PSO) with DNN is proposed for the classification of text, and it is named Swarm DNN. This model is validated on two datasets and the best results are obtained when implemented with the Swarm DNN model as it produced a high classification accuracy of 97.32% when tested on the BBC newsgroup text dataset and 87.99% when tested on 20 newsgroup text datasets. Similarly, when implemented with the ECCNN model, it produced a high classification accuracy of 97.11% when tested on the BBC newsgroup text dataset and 88.76% when tested on 20 newsgroup text datasets.</p>},
   author = {Sunil Kumar Prabhakar and Harikumar Rajaguru and Kwangsub So and Dong-Ok Won},
   doi = {10.3389/fncom.2022.900885},
   issn = {1662-5188},
   journal = {Frontiers in Computational Neuroscience},
   month = {6},
   publisher = {Frontiers Media SA},
   title = {A Framework for Text Classification Using Evolutionary Contiguous Convolutional Neural Network and Swarm Based Deep Neural Network},
   volume = {16},
   url = {https://www.frontiersin.org/articles/10.3389/fncom.2022.900885/full},
   year = {2022},
}

@article{Jia2022,
   abstract = {<p lang="fr"><![CDATA[<abstract> <p>There are two main factors involved in documents classification, document representation method and classification algorithm. In this study, we focus on document representation method and demonstrate that the choice of representation methods has impacts on quality of classification results. We propose a document representation strategy for supervised text classification named document representation based on global policy (<italic>DRGP</italic>), which can obtain an appropriate document representation according to the distribution of terms. The main idea of <italic>DRGP</italic> is to construct the optimization function through the importance of terms to different categories. In the experiments, we investigate the effects of <italic>DRGP</italic> on the 20 Newsgroups, Reuters21578 datasets, and using the <italic>SVM</italic> as classifier. The results show that the <italic>DRGP</italic> outperforms other text representation strategy schemes, such as Document Max, Document Two Max and global policy.</p> </abstract>]]></p>},
   author = {Longjia Jia and Bangzuo Zhang},
   doi = {10.3934/mbe.2022245},
   issn = {1551-0018},
   issue = {5},
   journal = {Mathematical Biosciences and Engineering},
   pages = {5223-5240},
   publisher = {American Institute of Mathematical Sciences (AIMS)},
   title = {A new document representation based on global policy for supervised term weighting schemes in text categorization},
   volume = {19},
   url = {http://www.aimspress.com/article/doi/10.3934/mbe.2022245},
   year = {2022},
}

@article{Tang2022,
   author = {Zhong Tang and Wenqiang Li and Yan Li},
   doi = {10.1016/j.eswa.2021.115985},
   issn = {09574174},
   journal = {Expert Systems with Applications},
   month = {3},
   pages = {115985},
   publisher = {Elsevier BV},
   title = {An improved supervised term weighting scheme for text representation and classification},
   volume = {189},
   url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417421013348},
   year = {2022},
}

@InProceedings{Lin2021,
  author    = {Yuxiao Lin and Yuxian Meng and Xiaofei Sun and Qinghong Han and Kun Kuang and Jiwei Li and Fei Wu},
  booktitle = {Findings},
  title     = {BertGCN: Transductive Text Classification by Combining GNN and BERT},
  year      = {2021},
  pages     = {1456-1462},
  publisher = {Association for Computational Linguistics},
  city      = {Stroudsburg, PA, USA},
  doi       = {10.18653/v1/2021.findings-acl.126},
  journal   = {Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021},
  url       = {https://aclanthology.org/2021.findings-acl.126},
}

@article{Shehzad2022,
   abstract = {<p>In text categorization, a well-known problem related to document length is that larger term counts in longer documents cause classification algorithms to become biased. The effect of document length can be eliminated by normalizing term counts, thus reducing the bias towards longer documents. This gives us term frequency (TF), which in conjunction with inverse document frequency (IDF) became the most commonly used term weighting scheme to capture the importance of a term in a document and corpus. However, normalization may cause term frequency of a term in a related document to become equal or smaller than its term frequency in an unrelated document, thus perturbing a term’s strength from its true worth. In this paper, we solve this problem by introducing a non-linear mapping of term frequency. This alternative to TF is called binned term count (BTC). The newly proposed term frequency factor trims large term counts before normalization, thus moderating the normalization effect on large documents. To investigate the effectiveness of BTC, we compare it against the original TF and its more recently proposed alternative named modified term frequency (MTF). In our experiments, each of these term frequency factors (BTC, TF, and MTF) is combined with four well-known collection frequency factors (IDF), RF, IGM, and MONO and the performance of each of the resulting term weighting schemes is evaluated on three standard datasets (Reuters (R8-21578), 20-Newsgroups, and WebKB) using support vector machines and K-nearest neighbor classifiers. To determine whether BTC is statistically better than TF and MTF, we have applied the paired two-sided t-test on the macro F1 results. Overall, BTC is found to be 52% statistically significant than TF and MTF. Furthermore, the highest macro F1 value on the three datasets was achieved by BTC-based term weighting schemes.</p>},
   author = {Farhan Shehzad and Abdur Rehman and Kashif Javed and Khalid A. Alnowibet and Haroon A. Babri and Hafiz Tayyab Rauf},
   doi = {10.3390/math10214124},
   issn = {2227-7390},
   issue = {21},
   journal = {Mathematics},
   month = {11},
   pages = {4124},
   publisher = {MDPI AG},
   title = {Binned Term Count: An Alternative to Term Frequency for Text Categorization},
   volume = {10},
   url = {https://www.mdpi.com/2227-7390/10/21/4124},
   year = {2022},
}

@article{Yang2022,
   author = {Fei Yang and Huyin Zhang and Shiming Tao and Sheng Hao},
   doi = {10.1007/s10489-021-02889-z},
   issn = {0924-669X},
   issue = {10},
   journal = {Applied Intelligence},
   month = {8},
   pages = {11324-11342},
   publisher = {Springer Science and Business Media LLC},
   title = {Graph representation learning via simple jumping knowledge networks},
   volume = {52},
   url = {https://link.springer.com/10.1007/s10489-021-02889-z},
   year = {2022},
}

@article{Attieh2023,
   author = {Joseph Attieh and Joe Tekli},
   doi = {10.1016/j.knosys.2022.110215},
   issn = {09507051},
   journal = {Knowledge-Based Systems},
   month = {2},
   pages = {110215},
   publisher = {Elsevier BV},
   title = {Supervised term-category feature weighting for improved text classification},
   volume = {261},
   url = {https://linkinghub.elsevier.com/retrieve/pii/S0950705122013119},
   year = {2023},
}

@article{Wang2023,
   author = {Yizhao Wang and Chenxi Wang and Jieyu Zhan and Wenjun Ma and Yuncheng Jiang},
   doi = {10.1016/j.eswa.2023.119658},
   issn = {09574174},
   journal = {Expert Systems with Applications},
   month = {6},
   pages = {119658},
   publisher = {Elsevier BV},
   title = {Text FCG: Fusing Contextual Information via Graph Learning for text classification},
   volume = {219},
   url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417423001598},
   year = {2023},
}

@InProceedings{Zhu2021,
  author    = {Hao Zhu and Piotr Koniusz},
  booktitle = {International Conference on Learning Representations},
  title     = {Simple Spectral Graph Convolution},
  year      = {2021},
  journal   = {International Conference on Learning Representations},
  url       = {https://openreview.net/forum?id=CYO5T-YjWZV},
}

@InProceedings{Xie2021a,
  author    = {Qianqian Xie and Jimin Huang and Pan Du and Min Peng and Jian-Yun Nie},
  booktitle = {North American Chapter of the Association for Computational Linguistics},
  title     = {Inductive Topic Variational Graph Auto-Encoder for Text Classification},
  year      = {2021},
  pages     = {4218-4227},
  publisher = {Association for Computational Linguistics},
  city      = {Stroudsburg, PA, USA},
  doi       = {10.18653/v1/2021.naacl-main.333},
  journal   = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  url       = {https://aclanthology.org/2021.naacl-main.333},
}

@article{Liu2020,
   abstract = {<p>Compared to sequential learning models, graph-based neural networks exhibit some excellent properties, such as ability capturing global information. In this paper, we investigate graph-based neural networks for text classification problem. A new framework TensorGCN (tensor graph convolutional networks), is presented for this task. A text graph tensor is firstly constructed to describe semantic, syntactic, and sequential contextual information. Then, two kinds of propagation learning perform on the text graph tensor. The first is intra-graph propagation used for aggregating information from neighborhood nodes in a single graph. The second is inter-graph propagation used for harmonizing heterogeneous information between graphs. Extensive experiments are conducted on benchmark datasets, and the results illustrate the effectiveness of our proposed framework. Our proposed TensorGCN presents an effective way to harmonize and integrate heterogeneous information from different kinds of graphs.</p>},
   author = {Xien Liu and Xinxin You and Xiao Zhang and Ji Wu and Ping Lv},
   doi = {10.1609/aaai.v34i05.6359},
   issn = {2374-3468},
   issue = {05},
   journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
   month = {4},
   pages = {8409-8416},
   publisher = {Association for the Advancement of Artificial Intelligence (AAAI)},
   title = {Tensor Graph Convolutional Networks for Text Classification},
   volume = {34},
   url = {https://ojs.aaai.org/index.php/AAAI/article/view/6359},
   year = {2020},
}

@inproceedings{Gupta2019,
   author = {Vivek Gupta and Ankit Kumar Saw and Pegah Nokhiz and Harshit Gupta and Partha Pratim Talukdar},
   journal = {European Conference on Artificial Intelligence},
   title = {Improving Document Classification with Multi-Sense Embeddings},
   year = {2019},
}

@InProceedings{Wang2020,
  author    = {Zhengjue Wang and Chaojie Wang and Hao Zhang and Zhibin Duan and Mingyuan Zhou and Bo Chen},
  booktitle = {International conference on artificial intelligence and statistics},
  title     = {Learning Dynamic Hierarchical Topic Graph with Graph Convolutional Network for Document Classification},
  year      = {2020},
  editor    = {Silvia Chiappa and Roberto Calandra},
  month     = {2},
  pages     = {3959-3969},
  publisher = {PMLR},
  volume    = {108},
  abstract  = {Constructing a graph with graph convolutional network (GCN)  to explore the relational structure of the data has attracted lots of interests in various tasks. However, for document classification, existing graph based methods often focus on the straightforward word-word and word-document relations, ignoring the hierarchical semantics. Besides, the graph construction is often independent from the task-specific GCN learning. To address these constrains, we integrate a probabilistic deep topic model into graph construction, and propose a novel trainable hierarchical topic graph (HTG), including word-level, hierarchical topic-level and document-level nodes, exhibiting semantic variation from fine-grained to coarse. Regarding the document classification as a document-node label generation task, HTG can be dynamically evolved with GCN by performing variational inference, which leads to an end-to-end document classification method, named dynamic HTG (DHTG). Besides achieving state-of-the-art classification results, our model learns an interpretable document graph with meaningful node embeddings and semantic edges.},
  journal   = {Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics},
  url       = {https://proceedings.mlr.press/v108/wang20l.html},
}

@inproceedings{Johnson2016,
   abstract = {One-hot CNN (convolutional neural network) has been shown to be effective for text categorization (Johnson &amp; Zhang, 2015a;b). We view it as a special case of a general framework which jointly trains a linear model with a non-linear feature generator consisting of 'text region embedding + pooling'. Under this framework, we explore a more sophisticated region embedding method using Long Short-Term Memory (LSTM). LSTM can embed text regions of variable (and possibly large) sizes, whereas the region size needs to be fixed in a CNN. We seek effective and efficient use of LSTM for this purpose in the supervised and semi-supervised settings. The best results were obtained by combining region embeddings in the form of LSTM and convolution layers trained on unlabeled data. The results indicate that on this task, embeddings of text regions, which can convey complex concepts, are more useful than embeddings of single words in isolation. We report performances exceeding the previous best results on four benchmark datasets.},
   author = {Rie Johnson and Tong Zhang},
   journal = {Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48},
   pages = {526-534},
   publisher = {JMLR.org},
   title = {Supervised and Semi-Supervised Text Categorization Using LSTM for Region Embeddings},
   year = {2016},
}

@InProceedings{Ding2020,
  author    = {Kaize Ding and Jianling Wang and Jundong Li and Dingcheng Li and Huan Liu},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  title     = {Be More with Less: Hypergraph Attention Networks for Inductive Text Classification},
  year      = {2020},
  pages     = {4927-4936},
  publisher = {Association for Computational Linguistics},
  city      = {Stroudsburg, PA, USA},
  doi       = {10.18653/v1/2020.emnlp-main.399},
  journal   = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  url       = {https://www.aclweb.org/anthology/2020.emnlp-main.399},
}

@InProceedings{Guidotti2022,
  author    = {Emanuele Guidotti and Alfio Ferrara},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Text Classification with Born's Rule},
  year      = {2022},
  editor    = {Alice H Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
  journal   = {Advances in Neural Information Processing Systems},
  url       = {https://openreview.net/forum?id=sNcn-E3uPHA},
}

@InProceedings{Khandve2022,
  author    = {Snehal Ishwar Khandve and Vedangi Kishor Wagh and Apurva Dinesh Wani and Isha Mandar Joshi and Raviraj Bhuminand Joshi},
  booktitle = {Proceedings of the 2022 14th International Conference on Machine Learning and Computing},
  title     = {Hierarchical Neural Network Approaches for Long Document Classification},
  year      = {2022},
  month     = {2},
  pages     = {115-119},
  publisher = {ACM},
  city      = {New York, NY, USA},
  doi       = {10.1145/3529836.3529935},
  isbn      = {9781450395700},
  journal   = {2022 14th International Conference on Machine Learning and Computing (ICMLC)},
  url       = {https://dl.acm.org/doi/10.1145/3529836.3529935},
}

@article{Yao2019,
   abstract = {<p>Text classification is an important and classical problem in natural language processing. There have been a number of studies that applied convolutional neural networks (convolution on regular grid, e.g., sequence) to classification. However, only a limited number of studies have explored the more flexible graph convolutional neural networks (convolution on non-grid, e.g., arbitrary graph) for the task. In this work, we propose to use graph convolutional networks for text classification. We build a single text graph for a corpus based on word co-occurrence and document word relations, then learn a Text Graph Convolutional Network (Text GCN) for the corpus. Our Text GCN is initialized with one-hot representation for word and document, it then jointly learns the embeddings for both words and documents, as supervised by the known class labels for documents. Our experimental results on multiple benchmark datasets demonstrate that a vanilla Text GCN without any external word embeddings or knowledge outperforms state-of-the-art methods for text classification. On the other hand, Text GCN also learns predictive word and document embeddings. In addition, experimental results show that the improvement of Text GCN over state-of-the-art comparison methods become more prominent as we lower the percentage of training data, suggesting the robustness of Text GCN to less training data in text classification.</p>},
   author = {Liang Yao and Chengsheng Mao and Yuan Luo},
   doi = {10.1609/aaai.v33i01.33017370},
   issn = {2374-3468},
   issue = {01},
   journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
   month = {7},
   pages = {7370-7377},
   publisher = {Association for the Advancement of Artificial Intelligence (AAAI)},
   title = {Graph Convolutional Networks for Text Classification},
   volume = {33},
   url = {https://ojs.aaai.org/index.php/AAAI/article/view/4725},
   year = {2019},
}

@Article{Raiaan2024,
  author    = {Mohaimenul Azam Khan Raiaan and Md. Saddam Hossain Mukta and Kaniz Fatema and Nur Mohammad Fahad and Sadman Sakib and Most Marufatul Jannat Mim and Jubaer Ahmad and Mohammed Eunus Ali and Sami Azam},
  journal   = {{IEEE} Access},
  title     = {A Review on Large Language Models: Architectures, Applications, Taxonomies, Open Issues and Challenges},
  year      = {2024},
  pages     = {26839--26874},
  volume    = {12},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/journals/access/RaiaanMFFSMAAA24.bib},
  doi       = {10.1109/ACCESS.2024.3365742},
}

@Article{Payandeh2023,
  author    = {Amirreza Payandeh and Kourosh Teimouri Baghaei and Pooya Fayyazsanavi and Somayeh Bakhtiari Ramezani and Zhiqian Chen and Shahram Rahimi},
  journal   = {{IEEE} Access},
  title     = {Deep Representation Learning: Fundamentals, Technologies, Applications, and Open Challenges},
  year      = {2023},
  pages     = {137621--137659},
  volume    = {11},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/journals/access/PayandehBFRCR23.bib},
  doi       = {10.1109/ACCESS.2023.3335196},
}

@inproceedings{Devlin2019,
   abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
   author = {Jacob Devlin and Ming Wei Chang and Kenton Lee and Kristina Toutanova},
   isbn = {9781950737130},
   booktitle = {NAACL HLT 2019 - 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies - Proceedings of the Conference},
   pages = {4171-4186},
   publisher = {Association for Computational Linguistics (ACL)},
   title = {BERT: Pre-training of deep bidirectional transformers for language understanding},
   volume = {1},
   year = {2019},
}

@article{Liu2019,
   abstract = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
   author = {Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov and Paul G Allen},
   month = {7},
   title = {RoBERTa: A Robustly Optimized BERT Pretraining Approach},
   url = {https://arxiv.org/abs/1907.11692v1},
   year = {2019},
}

@article{Touvron2023,
   abstract = {We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.},
   author = {Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timothée Lacroix and Baptiste Rozière and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
   month = {2},
   title = {LLaMA: Open and Efficient Foundation Language Models},
   url = {https://arxiv.org/abs/2302.13971v1},
   year = {2023},
}

@Article{Openai,
  author   = {Alec Radford Openai and Karthik Narasimhan Openai and Tim Salimans Openai and Ilya Sutskever Openai},
  title    = {Improving Language Understanding by Generative Pre-Training},
  abstract = {Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classification. Although large unlabeled text corpora are abundant, labeled data for learning these specific tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative fine-tuning on each specific task. In contrast to previous approaches, we make use of task-aware input transformations during fine-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures specifically crafted for each task, significantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9% on commonsense reasoning (Stories Cloze Test), 5.7% on question answering (RACE), and 1.5% on textual entailment (MultiNLI).},
  url      = {https://gluebenchmark.com/leaderboard},
}

@Comment{jabref-meta: databaseType:bibtex;}
